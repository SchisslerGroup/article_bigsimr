# Algorithms {#algorithms}

```{r ch020-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('020-simulation-algorithm.Rmd')
```

This section describes our methods involved in simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$.
Each $Y_i$ has a specified marginal CDF $F_i$ and its inverse $F^{-1}_i$.
To characterize dependency, every pair $(Y_i, Y_j)$ has a given Pearson correlation $\rho_P$, Spearman correlation $\rho_S$, and/or Kendall's $\tau$.
The method is best understand as a **high-performance Gaussian copula** (Equation \@ref(eq:gauss)) providing a high-dimensional NORTA-inspired algorithm.

## NORmal To Anything (NORTA)

The well-known NORTA algorithm [@Cario1997] can be used simulate a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$.
Specifically, the NORTA algorithm follows like this:

1. Simulate a random vector $\bf Z$ with $d$ **independent** and **identical** standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ so that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_{Y_i}^{-1}[\Phi(X_i)], \; i=1,2,...,d$.

With modern parallelized computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multicore and/or graphical-processing-unit (GPU) accelerated, algorithms --- providing the fast scaleability using readily-available hardware.

Matching specified Pearson correlation coefficients exactly (step 2 above), however, is problematic.
In general, there is no closed form correspondence between the components of the input $\Sigma_{\bf Z}$ and target $\Sigma_{\bf Y}$.
Matching the correlations involves evaluating or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d, \; i \neq j$.
For high-dimensional data, these evaluations are often too costly to enable feasible simulation studies.
For low-dimensional problems, methods and tools exist to match Pearson correlations precisely, see @Chen2001; @Xia17; @MB13 and the publicly available `nortaRA` R package.

To maintain scaleability (SP1), our solution is to essentially avoid this complication in Pearson matching.
Since our goal is to simulate non-normal marginals, we greatly prefer the use of rank-based measures $\rho_S$ and $\tau$ from a modeling standpoint.
Further, $\rho_S$ and $\tau$'s invariance under monotone transformation (see [Background](#background)), perserves the correlation coefficients through steps 3, 4, and 5 in the NORTA algorithm above.
This eliminates the need for computing the $\binom{d}{2}$ integrals to match exactly (nothing is for free, however, as discussed in below in Section \@ref(rand-vec-gen)).

Despite all this, if one does desire to characterize dependency using Pearson correlations, simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution [@Song00] in many practical applications.
The quality of this approximation depends on the setting, but in practice, for high-dimensional count data we find the accuracy to be adequate.
Later, we'll study the robustness of our method to this limitation in selected [Monte Carlo evalutations]({#simulations}).

## Random vector generation via `bigsimr::rvec` {#rand-vec-gen}

Now we describe `bigsimr::rvec`, our algorithm to generate random vectors.
It mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:

1. Pre-processing for nonparameteric dependency matching.  
	(i) Convert from either ${\bf R_{Spearman}}$ or ${\bf R_{Kendall}}$ into the corresponding MVN input correlation ${\bf R_{Pearson}}$.
	(ii) Check that ${\bf R_{Pearson}}$ is semi-positive definite.  
	(iii) If not compute a close semi-positive definite correlation matrix ${\bf \widetilde{R}_{Pearson}}$.  
2. Gaussian copula construction.  
	(i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_{Pearson}})$.  
	(ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$.  
3. Quantile evaluations.  
	(i) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

The pre-processing (Step 1) takes advantage of the closed-form relationships between $\rho_S$ and $\tau$ with $\rho_P$ for bivariate normal random variables via Equations \@ref(eq:convertSpearman) or \@ref(eq:convertKendall), respectively (implemented as `bigsimr::cor_covert`).

A complication often arises at this stage: the resultant matrix may become indefinite, either through numerical error or naturally occurring in the correlation conversion.
Researchers working in multivariate computation frequently encounter such difficulties and need to find a close positive (semi-)definite matrix.
A widely-available routine for this task in `R` is called `matrix::nearPD`, though it is not suitable for high dimensions.
To overcome this issue, we've developed `bigsimr::nearPSD`, a quadractically-convergent Netwon method for finding the nearest correlation matrix, developed by @QS2006.
We hope that this routine could be useful in many applications aside from our primary goal of random vector generation.

Once the target margins and algorithm inputs are determined, steps 2 and 3 are essentially a NORTA algorithm with modern high-performance computing implementations.
Specifically, step 2i uses either an efficient multicore multivariate normal simulator (the `R` package `mvnfast`[@Fasiolo2016]) or a using Google's JAX python library NumPy for graphical-processing-unit (GPU) acceleration of the Cholesky factorization and matrix multiplication (steps 3, 4 in the NORTA algorithm in the preceeding section).

## NORTA in higher dimensions

Sklar's theorem provides a useful characterization of multivariate distributions through copulas.
Yet the choice of copula-based simulation algorithm affects which joint distributions may be simulated.
Even in low-dimensional spaces (e.g., $d=3$), there exists valid multivariate distributions with *feasible* Pearson correlation matrices that NORTA cannot match exactly [@LH75].
This occurs when the bivariate transformations are applied to find the input correlation matrix yet when combined the resultant matrix is indefinite.
These situations do occur, even using exact analytic calculations. Such problematic target correlation matrices are termed *NORTA defective*.

@GH02 conducted a Monte Carlo study to estimate the probability of NORTA defective matrices while increasing the dimension $d$. 
They found at what is now considered low-to-moderate dimensions ($d \approx 20$) that almost *all* feasible matrices are NORTA defective.
This stems from the concentration of measure near the boundary of the space of all possible correlation matrices as dimension increases. Unfortunately, it is precisely near this boundary that NORTA defective matrices reside.

There is hope, however, as @GH02 also show that by augmenting the NORTA procedure by replacing the indefinite input correlation matrix with a close proxy will give approximate matching to the target --- with adequate performance for moderate $d$.
This provides evidence that our nearest positive semidefinite (PSD) augmented approach will maintain reasonable accuracy if our input matching scheme returns an indefinite matrix, at least for the rank-based matching scheme described above.

<!-- 
## Other high-performance `bigsimr` algorithms

<mark> ALEX, briefly discuss other algorithms here </mark>
- `cor_bounds()`  
- `cor_fast()`  
- `cor_nearPSD()`  

For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components from Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

your comment -->
