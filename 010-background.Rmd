# Background {#background}

```{r ch010-background, include=FALSE, cache=FALSE}
box::use(
  dplyr[...],
  ggplot2[...],
  stringr[str_sub],
  GGally[ggpairs, wrap],
  knitr[kable]
)

load("data/example_genes.rda")
load("data/example_brca.rda")
d <- 1000
```


The `Bigsimr` package presented here provides multiple high-performance algorithms that operate with HD multivariate data. All these algorithms were originally designed to support a single task: to generate random vectors drawn from multivariate probability distributions with given marginal distributions and dependency metrics. Specifically, our goal is to efficiently simulate a large number, $B$, of HD random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with *correlated* components and heterogeneous marginal distributions, described via cumulative distribution functions (CDFs) $F_i, i=1,\ldots,d$.

When designing this methodology, we developed the following properties to guide our effort. We divide the properties into two categories: (1) basic properties (BP) and 'scalability' properties (SP). The BPs are adapted from an existing criteria due to @Nik13a. A suitable simulation strategy should possess the following properties:

\setstretch{1.5}

* BP1: A wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of dependence parameters can be equal to the number of bivariate pairs in the vector.
* BP3: Flexible marginal modeling, generating heterogeneous data --- including mixed continuous and discrete marginal distributions.

\setstretch{2.0}

Moreover, the simulation method must *scale* to high dimensions:

\setstretch{1.5}

* SP1: Procedure must scale to high dimensions with practical compute times.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.

\setstretch{2.0}


## Motivating example: RNA-seq data


Simulating HD, non-normal, correlated data motivates this work, in pursuit of modeling RNA-sequencing (RNA-seq) data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. The RNA-seq data-generating process involves counting how often a particular form of messenger RNA (mRNA) is expressed in a biological sample. RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in HD data. For human-derived samples, this results in count data corresponding to over 20,000 genes (protein-coding genomic regions) or even over 77,000 isoforms when alternatively-spliced mRNA are counted [@Schissler2019]. Importantly, due to inherent biological processes, gene expression data exhibit correlation (co-expression) across genes [@BE07; @Schissler2018]. 

We illustrate our methodology using the Breast Invasive Carcinoma (BRCA) data set housed in The Cancer Genome Atlas (TCGA; see Acknowledgments). For ease of modeling and simplicity of exposition, we only consider high expressing genes. In turn, we begin by filtering to retain the top `r d` of the highest-expressing genes (in terms of median expression) of the over 20,000 gene measurements from $N=`r nrow(example_brca)`$ patients' tumor samples. This gives a great number of pairwise dependencies among the marginals (specifically, $`r choose(d,2)`$ correlation parameters). Table \@ref(tab:ch010-realDataTab) displays RNA-seq counts for three selected high-expressing genes for the first five patients' breast tumor samples. To help visualize the bivariate relationships for these three selected genes across all patients, Figure \@ref(fig:ch010-realDataFig) displays the marginal distributions and estimated Spearman's correlations.


```{r ch010-realDataTab}
set.seed(2020-02-25)
num_genes    <- 3
num_patients <- 5
gene_sample  <- sample(example_genes[1:1000], num_genes)

cap <- paste0("mRNA expression for three selected high-expressing genes, ",
              paste(gene_sample, collapse = ", "), 
              ", for the first five patients in the TCGA BRCA data set.")

small_brca <- example_brca %>%
  select(all_of(gene_sample)) %>%
  head(n = num_patients) %>%
  as_tibble(rownames = "Patient ID") %>%
  mutate(`Patient ID` = str_sub(`Patient ID`, end = 12))

small_brca %>%
  kable(format = "latex", booktabs = TRUE, caption = cap)
```


```{r ch010-realDataFig, cache=FALSE, fig.width=8, fig.cap="Pairwise scatterplots for three example genes (Table 1), with estimated Spearman's correlations and marginal density estimates displayed. The data possess outliers, heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology."}
ggpairs(
  data = example_brca[, gene_sample],
  upper = list(continuous = wrap('cor', method = "spearman"))
) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5))
```


## Measures of dependency


In multivariate analysis, an analyst must select a metric to quantify dependency. The most widely-known is the Pearson correlation coefficient that describes the linear association between two random variables $X$ and $Y$, and, it is given by


\begin{equation}
\rho_P(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ \mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}}.
\label{eq:pearson}
\end{equation}


As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector, the Pearson correlation completely describes the dependency between the components. For non-normal marginals with monotone correlation patterns, however, $\rho_P$ suffers some drawbacks and may mislead or fail to capture important relationships [@MK01]. Alternatively, analysts often prefer rank-based correlation measures to describe the degree of monotonic association.

Two nonparametric, rank-based measures common in practice are Spearman's correlation (denoted $\rho_S$) and Kendall's $\tau$. Spearman's $\rho_S$ has an appealing correspondence as the Pearson correlation coefficient on *ranks* of the values, thereby capturing nonlinear yet monotone relationships. Kendall's $\tau$, on the other hand, is the difference in probabilities of concordant and discordant pairs of observations, $(X_i, Y_i)$ and $(X_j, Y_j)$ (concordance meaning that orderings have the same direction, e.g., if $X_i < X_j$, then $Y_i < Y_j$). Note that concordance is determined by the ranks of the values, not the values themselves. Both $\tau$ and $\rho_S$ are *invariant under monotone transformations* of the underlying random variates. As we will describe more fully in the [Algorithms](#algorithms) section, this property enables matching rank-based correlations with speed (SP1) and accuracy (SP2).


*Correspondence among Pearson, Spearman, and Kendall correlations*

There is no closed form, general correspondence among the rank-based measures and the Pearson correlation coefficient, as the marginal distributions $F_i$ are intrinsic in their calculation. For *bivariate normal vectors*, however, the correspondence is well-known:


\begin{equation}
\label{eq:convertKendall}
\rho_{P} = \sin \left( \tau \times \frac{\pi}{2} \right), 
\end{equation}


\noindent and similarly for Spearman's $\rho$ [@K58],


\begin{equation}
\label{eq:convertSpearman}
\rho_P = 2 \times \sin \left( \rho_S \times \frac{\pi}{6} \right).
\end{equation}


*Marginal-dependent bivariate correlation bounds*

Given two marginal distributions, $\rho_P$ is not free to vary over the entire range of possible correlations $[-1,1]$. The *Frechet-Hoeffding bounds* are well-studied [@Nelsen2007; @BF17]. These constraints cannot be overcome through algorithm design. In general, the bounds are given by

\begin{equation}
\label{eq:frechet}
\rho_P^{max} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho_P^{min} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where $U$ is a uniform random variable on $(0,1)$ and $F^{-1}_1, F^{-1}_2$ are the inverse CDFs of $X_1$ and $X_2$, respectively, definitely by \@ref(eq:inverseCDF) when the variables are discrete.

\begin{equation}
F_{i}^{-1} = \inf\{y:F_{i}(y) \geq u \}.
\label{eq:inverseCDF}
\end{equation}

## Gaussian copulas

There is a connection of our simulation strategy to Gaussian *copulas* [@Nelsen2007]. A copula is a distribution function on $[0,1]^d$ that describes a multivariate probability distribution with standard uniform marginals. This provides a powerful, natural way to characterize joint probability. Consequently, the study of copulas is an important and active area of statistical theory and practice. For any random vector ${\bf X}=(X_1, \ldots, X_d)^\top$ with CDF $F$ and marginal CDFs $F_i$ there is a copula function $C(u_1, \ldots, u_d)$ satisfying

\begin{equation}
F(x_1, \ldots, x_d) = {\mathbb P}(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), \quad x_i \in {\mathbb R}, i=1,\ldots,d.
\label{eq:copula}
\end{equation}

A Gaussian copula has marginal CDFs that are all standard normal, $F_i = \Phi, \forall \, i$, where $\Phi(\cdot)$ is the standard normal CDF. This representation corresponds to a multivariate normal (MVN) distribution with standard normal marginal distributions and covariance matrix $R_P$. Since the marginals are standardized to have unit variance, however, $R_P$ is a Pearson correlation matrix. If $F_R$ is the CDF of such a multivariate normal distribution, then the corresponding Gaussian copula $C_R$ is defined through


\begin{equation}
\label{eq:gauss}
F_R(x_1, \ldots, x_d) = C_R(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

Here the copula function $C_R$ is the familiar multivariate normal CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where $(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_P)$. 

Sklar's Theorem [@Sklar1959; @Ubeda-Flores2017] guarantees that given inverse CDFs $F_i^{-1}$s and a valid correlation matrix (within the Frechet bounds) a random vector can be obtained via transformations involving copula functions. For example, using Gaussian copulas, we can construct a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)^\top$ with $Y_i \sim F_i$, viz. $Y_i = F_i^{-1}(\Phi(X_i)), i=1, \ldots, d$, where $(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_P)$.

## Other Multivariate Simulation Packages

To place `Bigsimr`'s implementation and algorithms in context with existing approaches, here we qualitatively compare to other multivariate simulation packages.  In `R`, the `MASS` package `mvnorm()` has long been available to produce random vectors from multivariate normal (and $t$) distributions. This procedure cannot simulate from arbitrary margins and dependency measures, as well as fails to readily scale to high dimensions. SAS PROC SIMNORMAL and Python's NumPy `random.multivariate_normal` both do similar tasks. Recently, the high-performance `R` package `mvnfast` [@Fasiolo2016] has been released that provides HD multivariate normal generation, but this lacks flexibility in marginal models and dependency modeling. For dependent discrete data, LD `R` packages exist, such as `GenOrd`. For copula computation, the full-featured `R` package `copula` provides many different copulas and a fairly general LD random generator `rCopula()`. High-dimesional random vectors via `copula` takes an impractical amount of computing time [@Li2019gpu]. The `nortaRA` R package matches Pearson coefficients near exactly and is reasonable for LD settings. Finally, we know of no other `R` package that can produce HD random vectors with flexible margins and dependence modeling. Table \@ref(tab:compare-table) summarizes the above discussion.

\begin{table}[h]
\centering
\caption{\label{tab:compare-table}Comparison of selected features of \pkg{Bigsimr} to existing packages for multivariate simulation.}
\begin{tabular}{@{}llll@{}}
Package & Scales to HD & Hetereogenous Margins & Flexible Dependence \\ \midrule
MASS      & No           & No                    & No                  \\
mvnfast   & Yes          & No                    & No                  \\
GenOrd    & No           & Discrete margins      & Yes                 \\
copula    & No           & Yes                   & No                  \\
nortaRA   & No           & Yes                   & No                  \\
bigsimr   & Yes          & Yes                   & Yes                
\end{tabular}
\end{table}


\clearpage

