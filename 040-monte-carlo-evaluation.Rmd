# Monte Carlo evaluations {#simulations}

```{r ch040-LoadLib040, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
## devtools::install_github("SchisslerGroup/bigsimr", ref = 'develop')
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
# reticulate::use_condaenv("bigsimr")
library(bigsimr)
library(tidyverse)
cores <- as.integer( parallel::detectCores() - 1 )
## cores <- as.integer( 1 )
set.seed(10162020)
```

```{r ch040-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('040-monte-carlo-evaluation.Rmd')
```

Before applying our methodology to real data simulation, we conduct several Monte Carlo studies to investigate method performance in comparison to other existing implementations. 
We focus the numerical experiments on assessing how well the procedure scales to high dimension with respect to reasonable computation times (property S1 above) and accurately matching marginal and dependency parameters. 
The simulations will proceed in increasing complexity --- leading up to the setting in our motivating example. 
We begin by exploring simple exchangeable (constant) correlation structures under a large number of simulation replicates while applying the algorithm above to first continuous then discrete marginal distributions to test the robustness of our algorithm since exact matching is not guaranteed.

## Bivariate experiments

*Bivariate Normal*.
Let's simulate a bivariate normal and check our correlation matching performance as N increases.
Here we have BVN( $\mu_1 = \mu_2 = 10, \rho_{type}$ ).
We vary $\rho$ across the entire possible range of correlations for each correlation type.

```{r ch040-biNormal, echo = FALSE, eval = TRUE}
mom_norm <- function(x) {
  m <- mean(x)
  s <- sd(x)
  list(mean = m, sd = s)
}

mu <- 10
sigma <- 1
margins <- alist(
    qnorm(mean = mu, sd = sigma),
    qnorm(mean = mu, sd = sigma)
)

type <- c("pearson", "spearman", "kendall")
## cores <- as.integer( parallel::detectCores() - 1 )
## n <- c(1e3, 1e4, 1e5)
n <- c(1e1, 1e2, 1e3)
adjustForDiscrete <- c(FALSE)

eps <- 1e-2
## grid_steps <- 100
grid_steps <- 10
sim_pars <- expand.grid(type = type, cores = cores, n = n,
                        stringsAsFactors = FALSE,
                        adjustForDiscrete = adjustForDiscrete)

res <- data.frame()
dir.create("./results/sims_norm", showWarnings = TRUE, recursive = TRUE)
for (i in 1:nrow(sim_pars)) {
    type <- sim_pars$type[i]
    cores <- sim_pars$cores[i]
    n <- sim_pars$n[i]
    adjustForDiscrete <- sim_pars$adjustForDiscrete[i]
    tmp_bounds <- cor_bounds(margins, type = type)
    cor_lo <- tmp_bounds$lower[1,2] + eps
    cor_hi <- tmp_bounds$upper[1,2] - eps
    cor_seq <- seq(cor_lo, cor_hi, length.out = grid_steps)
    ## rho = cor_seq[1]
    for (rho in cor_seq) {
        Rho <- matrix(rho, 2, 2)
        diag(Rho) <- 1.0
        ## Rho <- convertCor( rho = Rho, from = type, to = 'pearson' )
        time_data <- system.time({
            ## x <- rmvn(n = n, mu = rep( mu, 2 ), sigma = Rho)
            ## use rvec to test our software
            x <- rvec(n = n,
                      rho = Rho,
                      margins = margins,
                      type = type,
                      cores = cores)
        })
        ## Save the sims in case
        id <- paste0(
            "d", 2,
            "-N", n,
            "-c", cores,
            "-r", rho,
            "-Cor", type,
            "-adj", as.character(adjustForDiscrete),
            "-dev", "1CORE",
            "-lib", "bigsimr"
        )
        ## save sims in their own subdirectory
        saveRDS(x, file = paste0("./results/sims_norm/", id, ".rds"))

        ## Estimate statistics
        Rho_hat <- cor_fast(x, method = type)
        rho_hat <- Rho_hat[1, 2]
        norm_args_hat <- mom_norm(x[,1])
        mu_hat <- norm_args_hat$mean
        sigma_hat <- norm_args_hat$sd

        ## Save the results
        res <- rbind(res, data.frame(
                              method = "bigsimr",
                              device = "CPU",
                              type = type,
                              cores = cores,
                              margins = "norm",
                              adjustForDiscrete = adjustForDiscrete,
                              d = 2,
                              N = n,
                              rho = rho,
                              rho_hat = rho_hat,
                              mean = mu,
                              sd = sigma,
                              mean_hat = mu_hat,
                              sd_hat = sigma_hat,
                              sim_time = unname(time_data["elapsed"])
                          ))
    }
}
res$type <- factor(res$type, levels = c("pearson",  "spearman", "kendall" ) )
saveRDS(object = res, "./results/norm_sims.rds")
```

```{r ch040-biNormPlot, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, fig.height = 5, fig.width= 8, fig.align='center', fig.cap = "`bigsimr` recovers the Pearson specified correlations for MVN."}
dat <- readRDS("./results/norm_sims.rds")
dat %>%
    filter(cores == cores) %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    ## ggplot(aes(rho, rho_hat)) +
    geom_point() +
    geom_abline(slope = 1) +
    ## facet_wrap(~ + N) + theme_bw()
    facet_wrap(~ type + N) + theme_bw()
    
## ggsave('fig/plot-biNormPlot.pdf')
```

*Bivariate Gamma*. 
Similarly, let's check the performance for a non-symmetric continuous distribution: a standard (rate =1) bivariate gamma.
Here we have a Bivariate Gamma with $shape_1 = shape_2 = 10, \rho_{type}$.
We vary $\rho$ across the entire possible range of correlations for each correlation type.

```{r ch040-biGamma, echo = FALSE, eval = TRUE}
mom_gamma <- function(x) {
  m <- mean(x)
  s <- sd(x)
  list(shape = m^2 / s^2, rate = m / s^2)
}

shape <- 10
rate <- 1
margins <- alist(
    qgamma(shape = shape, rate = rate),
    qgamma(shape = shape, rate = rate)
)

type <- c("pearson", "spearman", "kendall")
## cores <- c(1L)
## n <- c(1e3, 1e4, 1e5)
n <- c(1e1, 1e2, 1e3)
adjustForDiscrete <- c(FALSE)

eps <- 1e-2
## grid_steps <- 100
grid_steps <- 10
sim_pars <- expand.grid(type = type, cores = cores, n = n,
                        stringsAsFactors = FALSE,
                        adjustForDiscrete = adjustForDiscrete)


dir.create("./results/sims_gamma", showWarnings = TRUE, recursive = TRUE)
res <- data.frame()
for (i in 1:nrow(sim_pars)) {
    
    type <- sim_pars$type[i]
    cores <- sim_pars$cores[i]
    n <- sim_pars$n[i]
    adjustForDiscrete <- sim_pars$adjustForDiscrete[i]

    tmp_bounds <- cor_bounds(margins, type = type)
    cor_lo <- tmp_bounds$lower[1,2] + eps
    cor_hi <- tmp_bounds$upper[1,2] - eps
    cor_seq <- seq(cor_lo, cor_hi, length.out = grid_steps)
    

    ## rho = cor_seq[1]
    for (rho in cor_seq) {
        Rho <- matrix(rho, 2, 2)
        diag(Rho) <- 1.0

        time_data <- system.time({
            x <- rvec(n = n,
                      rho = Rho,
                      margins = margins,
                      cores = cores,
                      type = type,
                      ensure_PSD = TRUE)
        })

        ## Save the sims in case
        id <- paste0(
            "d", 2,
            "-N", n,
            "-c", cores,
            "-r", rho,
            "-Cor", type,
            "-adj", as.character(adjustForDiscrete),
            "-dev", "CPU",
            "-lib", "bigsimr"
        )
        saveRDS(x, file = paste0("./results/sims_gamma/", id, ".rds"))

        ## Estimate statistics
        Rho_hat <- cor_fast(x, method = type)
        rho_hat <- Rho_hat[1, 2]
        gamma_args_hat <- mom_gamma(x[,1])
        shape_hat <- gamma_args_hat$shape
        rate_hat <- gamma_args_hat$rate

        ## Save the results
        res <- rbind(res, data.frame(
                              method = "bigsimr",
                              device = "CPU",
                              type = type,
                              cores = cores,
                              margins = "gamma",
                              adjustForDiscrete = adjustForDiscrete,
                              d = 2,
                              N = n,
                              rho = rho,
                              rho_hat = rho_hat,
                              shape = shape,
                              rate = rate,
                              shape_hat = shape_hat,
                              rate_hat = rate_hat,
                              sim_time = unname(time_data["elapsed"])
                          ))
    }
}
res$type <- factor(res$type, levels = c("pearson",  "spearman", "kendall" ) )
saveRDS(object = res, "./results/sims_gamma.rds")
```

```{r ch040-biGammaPlot, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, fig.height = 5, fig.width= 8, fig.align='center', fig.cap = "`bigsimr` recovers the Pearson specified correlations for Bivariate Gamma."}
dat <- readRDS("./results/sims_gamma.rds")
dat %>%
    filter(cores == cores) %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    ## ggplot(aes(rho, rho_hat)) +
    geom_point() +
    geom_abline(slope = 1) +
    ## facet_wrap(~ + N) + theme_bw()
    facet_wrap(~ type + N) + theme_bw()
    
## ggsave('fig/plot-biGammaPlot.pdf')
```

*Bivariate Negative Binomial*. 
Let's check the performance for a discrete distribution: a bivariate negative binomial.
Here we have Bivariate Negative Binomial ( $prob_1 = prob_2 = 0.5, size_1 = size_2 = 4,\rho_{type}$ ).
We vary $\rho$ across the entire possible range of correlations for each correlation type.

```{r ch040-biNegBin, echo = FALSE, eval = TRUE}
mom_nbinom <- function(x) {
  m <- mean(x)
  s <- sd(x)
  list(size = m^2 / (s^2 - m), prob = m / s^2)
}

size <- 10
prob <- 0.1
margins <- alist(
    qnbinom(size = size, prob = prob),
    qnbinom(size = size, prob = prob)
)

type <- c("pearson", "spearman", "kendall")
## cores <- cores
## n <- c(1e3, 1e4, 1e5)
n <- c(1e1, 1e2, 1e3)
adjustForDiscrete <- c(FALSE)

eps <- 1e-2
## grid_steps <- 100
grid_steps <- 10
sim_pars <- expand.grid(type = type, cores = cores, n = n,
                        stringsAsFactors = FALSE,
                        adjustForDiscrete = adjustForDiscrete)


dir.create("./results/nbinom_sims", showWarnings = TRUE, recursive = TRUE)
res <- data.frame()
for (i in 1:nrow(sim_pars)) {
    type <- sim_pars$type[i]
    cores <- sim_pars$cores[i]
    n <- sim_pars$n[i]
    adjustForDiscrete <- sim_pars$adjustForDiscrete[i]
    ## AGS. 17 Oct 2020. cor_bounds failed.
    ## I'll create a minimal working example and open an issue
    ## tmp_bounds <- cor_bounds(margins, type = type)
    ## cor_lo <- tmp_bounds$lower[1,2] + eps
    ## cor_hi <- tmp_bounds$upper[1,2] - eps
    cor_lo <- -0.75
    cor_hi <- 0.95
    cor_seq <- seq(cor_lo, cor_hi, length.out = grid_steps)

    for (rho in cor_seq) {
        Rho <- matrix(rho, 2, 2)
        diag(Rho) <- 1.0

        time_data <- system.time({
            x <- rvec(n = n,
                      rho = Rho,
                      margins = margins,
                      cores = cores,
                      type = type,
                      ensure_PSD = TRUE)
        })

        ## Save the sims in case
        id <- paste0(
            "d", 2,
            "-N", n,
            "-c", cores,
            "-r", rho,
            "-Cor", type,
            "-adj", as.character(adjustForDiscrete),
            "-dev", "CPU",
            "-lib", "bigsimr"
        )
        saveRDS(x, file = paste0("./results/nbinom_sims/", id, ".rds"))

        ## Estimate statistics
        Rho_hat <- cor_fast(x, method = type)
        rho_hat <- Rho_hat[1, 2]
        nbinom_args_hat <- mom_nbinom(x[,1])
        size_hat <- nbinom_args_hat$size
        prob_hat <- nbinom_args_hat$prob

        ## Save the results
        res <- rbind(res, data.frame(
                              method = "bigsimr",
                              device = "CPU",
                              type = type,
                              cores = cores,
                              margins = "nbinom",
                              adjustForDiscrete = adjustForDiscrete,
                              d = 2,
                              N = n,
                              rho = rho,
                              rho_hat = rho_hat,
                              size = size,
                              prob = prob,
                              size_hat = size_hat,
                              prob_hat = prob_hat,
                              sim_time = unname(time_data["elapsed"])
                          ))
    }
}
res$type <- factor(res$type, levels = c("pearson",  "spearman", "kendall" ) )
saveRDS(object = res, "./results/nbinom_sims.rds")
```

```{r ch040-biNegBinPlot, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, fig.path='fig/plot-', dev='png', fig.ext='png', fig.align='center', fig.width = 8, fig.height=5,fig.cap = "`bigsimr` recovers the correlations for bivariate negative binomial only approximately for Pearson but (nearly) exactly for the rank-based correlations."}
dat <- readRDS("./results/nbinom_sims.rds")
dat %>%
    filter(cores == cores) %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    ## ggplot(aes(rho, rho_hat)) +
    geom_point() +
    geom_abline(slope = 1) +
    ## facet_wrap(~ + N) + theme_bw()
    facet_wrap(~ type + N) + theme_bw()
## ggsave('fig/plot-biNegBinPlot.pdf')
```

## Scale up to Ultra-High Dimensions

```{r ch040-gpuVScpuFig, echo=F, out.width='80%', fig.align='center', fig.cap="Computation times as d increases. We filter to the top 1, 5, 10, 15, 20, 25\\% expressing genes (in terms of median expression.)"}
knitr::include_graphics("fig/cpu-gpu-times.png")
```
