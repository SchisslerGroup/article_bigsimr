--- 
title: "Simulating High-Dimensional Multivariate Data"
subtitle: "using the `Bigsimr` Package"
author:
  - Alfred G. Schissler*
  - Edward J. Bedrick
  - Alexander D. Knudson
  - Tomasz J. Kozubowski
  - Tin Nguyen
  - Anna K. Panorska
  - Juli Petereit
  - Walter W. Piegorsch
  - Duc Tran
site: bookdown::bookdown_site
bibliography: ["bigsimr.bib", "packages.bib", "alex.bib"]
abstract:  It is critical to realistically simulate data when employing Monte Carlo techinques and evaluating statistical methodology. Measurements are often correlated and high dimensional in this era of big data, such as data obtained through high-throughput biomedical experiments. Due to computational complexity and a lack of user-friendly software available to simulate these massive multivariate constructions, researchers resort to simulation designs that posit independence or perform arbitrary data transformations. This article introduces the `bigsimr` R package. This high-level package provides a flexible and scalable procedure to simulate high-dimensional random vectors with arbitrary marginal distributions and known Pearson, Spearman, or Kendall correlation matrix. `bigsimr` contains additional high-performance features, including multi-core algorithms to simulate random vectors, estimate correlation, and compute the nearest correlation matrix. Monte carlo studies quantify the accuracy and scalability of our approach, up to $d=10,000$. Finally, we demonstrate example applications enabled via `bigsimr` by applying these methods to a motivating dataset --- RNA-sequencing data obtained from breast cancer tumor samples.
---
```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```

*Keywords:*  Multivariate simulation, High-dimensional data, Nonparametric correlation, Gaussian copula, RNA-sequencing data, breast cancer

*=Corresponding author. aschissler@unr.edu. 1664 N Virginia St, Reno, NV 89557

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Introduction

Massive high-dimensional (HD) data sets are now commonplace in many areas of scientific inquiry. As new methods are developed for data, a fundamental challenge lies in designing and conducting simulation studies to assess the operating characteristics of analyzing such proposed methodology --- such as false positive rates, statistical power, interval coverage, and robustness --- often with comparison to existing methods. Further, efficient simulation empowers statistical computing strategies, such as the parametric bootstrap [@Chernick2008] to simulate from a hypothesized null model, providing inference in analytically challenging settings. Such Monte Carlo (MC) techniques become difficult for high-dimensional data using existing algorithms and tools. This is particularly true when simulating massive multivariate, non-normal distributions, arising naturally in many fields of study.

As many have noted, it can be vexing to simulate dependent, non-normal/discrete data, even for low dimensional settings [@MB13; @XZ19]. For continuous non-normal multivariate data, the well-known NORmal To Anything (NORTA) algorithm [@Cario1997] and other copula approaches [@Nelsen2007] are well-studied, with flexible, robust software available [@Yan2007; @Chen2001]. Yet these approaches do not scale in a timely fashion to high-dimensional problems [@Li2019gpu]. For discrete data, early simulation strategies had major flaws, such as failing to obtain the full range of possible correlations (e.g., admitting only positive correlations: see @Park1996). While more recent approaches [@MB13; @Xia17; @BF17] have largely remedied this issue for low-dimensional problems, the existing tools are not designed to scale to high dimensions.


Another central issue lies in characterizing dependence between components in the high-dimensional random vector. The choice of correlation in practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g., non-normal, discrete, infinite support, etc). For normal data, the Pearson product-moment correlation describes the dependency perfectly. As we will see, however, simulating arbitrary random vectors that match a target Pearson correlation matrix is computationally intense [@Chen2001; @Xia17]. On the other hand, an analyst might consider use of nonparametric correlation measures to better characterize monotone, non-linear dependence, such as Spearman's $\rho$ and Kendall's $\tau$. Throughout, we focus on matching these nonparametric dependence measures, as our aim lies in modeling non-normal data and these rank-based measures possess invariance properties favorable in our proposed methodology. We do, however, provide Pearson matching with some caveats.


With all this in mind, we present a scalable, flexible multivariate simulation algorithm. The crux of the method lies in the construction of a Gaussian copula in the spirit of the NORTA procedure. Further, we introduce the `bigsimr` R package that provides high-performance software implementing our algorithm. The algorithm design leverages useful properties of nonparametric correlation measures, namely invariance under monotone transformation and well-known closed-form relationships between dependence measures for the multivariate normal (MVN) distribution. 


Our study proceeds by providing background information, including a description of a motivating example application: RNA-sequencing (RNA-seq) breast cancer data. Then we describe and justify our simulation methodology and related algorithms. Next, we detail an illustrative low-dimensional example of basic use of the `bigsimr` R package. Then we proceed with Monte Carlo studies under various bivariate distributional assumptions to assess accuracy. We conclude the Monte Carlo evaluations by summarizing the computation time for increasingly higher dimensional vectors. After the MC evaluations, we simulate random vectors motivated by our RNA-seq example, evaluate the accuracy, and provide example statistical computing tasks, namely MC estimation of joint probabilities and evaluating HD correlation estimation efficiency. Finally, we discuss the method's utility, limitations, and future directions.
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Background {#background}

```{r ch010-background, include=FALSE, cache=FALSE}
box::use(
  dplyr[...],
  ggplot2[...],
  stringr[str_sub],
  GGally[ggpairs, wrap],
  knitr[kable]
)
load("data/example_genes.rda")
load("data/example_brca.rda")
d <- 1000
```


The `bigsimr` `R` package presented here provides multiple algorithms that operate with high-dimensional multivariate data; however, all these algorithms were originally designed to support a single task: to generate random vectors drawn from multivariate probability distributions with given marginal distributions and dependency metrics. Specifically, our goal is to efficiently simulate a large number, $B$, of HD random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with *correlated* components and heterogeneous marginal distributions, described via cumulative distribution functions (CDFs) $F_i$.


When designing this methodology, we developed the following properties to guide our effort. We divide the properties into two categories: (1) basic properties (BP) and "scalability" properties (SP). The BPs are adapted from an existing criteria due to @Nik13a. A suitable simulation strategy should possess the following properties:


* BP1: A wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of bivariate marginals can be equal to the number of dependence parameters.
* BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.


Moreover, the simulation method must *scale* to high dimensions:


* SP1: Procedure must scale to high dimensions with practical compute times.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.


## Motivating example: RNA-seq data


Simulating high-dimensional, non-normal, correlated data motivates this work --- in pursuit of modeling RNA-sequencing (RNA-seq) data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. The RNA-seq data-generating process involves counting how often a particular form of human messenger RNA (mRNA) is expressed in a biological sample. RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in high-dimensional data. For human-derived samples, this results in count data corresponding to over 20,000 genes (protein-coding genomic regions) or even over 77,000 isoforms when alternatively-spliced mRNA are counted [@Schissler2019]. Importantly, due to inherent biological processes, gene expression data exhibit correlation (co-expression) across genes [@BE07; @Schissler2018]. 


We illustrate our methodology using a well-studied Breast Invasive Carcinoma (BRCA) data set housed in The Cancer Genome Atlas (TCGA; see Acknowledgments).
For simplicity, we only consider high expressing genes. In turn, we begin by filtering to retain the top `r d` of the highest-expressing genes (in terms of median expression) of the over 20,000 gene measurements from $N=`r nrow(example_brca)`$ patients' tumor samples. This gives a great number of pairwise dependencies among the marginals (specifically, $`r choose(d,2)`$ correlation parameters). Table \@ref(tab:ch010-realDataTab) displays RNA-seq counts for three selected high-expressing genes for the first five patients' breast tumor samples. 
 
 
```{r ch010-realDataTab}
set.seed(2020-02-25)
num_genes    <- 3
num_patients <- 5
gene_sample  <- sample(example_genes[1:1000], num_genes)

cap <- paste0("mRNA expression for three selected high-expressing genes, ",
              paste(gene_sample, collapse = ", "), 
              ", for the first five patients in the TCGA BRCA data set.")

small_brca <- example_brca %>%
  select(all_of(gene_sample)) %>%
  head(n = num_patients) %>%
  as_tibble(rownames = "Patient ID") %>%
  mutate(`Patient ID` = str_sub(`Patient ID`, end = 12))

small_brca %>%
  kable(booktabs = TRUE, caption = cap)
```


To help visualize the bivariate relationships for these three selected genes across all patients, Figure \@ref(fig:ch010-realDataFig) displays the marginal distributions and estimated Spearman's correlations (see Equation \@ref(eq:spearman) below).


```{r ch010-realDataFig, cache=FALSE, fig.width=8, fig.cap="Marginal scatterplots, densities, and estimated pairwise Spearman's correlations for three example genes from Table 1. The data possess heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology."}
ggpairs(
  data = example_brca[, gene_sample],
  upper = list(continuous = wrap('cor', method = "spearman"))
) + theme_bw()
```


## Measures of dependency


In multivariate analysis, an analyst must select a metric to quantify dependency.
The most widely-known is the Pearson (product-moment) correlation coefficient that describes the linear association between two random variables $X$ and $Y$, and, it is given by


\begin{equation}
(\#eq:pearson)
\rho_P(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ \mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}}.
\end{equation}


As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector, the Pearson correlation completely describes the dependency between the components. For non-normal marginals with monotone correlation patterns, however, $\rho_P$ suffers some drawbacks and may mislead or fail to capture important relationships [@MK01]. Alternatively in these settings, analysts often prefer rank-based correlation measures to describe the degree of monotonic association.

Two nonparametric, rank-based measures common in practice are Spearman's correlation (denoted $\rho_S$) and Kendall's $\tau$. Spearman's $\rho_S$ has an appealing correspondence as the Pearson correlation coefficient on *ranks* of the values, thereby captures nonlinear yet monotone relationships. Kendall's $\tau$, on the other hand, is the difference in probabilities of concordant and discordant pairs of observations, $(X_i, Y_i)$ and $(X_j, Y_j)$, with concordance meaning that orderings have the same direction (e.g., if $X_i < X_j$, then $Y_i < Y_j$). Note that concordance is determined by the ranks of the values, not the values themselves.


Both $\tau$ and $\rho_S$ are *invariant under monotone transformations* of the underlying random variates. As we will describe more fully in the [Algorithms](#algorithms) section, this property enables matching rank-based correlations with speed (SP1) and accuracy (SP2).


*Correspondence among Pearson, Spearman, and Kendall correlations*

There is no closed form, general correspondence among the rank-based measures and the Pearson correlation coefficient, as the marginal distributions $F_i$ are intrinsic in their calculation. For *bivariate normal vectors*, however, the correspondence is well-known:


\begin{equation}
(\#eq:convertKendall)
\rho_{P} = \sin \left( \tau \times \frac{\pi}{2} \right), 
\end{equation}


\noindent and similarly for Spearman's $\rho$ [@K58],


\begin{equation}
(\#eq:convertSpearman)
\rho_P = 2 \times \sin \left( \rho_S \times \frac{\pi}{6} \right).
\end{equation}


*Discrete marginal considerations*

Spearman's correlation for discrete marginal suffers issues whenever there is a nonzero probability of ties. For example, the Spearman's correlation of a discrete-valued random variable $X$ with itself could be less than 1 [@MB13]. One remedy for this issue is to rescale Equation \@ref(eq:spearman). For two random variables $X,Y$ with probability mass functions (PMFs) or probability densities functions (PDFs) $p(x)$ and $q(y)$, respectively, define the rescaled Spearman's correlation as


\begin{equation}
(\#eq:spearmanRescaled)
\rho_{RS}(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}.
\end{equation}


Note that with continuous marginals the rescaling returns $\rho_S$. For discrete marginals with large or infinite support, computing the adjustment factors $\sum_x p(x)^3$, $\sum_y p(y)^3$ over all large number of pairs becomes expensive (often violating desired property SP1). Further, the infinite sums must be approximated for count-valued data.


Also, for a discrete random variable $Y_i$, some care must be taken to define the quantile function $F_{i}^{-1}$. A standard practice is to set


\begin{equation}
F_{i}^{-1} = \inf\{y:F_{i}(y) \geq u \}.
(\#eq:inverseCDF)
\end{equation}


*Marginal-dependent bivariate correlation bounds*

Given two marginal distributions, $\rho_P$ is not free to vary over the entire range of possible correlations $[-1,1]$. The so-called *Frechet-Hoeffding bounds* are well-studied [@Nelsen2007; @BF17]. These constraints cannot be overcome through algorithm design. In general, the bounds are given by


\begin{equation}
(\#eq:frechet)
\rho_P^{max} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho_P^{min} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}


\noindent where $U$ is a uniform random variable on $(0,1)$ and $F^{-1}_1, F^{-1}_2$ are the inverse CDFs of $X_1$ and $X_2$, respectively, definitely by \@ref(eq:inverseCDF) when the variables are discrete.


## Gaussian copulas


There is a strong connection of our simulation strategy to Gaussian *copulas* (see @Nelsen2007 for a technical introduction). A copula is a distribution function on $[0,1]^d$ that describes a multivariate probability distribution with standard uniform marginals. This provides a powerful, natural way to characterize joint probability structures. Consequently, the study of copulas is an important and active area of statistical theory and practice.


For any random vector ${\bf X}=(X_1, \ldots, X_d)^\top$ with CDF $F$ and marginal CDFs $F_i$ there is a copula function $C(u_1, \ldots, u_d)$ satisfying


\begin{equation}
(\#eq:copula)
F(x_1, \ldots,x_d) = \mathbb P(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), 
\end{equation}


$x_i \in \mathbb R, i=1,\ldots,d.$


A Gaussian copula has marginal CDFs that are all standard normal, $F_i = \Phi, \forall \, i$. This representation corresponds to a multivariate normal (MVN) distribution with standard normal marginal distributions and covariance matrix ${\bf R_P}$. As the marginals are standardized to have unit variance, however, ${\bf R_P}$ is a Pearson correlation matrix. If $F_{{\bf R}}$ is the CDF of such a multivariate normal distribution, then the corresponding Gaussian copula $C_{{\bf R}}$ is defined through


\begin{equation}
(\#eq:gauss)
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}


where $\Phi(\cdot)$ is the standard normal CDF. Note that the copula $C_{{\bf R}}$ is the familiar multivariate normal CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_P})$. 


Sklar's Theorem [@Sklar1959; @Ubeda-Flores2017] guarantees that given inverse CDFs $F_i^{-1}$s and a valid correlation matrix (within the Frechet bounds) a random vector can be obtained via transformations involving copula functions. For example, using Gaussian copulas, we can construct a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)^\top$ with $Y_i \sim F_i$, viz. $Y_i = F_i^{-1}(\Phi(X_i)), i=1, \ldots, d$, where $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_P})$.
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:010-background.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Algorithms {#algorithms}


This section describes our methods involved in simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,\ldots,d$. Each $Y_i$ has a specified marginal CDF $F_i$ and its inverse $F^{-1}_i$. To characterize dependency, every pair $(Y_i, Y_j)$ has a given Pearson correlation $\rho_P$, Spearman correlation $\rho_S$, and/or Kendall's $\tau$. The method can be described as a *high-performance Gaussian copula* (Equation \@ref(eq:gauss)) providing a high-dimensional NORTA-inspired algorithm.


## NORmal To Anything (NORTA)


The well-known NORTA algorithm [@Cario1997] simulates a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$. Specifically, NORTA algorithm proceeds as follows:


1. Simulate a random vector $\bf Z$ with $d$ *independent and identically distributed* (iid) standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ such that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_i^{-1}[\Phi(X_i)], \; i=1,...,d$.


With modern parallel computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multi-core and/or graphical-processing-unit (GPU) accelerated algorithms --- providing fast scalability using readily-available hardware.


Matching specified Pearson correlation coefficients exactly (step 2 above), however, is computationally costly. In general, there is no closed-form correspondence between the components of the input $\Sigma_{\bf Z}$ and target $\Sigma_{\bf Y}$. Matching the correlations involves evaluating or approximating $\binom{d}{2}$ integrals of the form 


\begin{equation}
    \mathrm{E}\left[Y_i Y_j\right] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} F_i^{-1}\left[\Phi(z_i)\right] F_j^{-1}\left[\Phi(z_j)\right] \phi(z_i, z_j, \rho_z) dz_i dz_j,
    (\#eq:pearsonIntegralRelation)
\end{equation}


where $\phi(\cdot)$ is the joint probability distribution function of two correlated standard normal variables. For HD data, (nearly) exact evaluation may become too costly to enable practical simulation studies. For low-dimensional problems, however, methods and tools exist to match Pearson correlations precisely; see [@Xia17] and the publicly available `nortaRA` R package [@Chen2001]. As described later, to enable HD Pearson matching we approximate these integrals.


*NORTA in higher dimensions*

Sklar's theorem provides a useful characterization of multivariate distributions through copulas. Yet the choice of copula-based simulation algorithm affects which joint distributions may be simulated. Even in low-dimensional spaces (e.g., $d=3$), there exist valid multivariate distributions with *feasible* Pearson correlation matrices that NORTA cannot match exactly [@LH75]. This occurs when the bivariate transformations are applied to find the input correlation matrix, yet when combined the resultant matrix is indefinite. These situations do occur, even using exact analytic calculations. Such problematic target correlation matrices are termed *NORTA defective*.


@GH02 conducted a Monte Carlo study to estimate the probability of encountering NORTA defective matrices while increasing the dimension $d$. They found that for what is now considered low-to-moderate dimensions ($d \approx 20$), almost *all* feasible matrices are NORTA defective. This stems from the concentration of measure near the boundary of the space of all possible correlation matrices as dimension increases. Unfortunately, it is precisely near this boundary that NORTA defective matrices reside.


There is hope, however, as @GH02 also showed that replacing an indefinite input correlation matrix with a close proxy will give approximate matching to the target --- with adequate performance for moderate $d$. This provides evidence that our nearest positive definite (PD) augmented approach will maintain reasonable accuracy if our input matching scheme returns an indefinite matrix, at least for the rank-based matching scheme described above.


## Random vector generator {#rand-vec-gen}


We now describe our algorithm to generate random vectors, which mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:


1. Mapping step
    (i) Convert the target Spearman correlation matrix $R_S$ to the corresponding MVN Pearson correlation $R_X$. Alternatively,  
    (i') Convert the target Kendall $\tau$ matrix $R_K$ to the corresponding MVN Pearson correlation $R_X$. Alternatively,  
    (i'') Convert the target Pearson correlation matrix to $R_P$ to the corresponding, approximate MVN Pearson correlation $R_X$.  
2. Check admissibility and, if needed, compute the nearest correlation matrix.
    (i) Check that $R_X$ is a correlation matrix, a positive definite matrix with 1's along the diagonal.   
	(ii) If $R_X$ is a correlation matrix, the input matrix is *admissible* in this scheme. Otherwise
    (ii$^\prime$) Replace $R_X$ with the nearest correlation matrix $\tilde{R}_X$, in the Frobenius norm.  
3. Gaussian copula
    (i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_X)$.  
    (ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz. $U_i=\Phi(X_i)$, $i=1, \ldots, d$.  
    (iii) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$.  


*Step 1*

The first two descriptions of the *Mapping step* employ the closed-form relationships between $\rho_S$ and $\tau$ with $\rho_P$ for bivariate normal random variables via Equations \@ref(eq:convertKendall) and \@ref(eq:convertSpearman), respectively (implemented as `cor_covert`). Initializing our algorithm to match the nonparametric correlations by computing these equations for all pairs is computationally trivial. 


For the computationally expensive process to match Pearson correlations, we approximate Equation \@ref(eq:pearsonIntegralRelation) for all pairs of margins. To this end, we implement the approximation scheme introduced by @xiao2019matching. Briefly, the many double integrals of the form in \@ref(eq:pearsonIntegralRelation) are approximated by weighted sums of Hermite polynomials. Matching coefficients for pairs of continuous distributions is made tractable by this method, but for discrete distributions (especially discrete distributions with large support sets or infinite support), the problem is not so simple or efficient.


Our solution is to approximate discrete distributions by a continuous distribution. The question then becomes that of which distribution to use. We found that Generalized S-Distributions [@muino2006gs] solve this problem by approximating a wide range of unimodal distributions, both continuous and discrete. Using the GS-Distribution approximation of discrete distributions in Pearson matching scheme above yields favorable results.


*Step 2*

Once $R_X$ has been determined, we check admissibility of the adjusted correlation via the steps described above. If $R_X$ is not a valid correlation matrix, then we compute the nearest correlation matrix. Finding the nearest correlation matrix is a common statistical computing problem. The defacto default function in `R` is `Matrix::nearPD`, an alternating projection algorithm due to @higham2002computing. As implemented the function fails to scale to HD. Instead, we provide the quadratically convergent algorithm based on the theory of strongly semi-smooth matrix functions (@qi2006quadratically). The nearest correlation matrix problem can be written down as the following convex optimization problem:


\begin{align*}
    \mathrm{min}\quad & \frac{1}{2} \Vert G - X \Vert^2 \\
    \mathrm{s.t.}\quad & X_{ii} = 1, \quad i = 1, \ldots , n, \\
    & X \in S_{+}^{n}
\end{align*}


For nonparametric correlation measures, our algorithm allows the generation of high-dimensional multivariate data with arbitrary marginal distributions with a broad class of admissible Spearman correlation matrices and Kendall $\tau$ matrices. The admissible classes consist of the matrices that map to a Pearson correlation matrix for a MVN. In particular, if we let $X$ be MVN with $d$ components and set


\begin{align*}
\Omega_P &= \{ R_P : R_P \textrm{ is a correlation matrix for } X \} \\
\Omega_K &= \{ R_K : R_K \textrm{ is a correlation matrix for } X \} \\
\Omega_S &= \{ R_S : R_S \textrm{ is a correlation matrix for } X \} \\
\end{align*}


then there are 1-1 mappings between these sets. We conjecture informally that the sets of admissible $R_S$ and $R_K$ are not highly restrictive. In particular, $R_P$ is approximately $R_S$ for a MVN, suggesting that the admissible set $\Omega_S$ should be flexible as $R_P$ can be any PD matrix with 1's along the diagonal. We provide methods to check whether a target $R_S$ is an element of the *admissible set* $\Omega_S$ (and similarly for $R_K$).


There is an increasing probability of encountering a non-admissible correlation matrix as dimension increases. In our experience, the mapping step for large $d$ almost always produces a $R_X$ that is not a correlation matrix. In Section \@ref(package) we provide a basic description of how to quantify and control the approximation error. Further, the RNA-seq example in Section \@ref(examples) provides an illustration of this in practice.


*Step 3* 

The final step implements a NORTA-inspired, Gaussian copula approach to produce the desired margins. Steps 1 and 2 determine the MVN Pearson correlation values that will eventually match the target correlation. Then all that is required is a fast MVN simulator, a standard normal CDF, and well-defined quantile functions for marginals. The MVN is transformed to a copula (distribution with standard uniform margins) by applying the normal CDF $\phi(\cdot)$. The quantile functions $F^{-1}$ are applied across the margins to return the desired random vector ${\bf Y}$.

```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:020-simulation-algorithm.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# The `bigsimr` R package {#package}


```{r ch030-basic-example-setup, echo=TRUE, cache=FALSE}
box::use(
  patchwork[...],
  dplyr[...],
  ggplot2[...],
  tidyr[drop_na],
  bigsimr[bigsimr_setup, distributions_setup],
  JuliaCall[julia_call]
)

Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
bs <- bigsimr_setup(pkg_check = FALSE)
dist <- distributions_setup()
```


This section describes a low-dimensional (2D) random vector simulation workflow via the `bigsimr` `R` package (https://github.com/SchisslerGroup/r-bigsimr).
The package `bigsimr` provides an interface to the native code written in Julia, registered as the `Bigsimr` Julia package (https://github.com/adknudson/Bigsimr.jl).
In addition to the native Julia `Bigsimr` package and `R` interface `bigsimr`, we also provide a python interface `bigsimr` (https://github.com/SchisslerGroup/python-bigsimr/) that interfaces with the Julia `Bigsimr` package. 
The Julia implementation provides a high-performance implementation of our proposed random vector generation algorithm and associated functions (see Section \@ref(algorithms)).
When designing `bigsimr`, we aimed to conveniently provide parallelized computation, through multi-core and GPU acceleration, while allowing advanced users to customize.

The subsections below describe the basic use of `bigsimr`, by stepping through an example workflow using the data set `airquality` that contains daily air quality measurements in New York, May to September 1973 [@Chambers1983].
This workflow proceeds from computing environment setup, data wrangling, estimation, simulation configuration, random vector generation, and, finally, result visualization.
The code chunk below prepares the computing environment:

<!-- 
`bigsimr` is an R package for simulating high-dimensional multivariate data with arbitrary marginal distributions. The efficiency behind generating multivariate samples comes from the ability to utilize a GPU during the generation of multivariate normal samples and the subsequent transformation to uniform samples (see section \@ref(rand-vec-gen)). A GPU is not necessary in order to use `bigsimr`, and the speed benefits generally only come when simulating very high dimensional data ($d > 10000$).

The next computational step after obtaining the correlated uniform marginals is the inverse transform into the target marginal distributions. Since this consists of $d$ independent transformations, we utilize parallelization to achieve higher throughput. As with many parallel algorithms, there is overhead associated with forking the task to utilize multiple cores, however cost is negligible compared to the total run-time.
-->

## Bivariate example description

We illustrate the use of `bigsimr` motivated by the New York air quality data set (`airquality`) included in the R `datasets` package.
First, we load the `bigsimr` library and a few other convenient data science packages, including the syntactically-elegant `tidyverse` suite of `R` packages.

For simplicity and to provide a minimal working example, we consider bivariate simulation of the two `airquality` variables `Temperature`, in degrees Fahrenheit, and `Ozone` level, in parts per billion.

```{r ch030-air-quality-data, echo=TRUE}
df <- airquality %>%
  select(Temp, Ozone) %>%
  drop_na()
```


```{r ch030-aq-glimpse}
glimpse(df)
```

Figure \@ref(fig:ch030-aq-joint-dist) visualizes the bivariate relationship between Ozone and Temperature. We aim to simulate random two-component vectors mimicking this structure. The margins are not normally distributed; particularly the Ozone level exhibits a strong positive skew.

```{r ch030-aq-joint-dist, cache=FALSE, fig.width= 8, fig.cap="Bivariate scatterplot of Ozone vs. Temp with estimated marginal densities. We model the Ozone data as marginally log-normal and the Temperature data as normal."}
p0 <- ggplot(df, aes(Temp, Ozone)) +
  geom_point(size = 1) +
  theme(legend.position = "none") + 
  labs(x = "Temperature")

pTemp <- ggplot(df, aes(Temp)) + 
  geom_density() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())

pOzone <- ggplot(df, aes(Ozone)) + 
    geom_density() +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  coord_flip()

pTemp + plot_spacer() + p0 + pOzone + 
  plot_layout(widths = c(3,1), heights = c(1, 3)) 
```

Next, we specify the marginal distributions and correlation coefficient (both type and magnitude). Here the analyst is free to be creative. For this example, we avoid goodness-of-fit considerations to determine the marginal distributions. But it seems sensible without domain knowledge to estimate these quantities from the data and `bigsimr` contains fast functions designed for this task.


## Specifying marginal distributions 

Based on the estimated densities in Figure \@ref(fig:ch030-aq-joint-dist), we assume `Temp` is normally distributed and `Ozone` is log-normally distributed, as the latter values are positive and skewed. We use the well-known, unbiased estimators for the normal distribution's parameters and maximum likelihood estimators for the log-normal parameters:


```{r ch030-aq-temp-pars, echo=TRUE}
df %>% 
  select(Temp) %>% 
  summarise_all(.funs = c(mean = mean, sd = sd))
```


```{r ch030-aq-ozone-pars, echo=TRUE}
mle_mean <- function(x) mean(log(x))
mle_sd <- function(x) mean( sqrt( (log(x) - mean(log(x)))^2 ) )

df %>% 
  select(Ozone) %>% 
  summarise_all(.funs = c(meanlog = mle_mean, sdlog = mle_sd))
```

Next, we configure the input marginals for later input into `rvec`. The marginal distributions are specified using `Julia`'s `Distributions` package and stored in a vector.

```{r ch030-margins-alist, echo=TRUE}
margins <- c(
  dist$Normal(mean(df$Temp), sd(df$Temp)),
  dist$LogNormal(mle_mean(df$Ozone), mle_sd(df$Ozone))
)
```

## Specifying correlation

As mentioned, the user must decide how to describe correlation, based on the particulars of the problem. For non-normal data and for improved simulation accuracy/scalability in our scheme, we advocate the use of Spearman's $\rho$ correlation matrix $R_S$ and Kendall's $\tau$ correlation matrix $R_K$. We also support Pearson correlation coefficient matching, while cautioning the user to check the performance for their parametric multivariate model (see [Monte Carlo evaluations](#simulations) below for evaluation strategies and guidance). These estimation methods are classical approaches, not designed for high-dimensional correlation estimation (see the [Conclusion and Discussion]({#discussion) sections for more on this).

```{r ch030-aq-cor, echo=TRUE}
(R_S <- bs$cor(as.matrix(df), bs$Spearman))
```

## Checking target correlation matrix admissibility

First we use `cor_bounds` to ensure that the pairwise target correlation values are valid prior to mapping step which constructs $R_X$ for the MVN input into NORTA (see Section \@ref(algorithms)). `cor_bounds` estimates the pairwise lower and upper correlation bounds using the Generate, Sort, and Correlate algorithm of @DH2011.

```{r ch030-cor-bounds, echo=TRUE}
bounds <- bs$cor_bounds(margins)
bounds$lower
```

Since our single estimated Spearman correlation is within the theoretical bounds, the correlation is valid as input to `rvec`. But even if the 2-dimensional bounds are satisfied for each pair of margins, this does not guarantee the feasibility of a $d-$variate distribution [@BF17].

To provide higher dimensional feasibility/admissibility checking, we begin by mapping using `cor_convert` and check admissibility.


```{r ch030-cor-convert, echo=TRUE}
# Step 1. Mapping
(R_X <- bs$cor_convert(R_S, bs$Spearman, bs$Pearson))

# Step 2. Check admissibility
julia_call("Bigsimr.iscorrelation", R_X)
```


The bounds on the Pearson correlation coefficient $R_P$ between these margins are restricted as seen below in our MC estimated correlation bounds:


```{r ch030-cor-bounds-pearson, echo=TRUE}
bs$cor_bounds(margins[1], margins[2], bs$Pearson, n_samples = 1e6)
```


Our MC estimate of the bounds slightly underestimates the theoretic bounds of $(-0.881, 0.881)$. An analytic derivation is presented as an Appendix.


## Simulating random vectors
Finally, we arrive at the main function of `bigsimr`, `rvec`. We now simulate $B=10,000$ random vectors from the assumed joint distribution of Ozone levels and Temp.


```{r ch030-sim-margins, echo=TRUE}
x <- bs$rvec(10000, R_X, margins)
df_sim <- as.data.frame(x)
colnames(df_sim) <- colnames(df)
```


Figure \@ref(fig:ch030-plot-sim) plots the 10,000 simulated points.


```{r ch030-plot-sim, fig.cap="Contour plot and marginal densities for the simulated bivariate distribution of Air Quality Temperatures and Ozone levels. The simulated points mimic the observed data with respect to both the marginal characteristics and bivariate association."}
p1 <- df_sim %>%
  ggplot(aes(Temp, Ozone)) +
  geom_density_2d_filled() +
  theme(legend.position = "none") +
  labs(x = "Simulated Temperature", y = "Simulated Ozone") +
  scale_y_continuous(limits = c(0, max(df$Ozone))) +
  scale_x_continuous(limits = range(df$Temp))

p1Temp <- ggplot(df_sim, aes(Temp)) + 
  geom_density(alpha = 0.5, fill = "lightseagreen") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())

p1Ozone <- ggplot(df_sim, aes(Ozone)) + 
  geom_density(alpha = 0.5, fill = "lightseagreen") + 
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  coord_flip() 

p1Temp + plot_spacer() + p1 + p1Ozone + 
  plot_layout(widths = c(3,1), heights = c(1, 3))
```
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:030-bigsimr-package.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Monte Carlo evaluations {#simulations}


```{r ch040-LoadLib040, include=FALSE}
box::use(
    dplyr[...],
    ggplot2[...],
    patchwork[...],
    tidyr[pivot_longer],
    bigsimr[bigsimr_setup, distributions_setup],
    knitr[kable],
    RColorBrewer[brewer.pal]
)
load("data/bivariate_normal_sims.rda")
load("data/bivariate_gamma_sims.rda")
load("data/bivariate_nbinom_sims.rda")
load("data/benchmark_dependences.rda")

Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
bs <- bigsimr_setup(pkg_check = FALSE)
dist <- distributions_setup()
prob <- 3e-04
```


Before applying our methodology to real data simulation, we conduct several Monte Carlo studies to investigate method performance. Since marginal parameter matching in our scheme is essentially a sequence of univariate inverse probability transforms, the challenging aspects are the accuracy of dependency matching and computational efficiency at high dimensions. To evaluate our methods in those respects, we design the following numerical experiments to first assess accuracy of matching dependency parameters in bivariate simulations and then time the procedure in increasingly large dimension $d$.


## Bivariate experiments


We select bivariate simulation configurations to ultimately simulate our motivating discrete-valued RNA-seq example, and, so we proceed in increasing complexity, leading to the model in our motivating application in Section \@ref(examples). We begin with empirically evaluating the dependency matching across all three supported correlations --- Pearson's, Spearman's, and Kendall's --- in identical, bivariate marginal configurations. For each pair of identical margins, we vary the target correlation across $\Omega$, the set of possible admissible values for correlation type, to evaluate the simulation's ability to obtain the theoretic bounds. The simulations progress from bivariate normal, to bivariate gamma (non-normal yet continuous), and bivariate negative binomial (mimicking RNA-seq counts).


Table \@ref(tab:sims) lists our identical-marginal, bivariate simulation configurations. We increase the simulate replicates $B$ to check that our results converge to the target correlations and gauge statistical efficiency. We select distributions beginning with a standard multivariate normal (MVN) as we expect the performance to be exact (up to MC error) for all correlation types. Then, we select a non-symmetric continuous distribution: a standard (rate =1) two-component multivariate gamma (MVG). Finally, we select distributions and marginal parameter values that are motivated by our RNA-seq data, namely values proximal to probabilities and sizes estimated from the data (see [Example applications](examples) for estimation details). Thus we arrive at a multivariate negative binomial (MVNB) $p_1 = p_2 = 3\times10^{-4}, r_1 = r_2 = 4, \rho \in \Omega$.


Table: (\#tab:sims) Identical margin, bivariate simulation configurations to evaluate correlation matching accuracy and efficiency.


| Simulation Reps ($B$) | Correlation Types | Identical-margin 2D distribution |
|-------------|:--------------:|----------------------:|
| $1000$    | Pearson ($\rho_P$) | ${ \bf Y} \sim MVN( \mu= 0 , \sigma = 1, \rho_i ), i=1,\ldots,100$ |
| $10,000$  | Spearman ($\rho_S$) | ${ \bf Y} \sim MVG( shape = 10, rate = 1, \rho_i ), i=1,\ldots,100$ |
| $100,000$ | Kendall ($\tau$)| ${ \bf Y} \sim MVNB(p = 3\times10^{-4}, r = 4,\rho_i), i=1,\ldots,100$ |


For each of the unique 9 simulation configurations described above, we estimate the correlation bounds and vary the correlations along a sequence of 100 points evenly placed within the bounds, aiming to explore $\Omega$. Specifically, we set correlations $\{ \rho_1 = ( \hat{l} + \epsilon), \rho_2 = (\hat{l} + \epsilon) + \delta, \ldots, \rho_{100} = (\hat{u} - \epsilon) \}$, with $\hat{l}$ and $\hat{u}$ being the estimated lower and upper bounds, respectively, and increment value $\delta$. The adjustment factor, $\epsilon=0.01$, is introduced to handle numeric issues when the bound is specified exactly.


```{r ch040-biNormPlot, eval=FALSE, fig.height=5, fig.width=8, fig.cap="`bigsimr` recovers the Pearson specified correlations for MVN."}
bivariate_normal_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-biGammaPlot, eval=FALSE, fig.height=5, fig.width=8, fig.cap="`bigsimr` recovers the Pearson specified correlations for Bivariate Gamma."}
bivariate_gamma_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-biNegBinPlot, eval=FALSE, fig.width=8, fig.height=5, fig.cap="`bigsimr` recovers the correlations for bivariate negative binomial only approximately for Pearson but (nearly) exactly for the rank-based correlations."}
bivariate_nbinom_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-combineBiSims, cache=FALSE}
allDat <- bind_rows(
    select(bivariate_normal_sims, margins, type, N, rho, rho_hat),
    select(bivariate_gamma_sims, margins, type, N, rho, rho_hat),
    select(bivariate_nbinom_sims, margins, type, N, rho, rho_hat)
) %>%
    mutate(margins = factor(margins, levels=c("norm", "gamma", "nbinom")),
           type = factor(type, levels=c("Pearson", "Spearman", "Kendall")),
           N = factor(N))
```


Figure \@ref(fig:ch040-bPlot) displays the aggregated bivariate simulation results. Table \@ref(tab:ch040-BiError) contains the mean absolute error (MAE) in reproducing the desired dependency measures for the three bivariate scenarios.


```{r ch040-BiError}
tabMAE <- allDat %>%
    group_by(N, type, margins) %>%
    summarize(MAE = mean(abs(rho - rho_hat))) %>%
    ungroup()

kable(tabMAE, booktabs = TRUE,
      col.names = c("No. of random vectors",
                    "Correlation type",
                    "Distribution",
                    "Mean abs. error"),
      caption = "Average abolute error in matching the target dependency across the entire range of possible correlations for each bivariate marginal.")
```


Overall, the studies show that our methodology is generally accurate across the entire range of possible correlation values all three dependency measures, at least in these limited simulation settings for the rank-based correlations. Our Pearson matching performs nearly as well as Spearman or Kendall, except a slight increase in error for negative binomial case. This is due the particularly large counts generated with our choice of parameters for $p$ and $r$.


```{r ch040-bPlot, cache=FALSE, fig.asp=1.30, fig.weight=4, fig.cap="Bivariate simulations match target correlations across the entire range of feasible correlations. The horizontal axis plots the specified target correlations for each bivariate margin. Normal margins are plotted in dark dark grey, gamma in medium grey, and negative binomial in light grey. As the number of simulated vectors $B$ increases from left to right, the variation in estimated correlations (vertical axis) decreases. The dashed line indicates equality between the specified and estimated correlations."}
# https://www.datanovia.com/en/blog/how-to-change-ggplot-facet-labels/
# New facet label names
repsLabs <- paste0("B=", c("1,000", "10,000", "100,000") )
names(repsLabs) <- c(1000, 10000, 100000)
typeLabs <- c("Pearson", "Spearman", "Kendall")
names(typeLabs) <- c("Pearson", "Spearman", "Kendall")

# Set colors
## RColorBrewer::display.brewer.all()
numColors <- 4
numGroups <- length(levels(allDat$margins))
myColors <- rev(RColorBrewer::brewer.pal(n = numColors, name = "Greys")[ ((numColors - numGroups) + 1):numColors])

allDat %>%
    ggplot(aes(x = rho, y = rho_hat, color = margins)) +
    geom_point(size = 2) +
    scale_color_manual(values = myColors ) +
    geom_abline(slope = 1, linetype = "dashed") +
    labs(x = "Specified Correlation", y = "Estimated Correlation") +
    facet_wrap(~ type + N, labeller = labeller(N = repsLabs, type = typeLabs)) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal")
```


<!-- 
See [Discussion](discussion) for future directions for fast Pearson matching and discrete-specific modifications.
-->


## Scale up to High Dimensions


With information of our method's accuracy from a low-dimensional perspective, we now turn to assessing whether the `bigsimr` can scale to larger dimensional problems with practical computation times. Specifically, we ultimately generate $B=1,000$ random vectors for $d=\{100, 250, 500, 1000, 2500, 5000, 10000\}$ for each correlation type, $\{Pearson, Spearman, Kendall\}$ while timing the algorithm's major steps.


To mimic the workflow, we first produce a synthetic "data set" by completing the following steps:  


1. Produce heterogeneous gamma marginals by randomly selecting the $j^{th}$ gamma shape parameter from $U_j \sim uniform(1,10), j=1,\ldots,d$ and the $j^{th}$ rate parameter from $V_j \sim exp(1/5), j=1,\ldots,d$, with the constant parameters determined arbitrarily.  
2. Produce a random full-rank Pearson correlation matrix via `cor_randPD` of size $d \times d$.  
3. Simulate a "data set" of $1,000 \times d$ random vectors via `rvec` (without matching the Pearson exactly).  


With the synthetic data set in hand, we complete and time the following 4 steps:


i. Estimate the correlation matrix from the "data" in the *Compute Correlation* step.  
1. Map the correlations to initialize the algorithm (Pearson to Pearson, Spearman to Pearson, or Kendall to Pearson) in the *Adjust Correlation* step.  
2. Check whether the mapping produces a valid correlation matrix and, if not, find the nearest PD correlation matrix in the *Check Admissibility* step.  
3. Simulate $1,000$ vectors via the Inverse Transform method in the *Simulate Data* step.


The experiments are conducted on a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, with all 16 threads employed during computation. The results for $d \leq 500$ are fast with all times under 3 seconds. Table \@ref(tab:ch040-moderateDtab) displays the total computation time of the algorithms steps 1 through 3, including estimation step i.


```{r ch040-moderateDtab}
dat_long <- benchmark_dependences %>%
  select(-total_time, -n_sim, -needed_near_pd) %>%
  pivot_longer(cols = c(corr_time:sim_time), names_to = "Step", values_to = "Time") %>%
  mutate(dim = factor(dim),
         Step = factor(Step,
                       levels = c("corr_time", "adj_time", "admiss_time", "sim_time"),
                       labels = c("Compute Correlation",
                                  "Adjust Correlation",
                                  "Check Admissibility",
                                  "Simulate Data"))) %>%
    rename(Correlation = corr_type, Dimensions = dim)

tabTimes  <- dat_long %>%
    filter(Dimensions %in% c(100, 250, 500)) %>%
    group_by(Dimensions, Correlation) %>%
    summarize('Total Time(Seconds)' = sum(Time)) %>%
    ungroup()

tabTimes %>%
    kable(booktabs = TRUE,
          col.names = c("Dimension",
                        "Correlation type",
                        "Total Time (Seconds)"),
          caption = 'Total time to produce 10,000 random vectors with a random correlation matrix and hetereogeneous gamma margins.')
```


The results larger dimensional vectors show scalability to ultra-high dimensions for all three correlation types, although the total times do become much larger. 
Figure \@ref(fig:ch040-largeDfig) displays computation times for $d=\{1000, 2500, 5000, 10000\}$. For $d$ equal to 1000 and 2500, the total time is under a couple of minutes. At $d$ of 5000 and 10,000, Pearson correlation matching in the *Adjust Correlation* step becomes costly. Interestingly, Pearson is actually faster than Kendall for $d=10,000$ due to bottlenecks in *Compute Correlation* and *Check Admissibility*. Uniformly, matching Spearman correlations is faster, with total times under 5 minutes for $d=10,000$, making Spearman correlations the most computationally-friendly dependency type. With this in mind, we scaled the simulation to $d=20,000$ for the Spearman type and obtained the 1,000 vectors in under an hour (data not shown). In principle, this would enable the simulation of an entire human-derived RNA-seq data set. We note that for a given target correlation matrix and margins, steps i, 1, and 2 only need to be computed once and the third step, *Simulate Data*, is fast under all schemes for all $d$ under consideration.


```{r ch040-largeDfig, fig.cap="Computation times as d increases."}
# Set colors
numColors <- 5
numGroups <- 4
myColors <- rev(brewer.pal(n=numColors, name="Greys")[((numColors-numGroups)+1):numColors])

dat_long %>%
    filter(Dimensions %in% c(1000, 2500, 5000, 10000, 20000)) %>%
    mutate(`Time (minutes)` = Time / 60) %>%
    ggplot(aes(Correlation, `Time (minutes)`, fill=Step)) +
    geom_bar(position = "stack", stat = "identity") +
    scale_fill_manual(values = myColors ) +
    scale_y_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30),
                     minor_breaks = NULL) +
    facet_grid(. ~ Dimensions) +
    theme(axis.text.x = element_text(angle = -90))
```

*Limitations, conclusions, and recommendations.*
In the bivariate studies, we chose arbitrary simulation parameters for three distributions, moving from the Gaussian to the discrete and non-normal, multivariate negative binomial. Under these conditions, the simulated random vectors sample the desired bivariate distribution across the entire range of pairwise correlations for the three dependency measures. The simulation results could differ for other choices of simulation settings. Specifying extreme correlations near the boundary or Frechet bounds could result in poor simulation performance. Fortunately, it is straightforward to evaluate simulation performance by using strategies similar to those completed above. We expect our random vector generation to perform well for the vast majority of NORTA-feasible correlation matrices, but advise to check the performance before making inferences/further analyses.
Finally, we note that one could use our single-pass algorithm `cor_fastPD` to produce a "close" (not nearest PD) to scale to even higher dimensions with greater loss of accuracy.
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:040-monte-carlo-evaluation.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# RNA-seq data applications  {#examples}


```{r ch050-LoadLib, include=FALSE, cache=FALSE}
box::use(
  dplyr[...],
  ggplot2[...],
  GGally[ggpairs, wrap],
  patchwork[...],
  bigsimr[...],
  knitr[include_graphics],
  JuliaCall[julia_call],
  ./R/utils[mom_nbinom]
)
load("data/example_brca.rda")
load("data/example_genes.rda")
sim_nbinom <- readRDS("results/brca_1000_sim.rds")
simRho <- readRDS("results/brca_1000_frobenius.rds")

Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
bs <- bigsimr_setup(pkg_check = FALSE)
dist <- distributions_setup()
```


This section demonstrates how to simulate multivariate data using `bigsimr`, aiming to replicate the structure of high-dimensional dependent count data. In an illustration of our proposed methodology, we seek to simulate RNA-sequencing data by producing simulated random vectors mimicking the observed data and its generating process. Modeling RNA-seq using multivariate probability distributions is natural as inter-gene correlation is an inherent part of biological processes [@Wang2009b]. And yet, many models do not account for this, leading to major disruptions to the operating characteristics of statistical estimation, testing, and prediction. See @Efron2012 for a detailed discussion with related methods and @Wu2012b, @Schissler2018; @Schissler2019 for applied examples. The following subsections apply `bigsimr`'s methods to real RNA-seq data, including replicating an estimated parametric structure, MC probability estimation, and MC evaluation of correlation estimation efficiency.

## Simulating High-Dimensional RNA-seq data


```{r ch050-readBRCA, cache=FALSE}
d <- 1000
brca1000 <- example_brca %>%
  select(all_of(1:d)) %>%
  mutate(across(everything(), as.double))
```


We begin by estimating the structure of the TCGA BRCA RNA-seq data set (see [Background](background)). Ultimately, we will simulate $B=10,000$ random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with $d=`r d`$. We assume a multivariate negative binomial (MVNB) model as RNA-seq counts are often over-dispersed and correlated. Since all $d$ selected genes exhibit over-dispersion (data not shown), we proceed to estimate the NB parameters $(r_i, p_i), i=1,\ldots,d$, to determine the target marginal PMFs $f_i$. To complete specification of the simulation algorithm inputs, we estimate the Spearman correlation matrix ${ \bf R}_{S}$ to characterize dependency.


<!--
This suggests simulating high-dimensional multivariate NB (MVB) with heterogeneous marginals would be useful tool in the development and evaluation of RNA-seq analytics. This procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. All these genes exhibit over-dispersion and, so, we proceed to estimate the NB parameters $(r_i, p_i), i=1,\ldots,d$ to determine the target marginal PMFs $g_i(y_i)$ (via method of moments). Notably, the $\hat{p}_i's$ are small --- ranging in $[3.934 \times 10^{-6} , 1.217 \times 10^{-2}]$. To complete the simulation algorithm inputs, we estimate the Pearson correlation matrix $\bf{R_Y}$ and set that as the target correlation.
-->


<!--
With the simulation targets specified, we proceed to simulate $N=10,000$ random vectors $\bf{Y}$ $=( Y_1,\ldots,Y_d)$ with target Pearson correlation $\bf{R_Y}$ and marginal PMFs $g_i(y_i)$ using a $\bf{T}$-Poisson hierarchy of Kind II. Specifically, we first employ the \emph{direct Gaussian copula} approach to generate $N$ random vectors following a standard multivariate Gamma distribution $\bf{T}$ with shape parameters $r_i$ equal to the target $n_i$ and Pearson correlation matrix $\bf{R_T}$. Care must be taken when setting the specifying $\bf{R}$ (refer to Equation \ref{gay.cop.pdf}) --- we employ Equation \ref{mix.poi.corr} to compute the scaling factors $c_{i,j}$ and adjust the underlying correlations to ultimately match the target $\bf{R_Y}$. Notably, of the $525,825$ pairwise correlations from the $1026$ genes, no scale factor was less than $0.9907$, indicating the model can produce essentially the entire range of possible correlations. Here we are satisfied with approximate matching of the specified Gamma correlation and set $\bf{R}$ = $\bf{R_T}$ in our Gaussian copula scheme ($\bf{R}$ indicating the specified multivariate Gaussian correlation matrix). Finally, we generate the desired random vector $Y_i=N_i(t_i)$ by simulating Poisson counts with expected value $\mu_i=\lambda_i \times T_i$, for $i=1,\ldots,d$ (with $\lambda_i=\frac{(1-p_i)}{p_i}$) and repeat $N=10,000$ times.
-->


With this goal in mind, we first estimate the desired correlation matrix using the fast implementation provided by `bigsimr`:


```{r ch050-estRhoBRCA, echo=TRUE, cache=FALSE}
# Estimate Spearman's correlation on the count data
R_S <- bs$cor(as.matrix(brca1000), bs$Spearman)
```


Next, we estimate the marginal parameters. We use the method of moments (MoM) to estimate the marginal parameters for the multivariate negative binomial model. While, marginal distributions are from the same probability family (NB), they are heterogeneous in terms of the parameters probability and size $(p_i, n_i)$ for $i,\ldots,d$. The functions below support this estimation for later use in `rvec`.


<!-- 
TYPESTE MoM estimators here.
-->


```{r ch050-nbHelpers, echo=TRUE}
make_nbinom_margins <- function(sizes, probs) {
  margins <- lapply(1:length(sizes), function(i) {
    dist$NegativeBinomial(sizes[i], probs[i])
  })
  do.call(c, margins)
}
```


We apply these estimators to all `r d` genes across the `r nrow(brca1000)` patients:


```{r ch050-estMargins, echo=TRUE}
nbinom_fit <- apply(brca1000, 2, mom_nbinom)
sizes <- nbinom_fit["size",]
probs <- nbinom_fit["prob",]
nb_margins <- make_nbinom_margins(sizes, probs)
```


Notably, the estimated marginal NB probabilities $\{ \hat{p}_i \}$ are small --- ranging in the interval $[`r min(probs)` , `r max(probs)`]$. This gives rise to highly variable counts and, typically, less restriction on potential pairwise correlation pairs. Once the functions are defined/executed to complete marginal estimation, we specify targets and generate the desired random vectors using `rvec`. Now we check admissibility of specified correlation matrix.


```{r ch050-cor-check, echo=TRUE}
# 1. Mapping step first
R_X <- bs$cor_convert(R_S, bs$Spearman, bs$Pearson)

# 2a. Check admissibility
(is_valid_corr <- julia_call("Bigsimr.iscorrelation", R_X))

# 2b. compute nearest correlation
if (!is_valid_corr) {
  R_X_pd <- bs$cor_nearPD(R_X)
  ## Quantify the error
  targets      <- R_X[lower.tri(R_X, diag = FALSE)]
  approximates <- R_X_pd[lower.tri(R_X_pd, diag = FALSE)]
  R_X          <- R_X_pd
}
summary(abs(targets - approximates))
```

While the exact $d \times d$ Spearman correlation matrix is not strictly admissible in our scheme (as seen by the non-positive definite result above), the approximation is close with a maximum absolute error of `r max( abs(targets - approximates))` and average absolute error of `r mean(abs(targets - approximates))` across the `r choose(d,2)` correlations.


```{r ch050-runBRCA-echo, echo=TRUE, eval=FALSE}
sim_nbinom <- bs$rvec(10000, R_X, nb_margins) 
colnames(sim_nbinom) <- colnames(brca1000)
```


Figure \@ref(fig:ch050-simDataFig) displays the simulated counts and pairwise relationships for our example genes from Table \@ref(tab:ch010-realDataTab).
Simulated counts roughly mimic the observed data but with a smoother appearance due to the assumed parametric form and with less extreme points then the observed data in Figure \@ref(fig:ch010-realDataFig).


```{r ch050-simDataFig, cache=FALSE, fig.cap="Simulated data for three selected high-expressing genes generally replicates the estimated data structure. The data do not exhibit outlying points, but do possess the desired Spearman correlations, central tendencies, and discrete values."}
set.seed(2020-02-25)
num_genes <- 3
gene_sample <- sample(example_genes[1:1000], num_genes)

ggpairs(data = as.data.frame(sim_nbinom[, gene_sample]),
        upper = list(continuous = wrap('cor', method = "spearman"))) + 
  theme_bw()
```


Figure \@ref(fig:ch050-figBRCA) displays the aggregated results of our simulation by comparing the specified target parameter (horizontal axes) with the corresponding quantities estimated from the simulated data (vertical axes). The evaluation shows that the simulated counts approximately match the target parameters and exhibit the full range of estimated correlation from the data.


<!-- 
Utilizing 15 CPU threads in a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, the simulation completed in just over 2 minutes.
-->


```{r ch050-figBRCA, out.width='80%', fig.cap="Simulated random vectors from a multivariate negative binomial replicate the estimated structure from an RNA-seq data set. The dashed red lines indicate equality between estimated parameters from simulated data (vertical axes) and the specified target parameters (horizontal axes)."}
include_graphics('fig/ch050-figBRCA.png')
```


*Limitations, conclusions, and recommendations.*
The results show overall good simulation performance for our choice of parameters settings. Our settings were motivated by modeling high-expressing genes from TCGA BRCA data set. In general, the ability to match marginal and dependence parameters depends on the particular joint probability model. We recommend to evaluate and tune your simulation until you can be assured of the accuracy.


## Simulation-based joint probability calculations


Many statistical tasks require evaluation of a joint probability mass (or density) function:


$$
P( {\bf Y} = {\bf y} ), \: y_i \in \chi_i.
$$


where $\chi_i$ is the sample space for the $i^{th}$ component of the random vector $\bf{Y}$. Compact representations with convenient computational forms are rare for high-dimensional constructions, especially with heterogeneous, correlated marginal distributions (or margins of mixed data types). Given a large number of simulated vectors as produced above, estimated probabilities are readily given by counting the proportion of simulated vectors meeting the desired condition. In our motivating application, one may ask what is the probability that all genes expressed greater than a certain threshold value ${ \bf y}_0$. This can be estimated as 


$$
\hat{P}( {\bf Y} \ge {\bf y_0 } ) = \frac{1}{B} \sum_{b=1}^B I( {\bf Y}^{ (b) } \ge {\bf y_0 } ),
$$


```{r ch050-densityEvaluation, include=FALSE}
d <- ncol(sim_nbinom)
B <- nrow(sim_nbinom)
y0 <- 1
pHat <- mean(apply(sim_nbinom, 1, function(Y) {
  all(Y >= y0)
}))
```


where ${\bf Y^{(b)} }$ is the $b^{th}$ simulated vector from a total of $B$ simulation replicates and $I(\cdot)$ is the indicator function. For example, we can estimate from our $B=10,000$ simulated vectors that the probability of all genes expressing (i.e., ${\bf y}_i \geq 1, \forall \; i$) is $`r pHat`$.


```{r ch050-densityEvaluationECHO, echo=TRUE}
<<ch050-densityEvaluation>>
pHat
```


## Evaluation of correlation estimation efficiency


MC methods are routinely used in many statistical inferential tasks including estimation, hypothesis testing, error rates, and empirical interval coverage rates. To conclude the example applications, we demonstrate how `bigsimr` can be used to evaluate estimation efficiency. In particular, we wish to assess the error in our correlation estimation above. We used a conventional method, based on classical statistical theory which was not designed for high-dimensional data. Indeed, high-dimensional covariance estimation (and precision matrices) is an active area of statistical science, .e.g., [@Won2013g; @VanWieringen2016].

In this small example, we simulate $m=30$ data sets with the number of simulated vectors matching the number of patients in the BRCA data set, $N=`r nrow(example_brca)`$. Since our simulation is much faster for the Pearson correlation type (see Figure \@ref(fig:ch040-largeDfig)), we only convert the Spearman correlation matrix once (and ensure it is positive definite). At each iteration, we estimate the quadratic loss (residual sum of squared errors) from the specified ${\bf R}_{S}$, producing a distribution of loss values.


```{r ch050-quadlossECHO, echo=TRUE, eval=FALSE}
# Simulate random vectors equal to the sample size
N <- nrow(example_brca)
# create m random vectors and estimate correlation
simRho <- replicate(n = m, expr = {
  tmpSim <- bs$rvec(N , R_X, nb_margins)
  bs$cor(tmpSim, bs$Spearman)
}, simplify = FALSE)
# Evaluate the residual sum of squared error
sapply(simRho, function(R) sum((R - R_S)^2))
```


```{r ch050-lossResults}
frobenius_loss <- sapply(simRho, function(R) sum((R - R_S)^2))
```


The `R` summary function supplies the mean-augmented five-number summary of the quadratic loss distribution computed above.


```{r ch050-loss-summary, echo=TRUE}
summary(frobenius_loss)
```


This distribution could be compared to high-dimensional designed covariance estimators for guidance on whether the additional complexity and computation time are warranted.
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:050-examples.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Conclusion and discussion {#discussion}


We have introduced a general-purpose high-dimensional multivariate simulation algorithm and provide a user-friendly, high-performance `R` package [`bigsimr`]. The random vector generation method is inspired by NORTA [@Cario1997] and Gaussian copula-based approaches [@MB13, @BF17, @Xia17]. The major contributions of this work are methods and software for flexible, scalable simulation of HD multivariate probability distributions with broad potential data analytic applications for modern, big-data statistical computing. For example, one could simulate high-resolution time series data, such as those consistent with an auto-regressive moving average model exhibiting a specified Spearman structure. Or our methods could be used to simulate sparsely correlated data, as many HD methods assume, via specifying a *spiked correlation matrix*.

Researchers working in multivariate computation frequently encounter such difficulties and need to find a close or nearest correlation matrix. A widely-available routine for this task in `R` is `matrix::nearPD`, though it is not suitable for high dimensions. To overcome this issue, we introduce `cor_nearPD` --- an implementation of @QS2006's method that computes the nearest correlation matrix to a given symmetric matrix in the Frobenius norm. This routine as many applications beyond our primary goal of random vector generation.

It is customary to compare new tools and algorithms directly to existing competing methods and software. In this study, however, we only employ our proposed methodology, as our previous work has shown that existing `R` tools are simply not designed to meet our high-dimensional goal (see @Li2019gpu for evaluations of the `R` `copula` package and others). For the bivariate simulations, existing packages such as [`nortaRA`](https://github.com/cran/NORTARA/blob/master/inst/doc/NORTARA.R) work well to match Pearson correlations exactly.

<!-- 
We advocate the use of Kendall's $\tau$ as it better captures correlation among components of non-normal, as well as non-linear patterns of association. Moreover, the matching Kendall's $\tau$ is nearly trivial due to the invariant of monotone transformations, after an adjustment to the input correlation matrix.
-->


<!-- 
We also show utility in our methodology through an application to differential gene expression analysis from RNA-sequencing data. The application results show that correlations indeed matter in the large-scale hypothesis testing, as many others have noted (for example, see @BE07, @Wu2012b). We also hope that the application provides an example workflow --- and other strategy to use in simulation design. Even the best-performing simulations we provide show a gap from the empirical distribution. To keep the demonstration straightforward, we did not attempt to match the empirical distribution of test statistics as precisely as possible. Yet our methodology could be more creatively applied to meet that goal. One could consider marginal distributions from different families, such as Poisson for a subset of genes. One could imagine finding a best fitting probability distribution among a class of distributions for each gene and this could perhaps a better fit. One could imagine additional structures/features in the data that an analyst could model to improve the correspondence with the empirical values, for example row correlations and high-dimensional covariance estimators (see @Won2013g).
-->


There are limitations to the methodology and implementation. We could only investigate selected multivariate distributions in our Monte Carlo studies. There may be instances that the methods do not perform well. Along those lines, we expect that correlation values close to the boundary of the feasible region could result in algorithm failure. Another issue is that for discrete distributions, we use continuous approximations when the support set is large. This could limit the scalability/accuracy for particular joint distributions.

<!-- 
The most obvious missing feature of the proposed methodology is the inability to match a Pearson correlation matrix exactly. As discussed in the [Algorithms](algorithms) section and extensively by @XZ19, this is a computationally intense procedure and Pearson's correlation is not a natural choice to describe dependency for non-normal marginals. While we do not provide an implementation directly supporting Pearson matching, users may supply their own input Pearson correlation after employing a supplementary matching scheme [@Cario1997; @XZ19].
-->

<!-- 
Further, while we provide the ability of the user to specify discrete marginals, exact matching a desired dependency measure is not yet obtained. One potential solution for this is *rescale* to account for ties [ref]. Finally we note, that the algorithm requires invertible marginal cdfs. This gives some restriction to the available marginal probability distributions (DOES IT? FOR EXAMPLE?). From a practical computing standpoint, the user's computing resource provides the ultimate limit on how large a dimension can be simulated using the `bigsimr` R package. A user must consider carefully the available memory and cores when conducting a Monte Carlo experiment. 
-->


<!-- 
Future work includes developing scaleable algorithms to match the Pearson correlation matrix more precisely, discrete-margin specific modifications including fast Spearman's correlation rescaling (see Equation \@ref(eq:spearmanRescaled)), and high-dimensional covariance estimation. From an implementation standpoint, `bigsimr` only supports Nvidia GPUs and redesigning the code using OpenCL would broaden the users who would benefit. As data-analytic problems grow to even larger dimension, multi-GPU support is a promising hardware-based future direction.
-->
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:060-conclusion-discussion.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Supplementary Materials {.unnumbered #misc}


We provide an open-source implementation of our methodology as the `bigsimr` R package, hosted on github, https://github.com/SchisslerGroup/r-bigsimr.


```{r, out.width='5%'}
knitr::include_graphics('images/hex-bigsimr.png')
```


# Acknowledgment(s) {.unnumbered #acknowledgments}


The authors gratefully acknowledge Heather Knudson's graphic design for the `bigsimr` R Package. The results published here are in whole or part based upon data generated by the TCGA Research Network: https://www.cancer.gov/tcga.


# Disclosure statement {.unnumbered #coi}


The authors report no conflict of interest.


# Funding {.unnumbered #funding}


Research reported in this publication was supported by grants from the National Institutes of General Medical Sciences (5 U54 GM104944, GM103440) from the National Institutes of Health.


<!-- 
Nomenclature/Notation
-->
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:070-supplementary-materials.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
# Appendix {.unnumbered #appendix}


\noindent Consider a bivariate example where we have a correlated $(Y_1, Y_2)^\top$ with $Y_1\sim N(\mu_1, \sigma_1^2)$ (normal) and $Y_2\sim LN(\mu_2, \sigma_2^2)$ (lognormal). First, let us note that when we get such a vector from bigsimr package then in fact it can be represented as 


\begin{equation}
\label{kram1}
(Y_1, Y_2)^\top \stackrel{d}{=} \left(X_1, e^{X_2}\right)^\top,
\end{equation}


where $(X_1, X_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)$, which is bivariate normal with mean vector $\boldsymbol \mu = (\mu_1, \mu_2)^\top$ and variance-covariance matrix 


\begin{equation}
\label{kram2}
\boldsymbol \Sigma = 
\left[
\begin{array}{cc}
\sigma_1^2 & \rho \sigma_1\sigma_2\\
\rho \sigma_1\sigma_2 & \sigma_2^2
\end{array}
\right].
\end{equation}


To see this, consider the three steps of the NORTA construction:


\begin{enumerate}

\item Generate $(Z_1, Z_2)^\top \sim N_2(\boldsymbol 0, \boldsymbol R)$, where 

\begin{equation}
\label{kram3}
\boldsymbol R = 
\left[
\begin{array}{cc}
1 & \rho \\
\rho & 1
\end{array}
\right].
\end{equation}


\item Transform $(Z_1, Z_2)^\top$ to $(U_1, U_2)^\top$ viz. $U_i =\Phi(Z_i)$,  $i=1,2$, where $\Phi(\cdot)$ is the standard normal CDF. 

\item Return $(Y_1, Y_2)^\top$, where $Y_i=F_i^{-1}(U_i)$, $i=1,2$, and $F_i(\cdot)$ is the CDF of $Y_i$ and $F_i^{-1}(\cdot)$ is its inverse (the quantile function). 

\end{enumerate}


In this example, $F_1$ is the CDF of $N(\mu_1, \sigma_1^2)$ while $F_2$ is the CDF of $LN(\mu_2, \sigma_2^2)$, so that the two required quantile functions are 


\begin{equation}
\label{kram4}
F_1^{-1}(u) = \mu_1+\sigma_1 \Phi^{-1}(u)\,\,\, \mbox{and} \,\,\, F_2^{-1}(u) = e^{\mu_2+\sigma_2 \Phi^{-1}(u)}, 
\end{equation}


as can be seen by standard calculation. Thus, when we apply Step 3 of the NORTA algorithm, we get 


\begin{equation}
\label{kram5}
F_1^{-1}(U_1) = \mu_1+\sigma_1 \Phi^{-1}(\Phi(Z_1)) =  \mu_1+\sigma_1 Z_1 \,\,\, \mbox{and} \,\,\, F_2^{-1}(U_2) = e^{\mu_2+\sigma_2 \Phi^{-1}(\Phi(Z_2))} = e^{\mu_2+\sigma_2 Z_2}, 
\end{equation}


where $(X_1, X_2)^\top =  (\mu_1+\sigma_1 Z_1, \mu_2+\sigma_2 Z_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)$. Consequently, the vector $(Y_1, Y_2)^\top$ obtained in Step 3 of the algorithm has the structure provided by (\ref{kram1}). 


\vspace{0.1in}


\noindent Next, we provide the exact covariance structure of the random vector $(Y_1, Y_2)^\top$ given by (\ref{kram1}), and relate the correlation of $Y_1$ and $Y_2$ to $\rho$, which is the correlation of the normal variables $Z_1$, $Z_2$ (and also $X_1$ and $X_2$). A straightforward albeit somewhat tedious algebra produces the following result. 


\begin{lemma}
Let ${\bf Y} = (Y_1, Y_2)^\top$ admit the stochastic representation (\ref{kram1}), where ${\bf X} = (X_1, X_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)$ with $\boldsymbol \mu$ and $\boldsymbol \Sigma$ as above. Then the mean vector and the variance-covariance matrix of ${\bf Y}$ are given by 

\begin{equation}
\label{kram6}
\boldsymbol \mu_{{\bf Y}} = \left(\mu_1, e^{\mu_2+\sigma_2^2/2}\right)^\top
\end{equation}

and 

\begin{equation}
\label{kram7}
\boldsymbol \Sigma = 
\left[
\begin{array}{cc}
\sigma_1^2 & \rho \sigma_1\sigma_2  e^{\mu_2+\sigma_2^2/2}  \\
\rho \sigma_1\sigma_2 e^{\mu_2+\sigma_2^2/2} & \left[ e^{\mu_2+\sigma_2^2/2}\right]^2 \left[e^{\sigma_2^2} -1 \right],
\end{array}
\right].
\end{equation}

respectively. 
\end{lemma}


\begin{proof}
The values of the means and the variances are obtained immediately from normal marginal distribution of $Y_1$ and lognormal marginal distribution of $Y_2$. It remains to establish the covariance of $Y_1$ and $Y_2$, 

\begin{equation}
\label{covy1y2}
\mbox{Cov}(Y_1, Y_2) = \mathbb E(Y_1 Y_2) - \mathbb E(Y_1)\mathbb E(Y_2), 
\end{equation}

and in particular the first expectation on the right-hand-side above. By using the tower property of expectations, the latter expectation can be expressed as 

\begin{equation}
\label{kre1}
\mathbb E(Y_1 Y_2)  =  \mathbb E\left(X_1 e^{X_2}\right)  =  \mathbb E \left\{ \mathbb E \left(X_1 e^{X_2} | X_1 \right) \right\} = \mathbb E \left\{ X_1 \mathbb E \left(e^{X_2} | X_1 \right) \right\}.
\end{equation}

Further, by using the fact that the conditional distribution of $X_2$ given $X_1=x_1$ is normal with mean $\mathbb E(X_2|X_1=x_1) = \mu_2+\rho\sigma_2(x_1-\mu_1)/\sigma_1$ and variance $\sigma_2^2(1-\rho^2)$, one can relate the inner expectation on the far right in (\ref{kre1}) to the moment generating function $M(t)$ of this conditional distribution evaluated at $t=1$, leading to 

\begin{equation}
\label{kre2}
E \left(e^{X_2} | X_1 \right) = e^{ \mu_2+\rho\sigma_2(x_1-\mu_1)/\sigma_1 + \sigma_2^2(1-\rho^2)/2},
\end{equation}

so that 

\begin{equation}
\label{kre3}
\mathbb E(Y_1 Y_2)  =  e^{ \mu_2+ \frac{1}{2} \sigma_2^2(1-\rho^2)} \mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\}.
\end{equation}

Since $X_1$ is normal with mean $\mu_1$ and variance $\sigma_1^2$, the expectation in (\ref{kre3}) becomes 

\begin{equation}
\label{kre4}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = \int_{-\infty}^\infty x_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (x_1-\mu_1)} \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{1}{2\sigma_1^2}(x_1-\mu_1)^2}.
\end{equation}

Upon substituting $u=x_1-\mu_1$, followed by some algebra, we arrive at 

\begin{equation}
\label{kre5}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = e^{\frac{1}{2}\sigma_2^2\rho^2} \int_{-\infty}^\infty (u+\mu_1)  \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{1}{2\sigma_1^2}(u-\rho\sigma_1\sigma_2)^2}.
\end{equation}

Since the integral in (\ref{kre5}) is the expectation $\mathbb E(X+\mu_1)$, where $X$ is normal with mean $\rho\sigma_1\sigma_2$ and variance $\sigma_1^2$, we conclude that 

\begin{equation}
\label{kre6}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = e^{\frac{1}{2}\sigma_2^2\rho^2} (\rho\sigma_1\sigma_2 +\mu_1),
\end{equation}

which, in view of (\ref{kre3}), leads to 

\begin{equation}
\label{kre7}
\mathbb E (Y_1Y_2) = e^{\mu_2 + \frac{1}{2}\sigma_2^2} (\rho\sigma_1\sigma_2 +\mu_1).
\end{equation}

Finally, (\ref{covy1y2}), along with the expressions for the means of $Y_1$ and $Y_2$, produce the covariance of $Y_1$ and $Y_2$:

\begin{equation}
\label{kre8}
\mbox{Cov}(Y_1, Y_2) = e^{\mu_2 + \frac{1}{2}\sigma_2^2} (\rho\sigma_1\sigma_2 +\mu_1) - \mu_1 e^{\mu_2 + \frac{1}{2}\sigma_2^2} = \rho\sigma_1\sigma_2 e^{\mu_2 + \frac{1}{2}\sigma_2^2}.
\end{equation}

This concludes the proof
\end{proof}


Using the above result, we can directly relate the correlation coefficient of $Y_1$ and $Y_2$ with that of $X_1$ and $X_2$, 


\begin{equation}
\label{kram8}
\rho_{Y_1, Y_2} = \rho \frac{\sigma_2}{\sqrt{e^{\sigma_2^2} -1}},
\end{equation}


where $\rho$ is the correlation of $X_1$ and $X_2$. The above result is useful when studying the possible range of correlation of $Y_1$ and $Y_2$ in the above example. It can be shown that the factor on the far right in (\ref{kram8}) is a monotonically decreasing function of $\sigma_2$ on $(0,\infty)$, with the limits of 1 and 0 at zero and infinity, respectively. Thus, in principle, the range of correlation of $Y_1$ and $Y_2$ is the same as that of $X_1$ and $X_2$, as the factor on the far right in (\ref{kram8}) can be made arbitrarily close to 1. However, by changing this factor we may affect the marginal distributions of $Y_1$ and $Y_2$. It can be shown that if the marginal distributions of $Y_1$ and $Y_2$ are fixed, then the relation (\ref{kram8}) becomes 


\begin{equation}
\label{kram9}
\rho_{Y_1, Y_2} = \rho \sqrt{\frac{\log(1+c_2^2)}{c_2^2}},
\end{equation}


where $c_2=\sigma_{Y_2}/\mu_{Y_2}$ is the *coefficient of variation* (CV) of the variable $Y_2$. Thus, the range of possible correlations in this model is not affected by the distribution of $Y_1$, and is determined by the CV of $Y_2$ as follows:


\begin{equation}
\label{kram10}
- \sqrt{\frac{\log(1+c_2^2)}{c_2^2}} \leq \rho_{Y_1, Y_2} \leq \sqrt{\frac{\log(1+c_2^2)}{c_2^2}}. 
\end{equation}

Plugging in the estimates of these quantities from Section \@ref(package), we see that 

\begin{equation}
\label{kram11}
\frac{\hat{\sigma}_2}{\sqrt{e^{\hat{\sigma}_2^2} -1}} = 0.881.
\end{equation}


Thus, the possible range of correlation becomes $(-0.881, 0.881)$. This approximately agrees with the MC results provided.
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:080-appendix.Rmd-->

```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```
`r if (knitr:::is_html_output()) '# References {-}'`
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:300-references.Rmd-->

---
institution:
  name: "University of Nevada, Reno"
  department: "Mathematics and Statistics"
  address:
    line1: "1664 N. Virginia Street / 0084"
    line2: "Reno, Nevada 89557"
  phone: "(775) 784-6773"
  fax: "(775) 784-6378"
  url: https://www.unr.edu/math/
  logo: "logos/blue.png"
author:
  first: "Alfred"
  last: "Schissler"
  short: "Grant"
  title: "Ph.D."
  position: "Assistant Professor, Statistics"
  signature: "signature.jpg"
to:
  first: "Alexander"
  last: "Knudson"
  short: "Alex"
date: "`r format(Sys.Date(), format='%d %B %Y')`"
opening: "Dear CSDA Annals of Statistical Data Science Editor(s):"
closing: "Sincerely,"
output:
  pdf_document:
    template: templateSingleSignature.tex
---
```{r include=FALSE, cache=FALSE}
options(digits = 4)

knitr::opts_chunk$set(
  # Text Results
  eval = TRUE,
  echo = FALSE,
  results = 'markup',
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  # Code Decoration
  comment = NA,
  background = '#F7F7F7',
  # Cache
  cache = TRUE,
  # Plots
  fig.show = "hold",
  fig.width = 6.5,
  fig.asp = 0.618,  # 1 / phi
  out.width = "85%",
  fig.align = 'center'
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)

seed_number <- 2020-02-25
set.seed(seed = seed_number)
```

As an invited speaker at CMStatistics 2020, I was delighted to learn of the opportunity to have our work included in the first issue of CSDA Annals of Statistical Data Science. 

On behalf of my co-authors, I'm pleased to submit our manuscript entitled *Simulating High-Dimensional Multivariate Data using the `Bigsimr` Package* for your consideration.
We feel that the work is well suited to the indicated themes and hope that it will be considered a valuable contribution to this prestigious journal.

I hope that everything is in order. If not, or if you have any additional needs, please do not hesitate to contact me. Thank you and all those involved in this process.

Best regards,
```{r include=FALSE, cache=FALSE}
# Remove all objects
rm(list = ls(all.names = TRUE))

# Unload all packages
```

<!--chapter:end:letter_bigsimr_CSDA.Rmd-->

