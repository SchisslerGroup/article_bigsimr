# The `bigsimr` R package

Short description and key features, including GPU acceleration.

The most computationally extensive lies in the second step of our algorithm. We
use a optimized and parallelized multivariate normal simulator within the `R`
package `mvnfast`. The rest of the code is also parallelized, but these steps
run rapidly in serial computation except for extremely large dimension.

```{r setup, eval = TRUE}
## bookdown::preview_chapter('030-bigsimr-package.Rmd')
library(bigsimr)
```

## Specifying marginals

As stated earlier, to generate multivariate data, we need a list of marginals (and their parameters), and a correlation structure (matrix). The marginal distributions can be built up as a list of lists, where each sublist contains the information for the target distribution.

```{r}
margins = alist(
  qnorm(mean = 3.14, sd = 0.1),
  qbeta(shape1 = 1, shape2 = 4),
  qnbinom(size = 10, prob = 0.75)
)
```

The things to point out here are that in each sublist (marginal), the first item is an unnamed character string with the R name of the distribution *without a letter prefix*. E.g. instead of `rnorm`, we pass in just `"norm"`. The second thing to note is that the remaining items are *named* arguments that go along with the distribution. A full list of built-in distributions is found in the appendix.

## Specify correlation

The next step is to define a correlation structure for the multivariate distribution. This correlation matrix can either come from observed data, or we can set it ourselves, or we can generate a random correlation matrix via `bigsimr::rcor`. Let's create a simple correlation matrix where all off-diagonal elements are 0.5. Since we have 3 marginals, we need a $3\times 3$ matrix.

```{r}
rho <- matrix(0.5, nrow = 3, ncol = 3)
diag(rho) <- 1.0
rho
```

Finally we can generate a random vector with our specified marginals and correlation structure. The last argument, `type`, is looking to know what kind of correlation matrix it is receiving. Right now it can handle Pearson, Spearman, or Kendall.

## Small sample

```{r}
x <- rvec(10, rho = rho, margins = margins, type = "spearman")
```

```{r, echo=FALSE}
warning("warning.warn('No GPU/TPU found, falling back to CPU.')")
```

Taking a look at our random vector, we see that it is 10 rows and 3 columns, one column for each marginal.

```{r}
x
```

We can simulate many more samples and then check the histogram of each margin, as well as the estimated correlation between the columns.

## Scaling up N

```{r, fig.width=7}
x <- rvec(1e6, rho = rho, margins = margins, type = "spearman")

par(mfrow=c(1,3))
hist(x[,1], breaks = 30, xlab = "", main = "Normal")
hist(x[,2], breaks = 30, xlab = "", main = "Beta")
hist(x[,3], breaks = 30, xlab = "", main = "Negative Binomial")
```

## Evaluation

```{r}
cor(x, method = 'spearman')
```

We can see that even with 100,000 samples, the estimated correlation of the simulated data is not exactly the same as the target correlation. This can be explained by the fact that some correlations are simply not possible due to the discrete nature of certain distributions. Another possibility is that the copula algorithm is biased and needs correction. 


## Easy cluster implementation through `rslurm`

Though `bigsimr` runs quickly, at large $d$ users may want to run jobs on a shared computing server. The R package `rslurm` makes it esay to run embarrasingly large parallel `rvec` calls. This example assumes that `bigsimr` is installed on a system
with a slurm scheduler installed.

```{r exampleRslurm, echo=FALSE, eval=FALSE, include=FALSE}
## run this code once and use the generated rds later.
## cyberhelp.sesync.org/rslurm/articles/rslurm.html
test_func <- function(par_mu, par_sd) {
    samp <- rnorm(10^6, par_mu, par_sd)
    c(s_mu = mean(samp), s_sd = sd(samp))
}
pars <- data.frame(par_mu = 1:10,
                   par_sd = seq(0.1, 1, length.out = 10))
library(rslurm)
sjob <- slurm_apply(test_func, pars, jobname = 'test_apply',
                    nodes = 2, cpus_per_node = 2, submit = TRUE)
list.files('_rslurm_test_apply', 'results')
res <- get_slurm_out(sjob, outtype = 'table')
head(res, 3)
cleanup_files(sjob) ## careful this deletes output also

## single function evaluation
sjob <- slurm_call(test_func, jobname = 'test_call',
                   list(par_mu = 5, par_sd = 1), submit = FALSE)


## try rvec
sjob <- slurm_call(rvec, jobname = 'rvec',
                   list(par_mu = 5, par_sd = 1), submit = FALSE)
```

```{r rvecRslurm, echo=TRUE, eval=FALSE}
library(rslurm)
## a single call of rvec
## had to edit rslurm/templates/submit_single_sh.txt
## source ~/.bashrc
## conda activate bigsimr
## conda env config vars list
sjob <- slurm_call(rvec, jobname = 'rvec',
                   list(n=1e6,
                        rho = rho,
                        margins = margins,
                        type = "spearman"),
                   submit = TRUE)

res <- readRDS(file.path('_rslurm_rvec', list.files('_rslurm_rvec', 'results')))
head(res, 3)
cleanup_files(sjob) ## careful this deletes output also
```

```{r rvecRslurmApply, echo=TRUE, eval=TRUE}
library(rslurm)
## a single call of rvec
## had to edit rslurm/templates/submit_single_sh.txt
## source ~/.bashrc
## conda activate bigsimr
## conda env config vars list
sjob <- slurm_call(rvec, jobname = 'rvec',
                   list(n=1e6,
                        rho = rho,
                        margins = margins,
                        type = "spearman"),
                   submit = TRUE)

res <- readRDS(file.path('_rslurm_rvec', list.files('_rslurm_rvec', 'results')))
head(res, 3)
cleanup_files(sjob) ## careful this deletes output also
```
