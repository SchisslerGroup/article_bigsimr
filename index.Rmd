--- 
title: "Simulating High-Dimensional Multivariate Data"
subtitle: "using the bigsimr R Package"
author: "Alexander D. Knudson, Tomasz J. Kozubowski, Anna K. Panorska, Juli Petereit, and Alfred G. Schissler"
bibliography: ["bigsimr.bib", "packages.bib"]
abstract:  It is critical to realistically simulate data when conducting Monte Carlo studies and methods. But measurements are often correlated and high dimensional in this era of big data, such as data obtained through high-throughput biomedical experiments. Due to computational complexity and a lack of user-friendly software available to simulate these massive multivariate constructions, researchers may resort to simulation designs that posit independence or perform arbitrary data transformations. This greatly diminishes insights into the empirical operating characteristics of any proposed methodology, such as false positive rates, statistical power, interval coverage, and robustness. This article introduces the `bigsimr` R package that provides a flexible, scaleable procedure to simulate high-dimensional random vectors with given marginal characteristics and dependency measures. We'll describe the functions included in the package, including multi-core and graphical-processing-unit accelerated algorithms to simulate random vectors, estimate correlation, and find close positive semi-definite matrices. Finally, we demonstrate the power of `bigsimr` by applying these functions to our motivating dataset --- RNA-sequencing data obtained from breast cancer tumor samples with sample size $n=1212$ patients and dimension $d>1000$.
---

<!-- keywords simulation multivariate models high-dimensional data R packages -->

```{r indexSetup, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(knitr)
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, cache = TRUE)
library(rmarkdown)
library(bookdown)
## setwd('~/article_bigsimr')
set.seed(09212020)
```

```{r copyBib, echo = FALSE, eval = FALSE, message=FALSE, warning=FALSE, results = "hide"}
system( 'cp ~/bib/bigsimr.bib ./.' )
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r serveHelper, include=FALSE, eval=FALSE}
## rmote::start_rmote()
## library(ggplot2); qplot(mpg, wt, data=mtcars, colour=cyl)
## bookdown::render_book("index.Rmd", "bookdown::pdf_book", output_dir = "pdf")
## bookdown::preview_chapter("index.Rmd")
bookdown::serve_book()
```

```{r countWords, echo = FALSE, eval = FALSE, message=FALSE, warning=FALSE, results = "hide"}
rmdFiles <- dir(pattern = 'Rmd$', ignore.case= T)
## rmdFiles <- rmdFiles[-grep('^_', rmdFiles)]
rmdFiles <- rmdFiles[-grep('references', rmdFiles)]
## rmdFiles <- rmdFiles[-grep('README', rmdFiles)]
## rmdFiles <- rmdFiles[-grep('article', rmdFiles)] ## after generating a single document
## wordcountaddin::word_count( rmdFiles[11] )
(counts <- sapply( rmdFiles, wordcountaddin::word_count, simplify = T ))
sum(counts)
```

# Introduction

Massive high-dimensional data sets are now commonplace in many areas of scientific inquiry.
As new methods are developed for these data, a fundamental challenge lies in designing and conducting simulation studies to assess the operating characteristics of proposed methodology, such as false positive rates, statistical power, interval coverage, and robustness --- often in comparison to existing methods.
Further, efficient simulation empowers statistical computing strategies, such as the parameteric bootstrap [@Rizzo2007] to simulate from a hypothesized null model, providing inference in analytically challenging settings.
Such Monte Carlo (MC) techniques become difficult for high-dimensional data with the current existing algorithms and tools.
This is particularly true when simulating massive *multivariate*, *non-normal* distributions, arising naturally in many fields of study.

As others have noted, it can be vexing to simulate dependent, non-normal/discrete data ---  even for low dimensional settings [@MB13; @XZ19].
For continous non--normal multivariate data, the well-known NORmal To Anything (NORTA) algorithm [@Cario1997] and other copula [@Nelsen2007] approaches are well-studied with flexible, robust software available [@Yan2007; @Chen2001].
Yet these approaches do not scale in a timely fashion to high-dimensional problems [@Li2019gpu]. 
For discrete data, early simulation strategies had major flaws --- such as failing to obtain the full range of possible dependencies (e.g., admitting only positive correlations @Park1996). 
While more recent approaches [@MB13; @Xia17; @BF17] have largely rememdied this issue for low-dimesional problems, the existing tools are not designed to scale to high dimensions.

Another central issue lies characterizing dependency between components in the high-dimensional random vector.
The choice of correlation in practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g. non-normal, discrete, infinite support, etc). 
<!-- Here we propose a scheme that ignores the analytic goal --- and, instead, aims to match the empirically-derived estimated correlation data and marginal characteristics.  -->
For normal data, the Pearson product-moment correlation describes the dependency perfectly.
As we will see, however, simulating arbitrary random vectors that match a target Pearson correlation matrix exactly is computationally intense [@Chen2001; @Xia17]. 
On the other hand, an analyst may consider the use of nonparametric correlation measures to better characterize monotone, non-linear dependency, such as Spearman's $\rho$ and Kendall's $\tau$.
Throughout, we'll emphasize matching these nonparametric dependency measures as our aim lies in modeling non-normal data and these rank-based measures possess invariance properties favorable in our proposed methodology.

With all this mind, we present a scalable, flexible multivariate simulation algorithm. 
The crux of the method lies in the construction of a Gaussian copula, in the spirit of the NORTA procedure.
Further, we introduce the `bigsimr` R package that provides parallelized, high-performance software implemented our NORTA-inspirsed algorithm.
The algorithm design leverages useful properties of nonparameteric correlation measures, namely invariance under monotone transformation and well-known closed form relationships between dependency measures for the multivariate normal (MVN) distribution. 

The study proceeds by providing background information, including a description of our motivating example application --- RNA-sequencing (RNA-seq) breast cancer data.
Then we describe and justify our simulation methodology and related algorithms.
Next, we detail an illustrative low-dimensional example of basic and advanced use of the `bigsimr` R package.
Then we proceed with Monte Carlo studies under various distributional assumptions to assess accuracy and scaleable.
After the MC evaluations, we revisit our high-dimensional motivating example and employ our methods to commonplace statistical computing tasks.
Finally, we'll discuss the method's utility, limitations, and future directions.

