# Algorithms {#algorithms}

```{r ch020-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('020-simulation-algorithm.Rmd')
```

This section describes our methods involved in simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$.
Each $Y_i$ has a specified marginal CDF $F_i$ and its inverse $F^{-1}_i$.
To characterize dependency, every pair $(Y_i, Y_j)$ has a given Pearson correlation $\rho_P$, Spearman correlation $\rho_S$, and/or Kendall's $\tau$.
The method can be described as a *high-performance Gaussian copula* (Equation \@ref(eq:gauss)) providing a high-dimensional NORTA-inspired algorithm.

## NORmal To Anything (NORTA)

The well-known NORTA algorithm [@Cario1997] simulates a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$.
Specifically, NORTA algorithm proceedings as follows:

1. Simulate a random vector $\bf Z$ with $d$ *independent* and *identical* standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ such that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_{Y_i}^{-1}[\Phi(X_i)], \; i=1,2,...,d$.

With modern parallelized computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multi-core and/or graphical-processing-unit (GPU) accelerated algorithms --- providing fast scalability using readily-available hardware.

Matching specified Pearson correlation coefficients exactly (step 2 above), however, is problematic.
In general, there is no closed-form correspondence between the components of the input $\Sigma_{\bf Z}$ and target $\Sigma_{\bf Y}$.
Matching the correlations involves evaluating or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d, \; i \neq j$.
For high-dimensional data, these evaluations are often too costly to enable feasible simulation studies.
For low-dimensional problems, however, methods and tools exist to match Pearson correlations precisely [@Xia17] and the publicly available `nortaRA` R package [@Chen2001].

To maintain scalability (SP1), our solution is to essentially avoid this complication in Pearson matching.
Since our goal is to simulate non-normal marginals, we greatly prefer the use of rank-based measures, such as $\rho_S$ and $\tau$, from a modeling standpoint.
Further, $\rho_S$ and $\tau$'s invariance under monotone transformation (see [Background](#background)), preserves the correlation coefficients through steps 3, 4, and 5 in the NORTA algorithm above.
This eliminates the need for computing the $\binom{d}{2}$ integrals to match exactly (nothing is for free, however, as discussed below in Section \@ref(rand-vec-gen)).

Now, if one does desire to characterize dependency using Pearson correlations, simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution [@Song00] in many practical applications.
The quality of this approximation depends on the setting, but in practice, for high-dimensional count data we find the accuracy to be adequate.
Later, we study robustness of our method to this limitation in selected [Monte Carlo evaluations]({#simulations}).

*NORTA in higher dimensions*. Sklar's theorem provides a useful characterization of multivariate distributions through copulas.
Yet the choice of copula-based simulation algorithm affects which joint distributions may be simulated.
Even in low-dimensional spaces (e.g., $d=3$), there exist valid multivariate distributions with *feasible* Pearson correlation matrices that NORTA cannot match exactly [@LH75].
This occurs when the bivariate transformations are applied to find the input correlation matrix, yet when combined the resultant matrix is indefinite.
These situations do occur, even using exact analytic calculations. Such problematic target correlation matrices are termed *NORTA defective*.

@GH02 conducted a Monte Carlo study to estimate the probability of encountering NORTA defective matrices while increasing the dimension $d$. 
They found for what is now considered low-to-moderate dimensions ($d \approx 20$) that almost *all* feasible matrices are NORTA defective.
This stems from the concentration of measure near the boundary of the space of all possible correlation matrices as dimension increases. Unfortunately, it is precisely near this boundary that NORTA defective matrices reside.

There is hope, however, as @GH02 also showed that replicating an indefinite input correlation matrix with a close proxy will give approximate matching to the target --- with adequate performance for moderate $d$.
This provides evidence that our nearest positive semi-definite (PSD) augmented approach will maintain reasonable accuracy if our input matching scheme returns an indefinite matrix, at least for the rank-based matching scheme described above.

## `bigsimr::rvec` for random vector generation  {#rand-vec-gen}

- Discuss how the HD NORTA defective problem and the scalability issues lead to our algorithm
We now describe `bigsimr::rvec`, our algorithm to generate random vectors, which mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:

1. Mapping step  
   i. Convert the target Spearman correlation matrix $R_S$ to the corresponding MVN Pearson correlation $R_X$.   
   or  
i$^\prime$. Convert the target Kendall $\tau$ matrix $R_K$ to the corresponding MVN Pearson correlation $R_X$.
2. Checking admissibility and nearest correlation computation
    (i) Check that $R_X$ is a correlation matrix, a PSD matrix with 1's along the diagonal.   
	(ii) If $R_X$ is a correlation matrix, the input matrix is *admissible* in this scheme.  
	Otherwise,  
(ii$^\prime$) Replace $R_X$ with the nearest correlation matrix $R_X$, in the Frobenius norm.
3. NORTA step
	(i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_X)$.  
	(ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$.  
	(iii) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

The "mapping step" takes advantage of the closed-form relationships between $\rho_S$ and $\tau$ with $\rho_P$ for bivariate normal random variables via Equations \@ref(eq:convertKendall) or \@ref(eq:convertSpearman), respectively (implemented as `bigsimr::cor_covert`).

Our algorithm allows the generation of high-dimensional multivariate data with arbitrary marginal distributions with a broad class of admissible Spearman correlation matrices and Kendall $\tau$ matrices.
The admissible classes consist of the matrices that map to a Pearson correlation matrix for a MVN.

In particular, if we let $X$ be MVN with $d$ components and set

$$
\begin{aligned}
\Omega_P &= \{ R_P : R_P \,  is \, a \,  correlation \,  matrix \,  for \,  X \} \\
\Omega_K &= \{ R_K : R_K \,  is \,  a \,  Kendall \,  \tau  \, matrix \,  for \,  X \} \\
\Omega_S &= \{ R_S : R_S \,  is \,  a \,  Spearman \,  correlation \,  matrix \,  for \,  X\} \\
\end{aligned}
$$

then there are 1-1 mappings between these sets.
We conjecture informally that the sets of admissible $R_S$ and $R_K$ are not highly restrictive.
In particular, $R_P$ is approximately $R_S$ for a MVN, suggesting that the admissible set $\Omega_S$ should be flexible as $R_P$ can be any PSD matrix with 1's along the diagonal. 
We provide methods to check whether a target $R_S$ is an element of the *admissible set* $\Omega_S$ (and similarly for $R_K$) and also for checking that each pairwise correlation value is admissible (via the MC estimation of the 2-dimensional Frechet bounds). 

Yet this scheme still suffers similar issues as the original Pearson-matching NORTA algorithm regarding the increasing probability of encountering a non-admissible correlation matrix as dimension increases.
In our experience, the *mapping step* for large $d$ almost always produces a $R_X$ that is not in $\Omega_P$ (or even a correlation matrix). The second step handles this issue by computing the nearest correlation matrix to $R_X$.
In Section \@ref(package) we provide a basic description of how to quantify and potentially control the approximation error.
Further, the RNA-seq example in Section \@ref(examples) provides an illustration of this in practice.

The final step implements a high-performance NORTA algorithm.
Specifically, step 3i uses either the efficient multi-core multivariate normal simulator `mvnfast`[@Fasiolo2016] or Google's JAX python library for GPU acceleration of the Cholesky factorization and matrix multiplication and steps 3ii, 3iii are parallelized over CPU cores.

<!-- 
## Other high-performance `bigsimr` algorithms

<mark> ALEX, briefly discuss other algorithms here </mark>
- `cor_bounds()`  
- `cor_fast()`  
- `cor_nearPSD()`  

For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components from Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

your comment -->
