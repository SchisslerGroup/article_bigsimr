# Algorithms {#algorithms}

```{r ch020-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('020-simulation-algorithm.Rmd')
```

This section describes the method for simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$. Each $Y_i$ has a specified marginal
distribution function $F_i$ and its inverse. To characterize dependency, every pair $(Y_i, Y_j)$ has either a specified Pearson correlation \@ref(eq:pearson), (rescaled) Spearman correlation \, or Kendall's $\tau$ ). The method only approximately matches the Pearson correlation in general, whereas the rank-based methods are exact.

The method is best understand as a **parallelized Gaussian copula** (see Equation \@ref(eq:gauss) to provide a high-performance NORTA (NORmal To Anything) algorithm.

`mvnfast` High-performance multivariate normal simulator [@Fasiolo2016]

`nortaRA` Implements exact Pearson matching (step 2 in NORTA) [@Chen2001] 

To simulate a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$,
there is the well-known NORTA algorithm [@Cario1997]. It follows like this:

1. Simulate a random vector $\bf Z$ with $d$ **independent** and **identical** standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ to corresponds with the
   specified output $\Sigma_{\bf Y}$ [@Chen2001; @Xia17]
3. Produce the Cholesky factor $M$ of $\Sigma_{\bf Z}$ so that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_{Y_i}^{-1}[\Phi(X_i)], \; i=1,2,...,d$.

We shall see that constructing continuous joint distributions that match a target Spearman or Kendall's correlations computes easily when employing Gaussian copulas, since this measures are invariant under the monotone transformations involved [refs]. 

<!-- 
For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components from Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

your comment -->

In contrast the rank-based correlations, matching specified Pearson correlation coefficients exactly is computational intense in this scheme. In general, there is no closed form correspondence and involving computing or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X|r}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d$ (see Xia17 and MB13). 
For accurate numeric approximation of these integrals, the functions must be evaluated hundreds of times. 
Others have used efficient Monte Carlo integration schemes (see @Chen2001), but scale poorly to large dimension in reasonable times (property **S2**). 
Despite all this, if one does desire to characterize dependency using Pearson correlations, we often see in practice --- and it is theoretically justified under certain conditions (@Song00) --- 
that simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution.
We'll study the robustness of our method to this limitation later in the application section (Section BLAH).

Putting the together the facts provided in the equations above, we come the following proposed simulation algorithm to produce a random vector ${\bf Y}$ with specified Spearman's correlation and marginal distributions. Note that all computational steps can be parallelized as each operation can be done or either the $d$ marginals or the $\binom{d}{2}$ pairs for the correlation values. Even the generation of multivariate normal random vectors is parallelized through optimized matrix multiplication/decomposition routines. 

## Random vector generation `bigsimr::rvec` algorithm {#rand-vec-gen}

<mark>make this algorithm pretty later </mark>

1. Preprocessing for nonparameteric dependency matching.  
	(i) Convert from either ${\bf R_{Spearman}}$ or ${\bf R_{Kendall}}$ into the corresponding MVN input correlation ${\bf R_{Pearson}}$ via \@ref(eq:convertSpearman) or \@ref(eq:convertKendall), respectivity.  
	(ii) Check that ${\bf R_{Pearson}}$ is semi-positive definite.  
	(ii) If not find a close semi-positive ${\bf \widetilde{R}_{Pearson}}$ via `cor_nearPSD()`.  
2. NORTA transformation  
	(1) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_{Pearson}})$;  
	(2) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
3. Return step  
	(3) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

## Other high-performance `bigsimr` algorithms

<mark> ALEX, briefly discuss other algorithms here </mark>

- `cor_bounds()`  
- `cor_covert()` make sure that you are clear this is for only MVN.  
- `cor_fast()`  
- `cor_nearPSD()`  
