# Background {#background}

```{r ch010-background, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide", cache=F}
library(tidyverse)
library(GGally)
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

The `bigsimr` `R` package presented here provides multiple algorithms that operate with high-dimensional multivariate data; however, all these algorithms were originally designed to support a single task:
to generate random vectors drawn from multivariate probability distributions with given marginal distributions and dependency metrics.
Specifically, our goal is to efficiently simulate a large number, $B$, of random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with *correlated* components and heterogeneous marginal distributions, described via cumulative distribution functions (CDFs) $F_i$, where $d$ can be very large and still be computed in practically useful timescale.

When designing this methodology, we developed the following properties to guide our effort.
We divide the properties into two categories: (1) basic properties (BP) and "scaleability" properties (SP). 
The BPs are adapted from an existing criteria due to @Nik13a.
Our simulation strategy should allow:

* BP1: A wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of bivariate marginals can be equal to the number of dependence parameters.
* BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.

Moreover, the simulation method must *scale* to high dimensions:

* SP1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.

To fix ideas and provide example applications enabled via `bigsimr`, the next section describes a motivating data set.

## Motivating example: RNA-seq data

Simulating high-dimensional, non-normal, correlated data motivates this work --- in pursuit of modeling RNA-sequencing (RNA-seq) data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. 
The RNA-seq data-generating process involves counting how often a particular form of human messenger RNA (mRNA) is expressed in a biological sample.
RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in high-dimensional data.
For human-derived samples, this results in count data corresponding to over 20,000 genes (protein-coding genomic regions) or even over 77,000 isoforms when alternatively-spliced mRNA are counted [@Schissler2019].
Importantly, due to inherent biological processes, gene expression data exhibits correlation --- co-expression --- across genes [@BE07; @Schissler2018]. 

```{r ch010-myProb, include=FALSE, cache = TRUE}
myProb <- 0.95
```

```{r ch010-processBRCA, echo=FALSE, eval=TRUE, results='none', cache=F}
brcaRDS <- paste0( "data/brca", round(myProb*100), ".rds" )
if ( !file.exists( brcaRDS ) | !file.exists( './data/examplePatients.txt' ))  {
    ## full workflow simulating RNA-seq data
    allDat <- readRDS( file = "~/Downloads/complete_processed_tcga2stat_RNASeq2_with_clinical.rds" )
    lastClinical <- which( names(allDat) == 'tumorsize' )
    brca <- allDat[ allDat$disease == "BRCA", ]
    ## Patient ID
    writeLines( as.character( brca$patient ), './data/examplePatients.txt' ) 
    brca <- brca[ , (lastClinical+1):ncol(allDat) ]
    dim(brca) ## num of genes 20501
    ## compute the median expression! avoid the 0s
    brcaMedian <- apply(brca, 2, median)
    ## retain top (1-myProb)*100% highest expressing genes for illustration
    cutPoint <- quantile( x = brcaMedian, probs = myProb )
    ## genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    brca <- brca[ , genesToKeep ]
    ## ncol(brca) / 20501
    ## print(d <- length(genesToKeep))
    ## save data as rds
    saveRDS(brca, brcaRDS)
}
brca <- readRDS(file = brcaRDS)
d <- ncol(brca)
```

We illustrate our methodology using a well-studied Breast Invasive Carcinoma (BRCA) data set housed in The Cancer Genome Atlas (TCGA; see Acknowledgments).
For simplicity, we only consider high expressing genes.
In turn, we begin by filtering to retain the top `r (1-myProb)*100`% of highest- expressing genes (in terms of median expression) of the 20,501 gene measurements from $N=`r nrow(brca)`$ patients' tumor samples, resulting in $d=`r d`$ genes.
This gives a massive number of pairwise dependencies among the marginals (specifically, $`r choose(d,2)`$ correlation parameters).
We further process the data to illustrate our methodology's flexible and robust simulation scheme, while better aligning with the actual data-generating process, by rounding @Li2011c RNA-seq by Expectation Maximization (RSEM) values to counts.
Table \@ref(tab:ch010-realDataTab) displays RNA-seq counts for three selected high-expressing genes for the first five patients' breast tumor samples. 
 
```{r ch010-realDataTab, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE}
## Describe and plot real data
if ( !file.exists( './data/exampleGenes.rds' ) )  {
    set.seed(10192020)
    exampleGenes <- colnames(brca)[sort(sample(ncol(brca), 3))]
    saveRDS(exampleGenes, file = './data/exampleGenes.rds')
}
exampleGenes <- readRDS('./data/exampleGenes.rds')
## Add row names per WWP comment
small <-  brca %>% select( exampleGenes ) %>% as.data.frame
numPatients <- 5
smallForTable <- head( small, n = numPatients )
smallForTable$PatientID <- head( readLines('./data/examplePatients.txt'), n = numPatients )
smallForTable <- smallForTable[ , c(4, 1:3 ) ]
smallForTable[ , 2:4 ] <- round( smallForTable[ , 2:4 ] )
knitr::kable(smallForTable ,
      booktabs = TRUE,
      caption = "mRNA expression for three selected high-expressing genes (*RPL5*, *TKNIP*, and *VIM*) for the first five patients in the TCGA BRCA data set."
)
```

To help visualize the bivariate relationships for these three selected genes across all patients, Figure \@ref(fig:ch010-realDataFig) plots the marginal distributions and estimated Spearman's correlations (see Equation \@ref(eq:spearman) below).

```{r ch010-realDataFig, cache=F, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap="Marginal scatterplots, densities, and estimated pairwise Spearman's correlations for three example genes from Table 1. The data possess heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology."}
# lowerfun <- function(data,mapping){
#   ggplot(data = data, mapping = mapping)+
#     geom_point()+
#     scale_x_continuous(limits = c(-1.5,1.5))+
#     scale_y_continuous(limits = c(-1.5,1.5))
# }  
 
GGally::ggpairs( data = small,
                # lower = list( continuous = wrap(lowerfun)),
                upper = list(
                    continuous = wrap('cor', method = "spearman")
                ), 
                ) + theme_bw()
```

## Measures of dependency

In multivariate analysis, an analyst must select a metric to quantify dependency.
The most widely-known is the Pearson (product-moment) correlation coefficient that describes the linear association between two random variables $X$ and $Y$, and, it is given by

\begin{equation}
(\#eq:pearson)
\rho_P(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ var(X)var(Y)\right]^{1/2}}.
\end{equation}

As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector, the Pearson correlation completely describes the dependency between the components. 
For non-normal marginals with monotone correlation patterns, however, $\rho_P$ suffers some drawbacks and may mislead or fail to capture important relationships [@MK01].
Alternatively in these settings, analysts often prefer rank-based correlation measures to describe the degree of monotonic association.

Two nonparametric, rank-based measures common in practice are Spearman's correlation (denoted $\rho_S$) and Kendall's $\tau$. 
Define

\begin{equation}
(\#eq:spearman)
\rho_S(X,Y) = 3 \left[ P\left[ (X_1 - X_2)(Y_1-Y_3) > 0 \right] - P\left[ (X_1 - X_2)(Y_1-Y_3) < 0 \right] \right],
\end{equation}

\noindent where $(X_1, Y_1) \overset{d}{=} (X,Y), X_2 \overset{d}{=} X, Y_3 \overset{d}{=} Y$ with $X_2$ and $Y_3$ are independent of one other and of $(X_1, Y_1)$. 
Spearman's $\rho_S$ has an appealing correspondence as the Pearson's correlation coefficient on *ranks* of the values, thereby captures nonlinear yet monotone relationships.

Kendall's $\tau$, on the other hand, is the difference in probabilities of concordant and discordant pairs of observations $(X_i, Y_i)$ and $(X_j, Y_j)$.
By concordance we mean that orderings have the same direction (e.g., if $X_i < X_j$, then $Y_i < Y_j$) and is determined by the ranks of the values, not the values themselves.

Both $\tau$ and $\rho_S$ are *invariant to under monotone transformations* of the underlying random variates.
As we will describe more fully in the [Algorithms](#algorithms) section, this property is essential to scaleable match rank-based correlations with speed (SP1) and accuracy (SP2).

*Correspondence among Pearson, Spearman, $\tau$ correlations*.
There is no closed form, general correspondence among the rank-based measures and the Pearson correlation coefficient, as the marginal distributions $F_i$ are intrinsic in their calculation.
For *bivariate normal vectors*, however, the correspondence is well-known:

\begin{equation}
(\#eq:convertKendall)
\rho_{P} = sin \left( \tau \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's $\rho$ [@K58],

\begin{equation}
(\#eq:convertSpearman)
\rho_P = 2 \times sin \left( \rho_S \times \frac{\pi}{6} \right).
\end{equation}

These features are critical in our simulation algorithm to broaden the dependency measures supported by `bigsimr`, in a computationally effective manner.

*Discrete marginal considerations*.
Spearman's correlation for discrete marginal suffers issues whenever there is a nonzero probability of ties (for example, the Spearman's correlation of a discrete-valued random variable $X$ with itself could be less than 1; @MB13).
One remedy for this issue is to rescale Equation \@ref(eq:spearman). 
For two random variables $X,Y$ with probability mass functions (PMFs) or probability densities functions (PDFs) $p(x)$ and $q(y)$, respectively, define the rescaled Spearman's correlation as

\begin{equation}
(\#eq:spearmanRescaled)
\rho_{RS}(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}.
\end{equation}

Note that with continuous marginals the rescaling returns $\rho_S$.
For discrete marginals with large or infinite support, computing the adjustment factors $\sum_x p(x)^3$, $\sum_y p(y)^3$ over all large number of pairs becomes expensive (often violating desired property SP1).
Further the infinite sums must be approximated for count-valued data.

Also, for a discrete random variable $Y_i$, some care must be taken to define the quantile function $F_{i}^{-1}$.
Let 

\begin{equation}
F_{i}^{-1} = inf\{y:F_{i}(y) \geq u \}.
(\#eq:inverseCDF)
\end{equation}

*Marginal-dependent bivariate correlation bounds*. 
Given two marginal distributions, $\rho_P$ is not free to vary over the entire range of possible correlations $[-1,1]$.
The so-called *Frechet-Hoeffding bounds* are well-studied (see @Nelsen2007 and @BF17).
This gives strict restraints on the possible correlations and cannot be overcome through algorithm design.
In general, the bounds are given by

\begin{equation}
(\#eq:frechet)
\rho_P^{max} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho_P^{min} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where $U$ is a uniform random variable in $(0,1)$, and $F^{-1}_1, F^{-1}_2$ are the inverse CDFs of random variables $X_1$ and $X_2$, respectively. 
For discrete random variables, define $F^{-1}$ as in Equation \@ref(eq:inverseCDF).

## Gaussian copulas

There is a strong connection of our simulation strategy to Gaussian *copulas* (see @Nelsen2007 for a technical introduction).
A copula is a distribution function on $[0,1]^d$ that describes a multivariate probability distribution with standard uniform marginals.
This provides a powerful, natural way to characterize joint probability structures.
Consequently, the study of copulas is an important and active area of statistical theory and practice.

For any random vector ${\bf X}=(X_1, \ldots, X_d)$ with CDF $F$ and marginal CDFs $F_i$ there is a copula function 
$C(u_1, \ldots, u_d)$ satisfying

\[
F(x_1, \ldots,x_d) = \mathbb P(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), \,\,\, x_i\in \mathbb R, i=1,\ldots,d. 
\]  

A Gaussian copula has marginal CDFs as the normal CDF, $\Phi$. 
This representation corresponds to a multivariate normal distribution with standard normal marginal distributions and covariance matrix ${\bf R_P}$.
As the marginals are standardized to have unit variance, however, ${\bf R_P}$ is a Pearson correlation matrix. 
If $F_{{\bf R}}$ is the CDF of such a multivariate normal distribution, then the corresponding Gaussian copula $C_{{\bf R}}$ is defined through

\begin{equation}
(\#eq:gauss)
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

where $\Phi(\cdot)$ is the standard normal CDF.
Note that the copula $C_{{\bf R}}$ is the familar multivariate normal CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_P})$. 

Sklar's Theorem [@Sklar1959; @Ubeda-Flores2017] guarantees that given inverse CDFs $F_i^{-1}$s and a valid correlation matrix (within the Frechet bounds) a random vector can be obtained via transformations involving copula functions.
For example, using Gaussian copulas, we can construct a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)$ with $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$, via ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$ povides $Y_i \sim F_i, \, \forall \, i$ .
