# Example applications for RNA-seq data {#examples}

```{r ch050-LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide", cache=F}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
# reticulate::use_condaenv("bigsimr")
library(bigsimr)
library(tidyverse)
cores <- as.integer( parallel::detectCores() - 1 )
set.seed(10142020)
```

```{r ch050-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('050-examples.Rmd')
```

This section demonstrates how to simulate multivariate data using `bigsimr`, aiming to replicate the structure of high-dimensional dependent count data.
Simulating RNA-sequencing (RNA-seq) data is a primary motivating application of the proposed methodology, seeking scaleable Monte Carlo (MC) methods for realistic multivariate simulation for these data.
Intergene correlation is an inherent part of biological processes.
Yet many models do not account for this, leading to major distruptions to the operating characteristics of statistical estimation, testing, and prediction.
See @Efron2012 for a detailed discussion with related methods and see @Wu2012b, @Schissler2018, @Schissler2019 for applied examples.
The following subsections apply `bigsimr`'s methods to real RNA-seq data, including replicated an estimated parameteric structure, MC probability estimation, and MC correlation estimation efficiency assessment.

## Simulating High-Dimensional RNA-seq data

```{r ch050-readBRCA, echo=FALSE, eval=TRUE}
## full workflow simulating RNA-seq data
brca <- readRDS(file = brcaRDS)
## smaller set for prototyping
## d=5
## brca <- brca[ , 1:d]
```


In an illustration of our proposed methodology applied to real data, we seek to simulate RNA-sequencing data by producing simulated random vectors with assumed marginal distributions with estimated parameters.
Our goal is to replicate the structure of a breast cancer data set (BRCA data set from The Cancer Genome Atlas).
Specifically, we will simulate $B=10,000$ random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$
All these genes exhibit over-dispersion and, so, we proceed to estimate the NB parameters $(r_i, p_i), i=1,\ldots,d$ to determine the target marginal PMFs $g_i(y_i)$ (via method of moments).
Often researchers posit a negative binomial (NB) model as RNA-seq counts are often over-dispersed that a Poisson model would suggest.
This suggests simulating high-dimensional multivariate NB (MVB) with heterogeneous marginals would be useful tool in the development and evaluation of RNA-seq analytics.
This procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. 
To specify the simulation algorithm inputs, we estimate the Spearman correlation matrix $\bf{R^{Spearman}_Y}$ and marginal NB parameters.

<!--
All these genes exhibit over-dispersion and, so, we proceed to estimate the NB parameters $(r_i, p_i), i=1,\ldots,d$ to determine the target marginal PMFs $g_i(y_i)$ (via method of moments).
Notably, the $\hat{p}_i's$ are small --- ranging in $[3.934 \times 10^{-6} , 1.217 \times 10^{-2}]$.
To complete the simulation algorithm inputs, we estimate the Pearson correlation matrix $\bf{R_Y}$ and set that as the target correlation.
 -->

<!--
With the simulation targets specified, we proceed to simulate $N=10,000$ random vectors $\bf{Y}$ $=( Y_1,\ldots,Y_d)$ with target Pearson correlation $\bf{R_Y}$ and marginal PMFs $g_i(y_i)$ using a $\bf{T}$-Poisson hierarchy of Kind II.
Specifically, we first employ the \emph{direct Gaussian copula} approach to generate $N$ random vectors following a standard multivariate Gamma distribution $\bf{T}$ with shape parameters $r_i$ equal to the target $n_i$ and Pearson correlation matrix $\bf{R_T}$.
Care must be taken when setting the specifying $\bf{R}$ (refer to Equation \ref{gay.cop.pdf}) --- we employ Equation \ref{mix.poi.corr} to compute the scaling factors $c_{i,j}$ and adjust the underlying correlations to ultimately match the target $\bf{R_Y}$.
Notably, of the $525,825$ pairwise correlations from the $1026$ genes, no scale factor was less than $0.9907$, indicating the model can produce essentially the entire range of possible correlations.
Here we are satisfied with approximate matching of the specified Gamma correlation and set $\bf{R}$ = $\bf{R_T}$ in our Gaussian copula scheme ($\bf{R}$ indicating the specified multivariate Gaussian correlation matrix).
Finally, we generate the desired random vector $Y_i=N_i(t_i)$ by simulating Poisson counts with expected value $\mu_i=\lambda_i \times T_i$, for $i=1,\ldots,d$ (with $\lambda_i=\frac{(1-p_i)}{p_i}$) and repeat $N=10,000$ times.
-->

With our goal in mind, we first estimate the desired correlation matrix using the fast implementation provided by `bigsimr`:

```{r ch050-estRhoBRCA, echo=TRUE, eval=TRUE}
## 1. Estimate Spearman's correlation on the count data
## corType <- 'pearson'
corType <- 'spearman'
system.time( nb_Rho <- bigsimr::cor_fast( brca, method = corType ) )
```

Next we estimate the marginal parameters. 
Here we use the basic method of moments (MoM) to estimate the marginal parameters for the multivariate negative binomial model.
We model the marginals distributions as coming from the same probability family (NB) yet are hetereogeneous in terms of the parameters probability and size $(p_i, n_i)$ for $i,\ldots,d$.
The code below features some advanced `R` utilities for concise, rapid, and generalizable target marginal specification:

<!-- 
TYPESTE MoM estimators here.
your comment -->

```{r ch050-nbHelpers, echo=TRUE}
make_nbinom_alist <- function(sizes, probs) {
  lapply(1:length(sizes), function(i) {
    substitute(qnbinom(size = s, prob = p), 
               list(s = sizes[i], p = probs[i]))
  })
}
## make_nbinom_alist(c(20, 21, 22), c(0.3, 0.4, 0.5))
nbinom_mom <- function(x) {
  m <- mean(x)
  s <- sd(x)
  s2 <- s^2
  p <- m/s2
  r <- m^2 / (s2 - m)
  c(r, p)
}
```

Once the functions are defined to complete marginal estimation, we specify the desired multivariate negative binomial distribution and generate the desired random vectors:

```{r ch050-estMargins, echo = TRUE, eval = TRUE}
sizes <- apply( unname(as.matrix(brca)), 2, nbinom_mom )[1, ]
probs <- apply( unname(as.matrix(brca)), 2, nbinom_mom )[2, ]
```

Notably, the marginal NB probabilities $\hat{p}_i's$ are small --- ranging in $[`r min(probs)` , `r max(probs)`]$.
This gives rise to highly variable counts and, typically, less restriction on potential pairwise correlation pairs.
Once the functions are defined/executed to complete marginal estimation, we specify targets and generate the desired random vectors using `rvec`:

```{r ch050-runBRCA-echo, echo=TRUE, eval=FALSE, cache = FALSE}
## Set the number of random vectors
n <- 10000
## construct margins
nb_margins <- make_nbinom_alist(sizes, probs)
## run sims
sim_nbinom <- rvec(n, nb_Rho, nb_margins, type = corType,
                   ensure_PSD = TRUE, cores = cores) 
colnames(sim_nbinom) <- names(brca)
```

```{r ch050-runBRCA, echo=FALSE, results=FALSE, cache = FALSE}
brcaSimRDS <- paste0( "results/brca", round(myProb*100), "sim.rds" )
if( !file.exists(brcaSimRDS) ) {
    ## Set the number of random vectors
    n <- 10000
    ## construct margins
    nb_margins <- make_nbinom_alist(sizes, probs)
    ## run sims
    system.time( sim_nbinom <- rvec(n, nb_Rho, nb_margins, type = corType,
                                    ensure_PSD = TRUE, cores = cores) )
    colnames(sim_nbinom) <- names(brca)
    saveRDS( sim_nbinom, brcaSimRDS )
}
sim_nbinom <- readRDS( brcaSimRDS )
```

Figure \@ref(fig:ch050-simDataFig) displays the simulated counts and pairwise relationships for our example genes in \@ref(fig:ch010-realDataFig).
Simulated counts roughly mimick the observed data but with a smoothier appearance due to the assumed parameter form.

```{r ch050-simDataFig, echo = FALSE, eval=TRUE, cache=T, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap='Simulated data for 3 selected high-expressing genes, replicating the observed data structure.'}
## retrieve the examples from an earlier chapter
## there is probably a better way to do this
exampleGenes <- readRDS(file = './data/exampleGenes.rds') 
GGally::ggpairs(data = as.data.frame(sim_nbinom[ ,exampleGenes] ))
```

Figure \@ref(fig:ch050-figBRCA) displays the aggregated results of our simulation by comparing the specified target parameter (horizontal axes) with the corresponding quantities estimated from the simulated data (vertical axes).
The evaluation shows that the simulated counts approximately match the target parameters and exhibit the full range of estimated correlation from the data.
Utilizing 15 CPU threads in a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, the simulation completed just shy of 7.5 minutes.

```{r ch050-figBRCA, echo=F, eval=T, cache=T, fig.cap="`bigsimr` produces simulated random vectors from a multivariate negative binomial that replicate the estimated structure from an RNA-seq data set. The dashed red lines indicated equality between estimated parameters from simulated data(vertical axes) and the specified target parameters (horizontal axes)."}
par(mfrow=c(1,3))
## estimate rho from sims and plot
nb_Rho_hat <- cor_fast(sim_nbinom, method = corType)
plot(nb_Rho[lower.tri(nb_Rho)], nb_Rho_hat[lower.tri(nb_Rho_hat)],
     xlim = c(-1, 1), ylim = c(-1, 1),
     pch = 4, cex = 1, col = rgb(0, 0, 1, 0.05),
     xlab = "Target Correlation", ylab = "Estimated Correlation",
     main = "Correlation Matching")
abline(a = 0, b = 1, col="red", lty="dashed")
## estimate prob,size from sims and plot
nbinom_size_prob_hat <- apply(sim_nbinom, 2, nbinom_mom)
plot(sizes, nbinom_size_prob_hat[1,],
     pch = 4, cex = 1, col = rgb(0, 0, 1, 1),
     xlab = "Target Size", ylab = "Estimated Size",
     main = "Marginal NB Size Matching")
abline(a=0, b=1, col="red", lty="dashed")
plot(log(probs), log(nbinom_size_prob_hat[2,]),
     pch = 4, cex = 1, col = rgb(0, 0, 1, 1),
     xlab = "Target Log Probability", ylab = "Estimated Log Probability",
     main = "Marginal NB Probability Matching")
abline(a=0, b=1, col="red", lty="dashed")
par(mfrow=c(1,1))
```

## Simulation-based UHD joint probability calculations

To conduct statistical inference a critical task is to evaluate the joint probability mass (or density) function:

$$
P( {\bf Y} = {\bf y} ), y_i \in \chi_i.
$$

where $\chi_i$ is the sample space for the $i^{th}$ component of the random vector $\bf{Y}$.
Compact representations with convenient computational forms are rare for high-dimensional constructions, especially with hetereogeneous, correlated marginal distributions (or margins of mixed data types).
Given a large number simulated vectors as produced above, estimated probabilities are readily given by counting the proportion of simulated vectors meeting the desired condition.
In our motivating application, one may ask what is the probability that all genes expressed greater than a certain threshold value ${ \bf y}_0$.

Then we estimate

$$
\hat{P}( {\bf Y} >= {\bf y_0 } ) = \sum_{b=1}^B I( {\bf Y^{(b) }} > {\bf y_0 } ) / B
$$

where ${\bf Y^{(b)} }$ is the $b^{th}$ simulated vector in a total of $B$ simulation replicates and $I ( )$ is the indicator function.
For example, we can estimate from our $B=10,000$ simulated vectors the probability that all genes are expressed (i.e., ${\bf y}_i \geq 1, \forall \; i$).

```{r ch050-densityEvaluation, echo=TRUE, eval=TRUE, cache=F}
d <- ncol(sim_nbinom)
B <- nrow(sim_nbinom)
threshold <- rep( 1, d)
mean(apply( sim_nbinom,  1,
           function(X, y0=threshold) {
               all( X > y0) }
           ))
```


<!-- 
## Monte Carlo estimation of MSE 

Monte Carlo (MC) methods offer flexible and straightfoward statistical inferenec in this typically difficult analytic setting.
MC methods are routinely used many statistical inferential tasks including estimation, hypothesis testing, error rates, and empirical interval coverage rates.
For an concise introduction to these Methos, see, for example @Rizzo2007, Ch. 6.
As a simple example, let's now employ `bigsimr` to estimate the mean squared error (MSE) of the method of moments estimators we used in section X.

Specifically, 
TYPESET THE MATH HERE. REF to MoM above.

```{r ch050-mom-mse, echo=TRUE, eval=FALSE, cache=FALSE}
## system( "rm results/mseSizes.rds")
brcaSimMSEsizesRDS <- paste0( "results/brca", round(myProb*100), "simMSEsizes.rds" )
if ( !file.exists( brcaSimMSEsizesRDS) )  {
    n <- nrow(brca)
    m <- 100
    momSizes <- replicate(n = m, expr =
                                     { tmpSim <- rvec(n, nb_Rho, brcaMargins, type = corType, ensure_PSD = TRUE, cores = cores)
                                         tmpMom <- apply( unname(as.matrix(tmpSim)), 2, estimateNegBinMoM )
                                         tmpSizes <- unlist( lapply( X = tmpMom, function(x) x$size ) )
                                     }
                          )
    sqDiffSizes <- apply( momSizes, 2, function(x) { (x  - sizes)^2 } )
    mseSizes <- rowMeans( sqDiffSizes )
    saveRDS(mseSizes, brcaSimMSEsizesRDS)
}
mseSizes <- readRDS( brcaSimMSEsizesRDS )
```

Figure X.

```{r ch050-fig-mseSizes, echo = FALSE, eval=FALSE, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap='Real data for the 3 selected high-expressing genes'}
mseSizes <- readRDS(file = brcaSimMSEsizesRDS)
qplot(mseSizes, bins = 20) + theme_bw()
```

One could then compare the MSE of the baisc MoM estimation scheme to other strategies ( such as MLE, trimmed approaches, etc.) and decide whether a more computational costly approach is warrented or if the MoM will suffice.

your comment -->
