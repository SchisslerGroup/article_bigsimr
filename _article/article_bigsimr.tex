\documentclass[
]{jss}

\usepackage[utf8]{inputenc}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
\\
}
\title{Simulating High-Dimensional Multivariate Data using the \pkg{Bigsimr} Package}

\Plainauthor{}
\Plaintitle{Simulating High-Dimensional Multivariate Data using the Bigsimr Package}
\Shorttitle{\pkg{Bigsimr}: Simulate High-Dimensional Data}

\Abstract{
It is critical to realistically simulate data when employing Monte Carlo techniques and evaluating statistical methodology. Measurements are often correlated and high dimensional in this era of big data, such as data obtained through high-throughput biomedical experiments. Due to computational complexity and a lack of user-friendly software available to simulate these massive multivariate constructions, researchers resort to simulation designs that posit independence or perform arbitrary data transformations. This article introduces the \textbackslash pkg\{Bigsimr\} R package. This high-level package provides a flexible and scalable procedure to simulate high-dimensional random vectors with arbitrary marginal distributions and known Pearson, Spearman, or Kendall correlation matrix. \textbackslash pkg\{Bigsimr\} contains additional high-performance features, including multi-core algorithms to simulate random vectors, estimate correlation, and compute the nearest correlation matrix. Monte Carlo studies quantify the accuracy and scalability of our approach, up to \(d=10,000\). Finally, we demonstrate example applications enabled via \textbackslash pkg\{Bigsimr\} by applying these methods to a motivating dataset --- RNA-sequencing data obtained from breast cancer tumor samples.
}

\Keywords{multivariate simulation, high-dimensional data, nonparametric correlation, Gaussian copula, RNA-sequencing data, breast cancer}
\Plainkeywords{multivariate simulation, high-dimensional data, nonparametric correlation, Gaussian copula, RNA-sequencing data, breast cancer}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
  }

% Pandoc citation processing

% Pandoc header
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{ragged2e}

\usepackage{setspace}
\setstretch{2.0}




\begin{document}

\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Massive high-dimensional (HD) data sets are now commonplace in many areas of scientific inquiry. As new methods are developed for data, a fundamental challenge lies in designing and conducting simulation studies to assess the operating characteristics of analyzing such proposed methodology --- such as false positive rates, statistical power, interval coverage, and robustness --- often with comparison to existing methods. Further, efficient simulation empowers statistical computing strategies, such as the parametric bootstrap \citep{Chernick2008} to simulate from a hypothesized null model, providing inference in analytically challenging settings. Such Monte Carlo (MC) techniques become difficult for high-dimensional data using existing algorithms and tools. This is particularly true when simulating massive multivariate, non-normal distributions, arising naturally in many fields of study.

As many have noted, it can be vexing to simulate dependent, non-normal/discrete data, even for low dimensional settings \citep{MB13, XZ19}. For continuous non-normal multivariate data, the well-known NORmal To Anything (NORTA) algorithm \citep{Cario1997} and other copula approaches \citep{Nelsen2007} are well-studied, with flexible, robust software available \citep{Yan2007, Chen2001}. Yet these approaches do not scale in a timely fashion to high-dimensional problems \citep{Li2019gpu}. For discrete data, early simulation strategies had major flaws, such as failing to obtain the full range of possible correlations (e.g., admitting only positive correlations: see \citet{Park1996}). While more recent approaches \citep{MB13, Xia17, BF17} have largely remedied this issue for low-dimensional problems, the existing tools are not designed to scale to high dimensions.

Another central issue lies in characterizing dependence between components in the high-dimensional random vector. The choice of correlation in practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g., non-normal, discrete, infinite support, etc). For normal data, the Pearson product-moment correlation describes the dependency perfectly. As we will see, however, simulating arbitrary random vectors that match a target Pearson correlation matrix is computationally intense \citep{Chen2001, Xia17}. On the other hand, an analyst might consider use of nonparametric correlation measures to better characterize monotone, non-linear dependence, such as Spearman's \(\rho\) and Kendall's \(\tau\). Throughout, we focus on matching these nonparametric dependence measures, as our aim lies in modeling non-normal data and these rank-based measures possess invariance properties favorable in our proposed methodology. We do, however, provide Pearson matching with some caveats.

With all this in mind, we present a scalable, flexible multivariate simulation algorithm. The crux of the method lies in the construction of a Gaussian copula in the spirit of the NORTA procedure. Further, we introduce the \texttt{bigsimr} R package that provides high-performance software implementing our algorithm. The algorithm design leverages useful properties of nonparametric correlation measures, namely invariance under monotone transformation and well-known closed-form relationships between dependence measures for the multivariate normal (MVN) distribution.

Our study proceeds by providing background information, including a description of a motivating example application: RNA-sequencing (RNA-seq) breast cancer data. Then we describe and justify our simulation methodology and related algorithms. Next, we detail an illustrative low-dimensional example of basic use of the \texttt{bigsimr} R package. Then we proceed with Monte Carlo studies under various bivariate distributional assumptions to assess accuracy. We conclude the Monte Carlo evaluations by summarizing the computation time for increasingly higher dimensional vectors. After the MC evaluations, we simulate random vectors motivated by our RNA-seq example, evaluate the accuracy, and provide example statistical computing tasks, namely MC estimation of joint probabilities and evaluating HD correlation estimation efficiency. Finally, we discuss the method's utility, limitations, and future directions.

\hypertarget{background}{%
\section{Background}\label{background}}

The \texttt{bigsimr} \texttt{R} package presented here provides multiple algorithms that operate with high-dimensional multivariate data; however, all these algorithms were originally designed to support a single task: to generate random vectors drawn from multivariate probability distributions with given marginal distributions and dependency metrics. Specifically, our goal is to efficiently simulate a large number, \(B\), of HD random vectors \({\bf Y}=(Y_1, \ldots, Y_d)^\top\) with \emph{correlated} components and heterogeneous marginal distributions, described via cumulative distribution functions (CDFs) \(F_i\).

When designing this methodology, we developed the following properties to guide our effort. We divide the properties into two categories: (1) basic properties (BP) and ``scalability'' properties (SP). The BPs are adapted from an existing criteria due to \citet{Nik13a}. A suitable simulation strategy should possess the following properties:

\setstretch{1.5}

\begin{itemize}
\tightlist
\item
  BP1: A wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
\item
  BP2: Flexible dependence, meaning that the number of bivariate marginals can be equal to the number of dependence parameters.
\item
  BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.
\end{itemize}

\setstretch{2.0}

Moreover, the simulation method must \emph{scale} to high dimensions:

\setstretch{1.5}

\begin{itemize}
\tightlist
\item
  SP1: Procedure must scale to high dimensions with practical compute times.
\item
  SP2: Procedure must scale to high dimensions while maintaining accuracy.
\end{itemize}

\setstretch{2.0}

\hypertarget{motivating-example-rna-seq-data}{%
\subsection{Motivating example: RNA-seq data}\label{motivating-example-rna-seq-data}}

Simulating high-dimensional, non-normal, correlated data motivates this work --- in pursuit of modeling RNA-sequencing (RNA-seq) data \citep{Wang2009b, Conesa2016b} derived from breast cancer patients. The RNA-seq data-generating process involves counting how often a particular form of human messenger RNA (mRNA) is expressed in a biological sample. RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in high-dimensional data. For human-derived samples, this results in count data corresponding to over 20,000 genes (protein-coding genomic regions) or even over 77,000 isoforms when alternatively-spliced mRNA are counted \citep{Schissler2019}. Importantly, due to inherent biological processes, gene expression data exhibit correlation (co-expression) across genes \citep{BE07, Schissler2018}.

We illustrate our methodology using a well-studied Breast Invasive Carcinoma (BRCA) data set housed in The Cancer Genome Atlas (TCGA; see Acknowledgments).
For ease of modeling and simplicity of exposition, we only consider high expressing genes. In turn, we begin by filtering to retain the top 1000 of the highest-expressing genes (in terms of median expression) of the over 20,000 gene measurements from \(N=878\) patients' tumor samples. This gives a great number of pairwise dependencies among the marginals (specifically, \(\ensuremath{4.995\times 10^{5}}\) correlation parameters). Table \ref{tab:ch010-realDataTab} displays RNA-seq counts for three selected high-expressing genes for the first five patients' breast tumor samples. To help visualize the bivariate relationships for these three selected genes across all patients, Figure \ref{fig:ch010-realDataFig} displays the marginal distributions and estimated Spearman's correlations.

\begin{CodeChunk}
\begin{CodeInput}
R> set.seed(2020-02-25)
R> num_genes    <- 3
R> num_patients <- 5
R> gene_sample  <- sample(example_genes[1:1000], num_genes)
R> 
R> cap <- paste0("mRNA expression for three selected high-expressing genes, ",
+               paste(gene_sample, collapse = ", "), 
+               ", for the first five patients in the TCGA BRCA data set.")
R> 
R> small_brca <- example_brca %>%
+   select(all_of(gene_sample)) %>%
+   head(n = num_patients) %>%
+   as_tibble(rownames = "Patient ID") %>%
+   mutate(`Patient ID` = str_sub(`Patient ID`, end = 12))
R> 
R> small_brca %>%
+   kable(format = "latex", booktabs = TRUE, caption = cap)
\end{CodeInput}
\begin{table}

\caption{\label{tab:ch010-realDataTab}mRNA expression for three selected high-expressing genes, STAU1, FKBP1A, NME2, for the first five patients in the TCGA BRCA data set.}
\centering
\begin{tabular}[t]{lrrr}
\toprule
Patient ID & STAU1 & FKBP1A & NME2\\
\midrule
TCGA-A1-A0SB & 10440 & 11354 & 17655\\
TCGA-A1-A0SD & 21523 & 20221 & 14653\\
TCGA-A1-A0SE & 21733 & 22937 & 35251\\
TCGA-A1-A0SF & 11866 & 19650 & 16551\\
TCGA-A1-A0SG & 12486 & 12089 & 10434\\
\bottomrule
\end{tabular}
\end{table}

\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> ggpairs(
+   data = example_brca[, gene_sample],
+   upper = list(continuous = wrap('cor', method = "spearman"))
+ ) + 
+   theme_bw() +
+   theme(axis.text.x = element_text(angle = -90, vjust = 0.5))
\end{CodeInput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch010-realDataFig-1} 

}

\caption[Marginal scatterplots, densities, and estimated pairwise Spearman's correlations for three example genes from Table 1]{Marginal scatterplots, densities, and estimated pairwise Spearman's correlations for three example genes from Table 1. The data possess outliers, heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology.}\label{fig:ch010-realDataFig}
\end{figure}
\end{CodeChunk}

\hypertarget{measures-of-dependency}{%
\subsection{Measures of dependency}\label{measures-of-dependency}}

In multivariate analysis, an analyst must select a metric to quantify dependency.
The most widely-known is the Pearson (product-moment) correlation coefficient that describes the linear association between two random variables \(X\) and \(Y\), and, it is given by

\begin{equation}
\rho_P(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ \mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}}.
\label{eq:pearson}
\end{equation}

As \citet{MB13} and \citet{MK01} discuss, for a bivariate normal \((X,Y)\) random vector, the Pearson correlation completely describes the dependency between the components. For non-normal marginals with monotone correlation patterns, however, \(\rho_P\) suffers some drawbacks and may mislead or fail to capture important relationships \citep{MK01}. Alternatively in these settings, analysts often prefer rank-based correlation measures to describe the degree of monotonic association.

Two nonparametric, rank-based measures common in practice are Spearman's correlation (denoted \(\rho_S\)) and Kendall's \(\tau\). Spearman's \(\rho_S\) has an appealing correspondence as the Pearson correlation coefficient on \emph{ranks} of the values, thereby captures nonlinear yet monotone relationships. Kendall's \(\tau\), on the other hand, is the difference in probabilities of concordant and discordant pairs of observations, \((X_i, Y_i)\) and \((X_j, Y_j)\), with concordance meaning that orderings have the same direction (e.g., if \(X_i < X_j\), then \(Y_i < Y_j\)). Note that concordance is determined by the ranks of the values, not the values themselves.

Both \(\tau\) and \(\rho_S\) are \emph{invariant under monotone transformations} of the underlying random variates. As we will describe more fully in the \protect\hyperlink{algorithms}{Algorithms} section, this property enables matching rank-based correlations with speed (SP1) and accuracy (SP2).

\emph{Correspondence among Pearson, Spearman, and Kendall correlations}

There is no closed form, general correspondence among the rank-based measures and the Pearson correlation coefficient, as the marginal distributions \(F_i\) are intrinsic in their calculation. For \emph{bivariate normal vectors}, however, the correspondence is well-known:

\begin{equation}
\label{eq:convertKendall}
\rho_{P} = \sin \left( \tau \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's \(\rho\) \citep{K58},

\begin{equation}
\label{eq:convertSpearman}
\rho_P = 2 \times \sin \left( \rho_S \times \frac{\pi}{6} \right).
\end{equation}

\emph{Marginal-dependent bivariate correlation bounds}

Given two marginal distributions, \(\rho_P\) is not free to vary over the entire range of possible correlations \([-1,1]\). The so-called \emph{Frechet-Hoeffding bounds} are well-studied \citep{Nelsen2007, BF17}. These constraints cannot be overcome through algorithm design. In general, the bounds are given by

\begin{equation}
\label{eq:frechet}
\rho_P^{max} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho_P^{min} = \rho_P \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where \(U\) is a uniform random variable on \((0,1)\) and \(F^{-1}_1, F^{-1}_2\) are the inverse CDFs of \(X_1\) and \(X_2\), respectively, definitely by \eqref{eq:inverseCDF} when the variables are discrete.

\begin{equation}
F_{i}^{-1} = \inf\{y:F_{i}(y) \geq u \}.
\label{eq:inverseCDF}
\end{equation}

\hypertarget{gaussian-copulas}{%
\subsection{Gaussian copulas}\label{gaussian-copulas}}

There is a strong connection of our simulation strategy to Gaussian \emph{copulas} (see \citet{Nelsen2007} for a technical introduction). A copula is a distribution function on \([0,1]^d\) that describes a multivariate probability distribution with standard uniform marginals. This provides a powerful, natural way to characterize joint probability structures. Consequently, the study of copulas is an important and active area of statistical theory and practice.

For any random vector \({\bf X}=(X_1, \ldots, X_d)^\top\) with CDF \(F\) and marginal CDFs \(F_i\) there is a copula function \(C(u_1, \ldots, u_d)\) satisfying

\begin{equation}
F(x_1, \ldots, x_d) = {\mathbb P}(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), 
\label{eq:copula}
\end{equation}

\(x_i \in {\mathbb R}, i=1,\ldots,d.\)

A Gaussian copula has marginal CDFs that are all standard normal, \(F_i = \Phi, \forall \, i\). This representation corresponds to a multivariate normal (MVN) distribution with standard normal marginal distributions and covariance matrix \({\bf R_P}\). As the marginals are standardized to have unit variance, however, \({\bf R_P}\) is a Pearson correlation matrix. If \(F_{{\bf R}}\) is the CDF of such a multivariate normal distribution, then the corresponding Gaussian copula \(C_{{\bf R}}\) is defined through

\begin{equation}
\label{eq:gauss}
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

where \(\Phi(\cdot)\) is the standard normal CDF. Note that the copula \(C_{{\bf R}}\) is the familiar multivariate normal CDF of the random vector \((\Phi(X_1), \ldots, \Phi(X_d))\), where \((X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_P})\).

Sklar's Theorem \citep{Sklar1959, Ubeda-Flores2017} guarantees that given inverse CDFs \(F_i^{-1}\)s and a valid correlation matrix (within the Frechet bounds) a random vector can be obtained via transformations involving copula functions. For example, using Gaussian copulas, we can construct a random vector \({\bf Y} = (Y_1, \ldots, Y_d)^\top\) with \(Y_i \sim F_i\), viz.~\(Y_i = F_i^{-1}(\Phi(X_i)), i=1, \ldots, d\), where \((X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_P})\).

\hypertarget{algorithms}{%
\section{Algorithms}\label{algorithms}}

This section describes our methods involved in simulating a random vector \(\bf Y\) with \(Y_i\) components for \(i=1,\ldots,d\). Each \(Y_i\) has a specified marginal CDF \(F_i\) and its inverse \(F^{-1}_i\). To characterize dependency, every pair \((Y_i, Y_j)\) has a given Pearson correlation \(\rho_P\), Spearman correlation \(\rho_S\), and/or Kendall's \(\tau\). The method can be described as a \emph{high-performance Gaussian copula} (Equation \eqref{eq:gauss}) providing a high-dimensional NORTA-inspired algorithm.

\hypertarget{normal-to-anything-norta}{%
\subsection{NORmal To Anything (NORTA)}\label{normal-to-anything-norta}}

The well-known NORTA algorithm \citep{Cario1997} simulates a random vector \(\bf Y\) with variance-covariance matrix \(\Sigma_{\bf Y}\). Specifically, NORTA algorithm proceeds as:

\setstretch{1.5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simulate a random vector \(\bf Z\) with \(d\) \emph{independent and identically distributed} (iid) standard normal components.
\item
  Determine the input matrix \(\Sigma_{\bf Z}\) that corresponds with the specified output \(\Sigma_{\bf Y}\).
\item
  Produce a Cholesky factor \(M\) of \(\Sigma_{\bf Z}\) such that \(M M^{\prime}=\Sigma_{\bf Z}\).
\item
  Set \(X\) by \(X \gets MZ\).
\item
  \(\text{Return} \; Y \; \text{where} \; Y_i \gets F_i^{-1}[\Phi(X_i)], \; i=1,...,d\).
\end{enumerate}

\setstretch{2.0}

With modern parallel computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multi-core and/or graphical-processing-unit (GPU) accelerated algorithms --- providing fast scalability using readily-available hardware.

Matching specified Pearson correlation coefficients exactly (step 2 above), however, is computationally costly. In general, there is no closed-form correspondence between the components of the input \(\Sigma_{\bf Z}\) and target \(\Sigma_{\bf Y}\). Matching the correlations involves evaluating or approximating \(\binom{d}{2}\) integrals of the form

\begin{equation}
    \mathrm{E}\left[Y_i Y_j\right] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} F_i^{-1}\left[\Phi(z_i)\right] F_j^{-1}\left[\Phi(z_j)\right] \phi(z_i, z_j, \rho_z) dz_i dz_j,
    \label{eq:pearsonIntegralRelation}
\end{equation}

where \(\phi(\cdot)\) is the joint probability distribution function of two correlated standard normal variables. For HD data, (nearly) exact evaluation may become too costly to enable practical simulation studies. For low-dimensional problems, however, methods and tools exist to match Pearson correlations precisely; see \citep{Xia17} and the publicly available \texttt{nortaRA} R package \citep{Chen2001}. As described later, to enable HD Pearson matching we approximate these integrals.

\emph{NORTA in higher dimensions}

Sklar's theorem provides a useful characterization of multivariate distributions through copulas. Yet the choice of copula-based simulation algorithm affects which joint distributions may be simulated. Even in low-dimensional spaces (e.g., \(d=3\)), there exist valid multivariate distributions with \emph{feasible} Pearson correlation matrices that NORTA cannot match exactly \citep{LH75}. This occurs when the bivariate transformations are applied to find the input correlation matrix, yet when combined the resultant matrix is indefinite. These situations do occur, even using exact analytic calculations. Such problematic target correlation matrices are termed \emph{NORTA defective}.

\citet{GH02} conducted a Monte Carlo study to estimate the probability of encountering NORTA defective matrices while increasing the dimension \(d\). They found that for what is now considered low-to-moderate dimensions (\(d \approx 20\)), almost \emph{all} feasible matrices are NORTA defective. This stems from the concentration of measure near the boundary of the space of all possible correlation matrices as dimension increases. Unfortunately, it is precisely near this boundary that NORTA defective matrices reside.

There is hope, however, as \citet{GH02} also showed that replacing an indefinite input correlation matrix with a close proxy will give approximate matching to the target --- with adequate performance for moderate \(d\). This provides evidence that our nearest positive definite (PD) augmented approach will maintain reasonable accuracy if our input matching scheme returns an indefinite matrix, at least for the rank-based matching scheme described above.

\hypertarget{rand-vec-gen}{%
\subsection{Random vector generator}\label{rand-vec-gen}}

We now describe our algorithm to generate random vectors, which mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:

\setstretch{1.5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mapping step

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \tightlist
    \item
      Convert the target Spearman correlation matrix \(R_S\) to the corresponding MVN Pearson correlation \(R_X\). Alternatively,
    \end{enumerate}
  \item
    (i') Convert the target Kendall \(\tau\) matrix \(R_K\) to the corresponding MVN Pearson correlation \(R_X\). Alternatively,
  \item
    (i'\,') Convert the target Pearson correlation matrix to \(R_P\) to the corresponding, approximate MVN Pearson correlation \(R_X\).
  \end{itemize}
\item
  Check admissibility and, if needed, compute the nearest correlation matrix.

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \tightlist
    \item
      Check that \(R_X\) is a correlation matrix, a positive definite matrix with 1's along the diagonal.
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      If \(R_X\) is a correlation matrix, the input matrix is \emph{admissible} in this scheme. Otherwise
    \end{enumerate}
  \item
    (ii') Replace \(R_X\) with the nearest correlation matrix \(\tilde{R}_X\), in the Frobenius norm.
  \end{itemize}
\item
  Gaussian copula

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \tightlist
    \item
      Generate \({\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_X)\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \setcounter{enumii}{1}
    \tightlist
    \item
      Transform \({\bf X}\) to \({\bf U} = (U_1, \ldots, U_d)\) viz.~\(U_i=\Phi(X_i)\), \(i=1, \ldots, d\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumii{(\roman{enumii})}
    \setcounter{enumii}{2}
    \tightlist
    \item
      Return \({\bf Y} = (Y_1, \ldots, Y_d)\), where \(Y_i=F_i^{-1}(U_i)\), \(i=1, \ldots, d\).
    \end{enumerate}
  \end{itemize}
\end{enumerate}

\setstretch{2.0}

\emph{Step 1}

The first two descriptions of the \emph{Mapping step} employ the closed-form relationships between \(\rho_S\) and \(\tau\) with \(\rho_P\) for bivariate normal random variables via Equations \eqref{eq:convertKendall} and \eqref{eq:convertSpearman}, respectively (implemented as \texttt{cor\_covert}). Initializing our algorithm to match the nonparametric correlations by computing these equations for all pairs is computationally trivial.

For the computationally expensive process to match Pearson correlations, we approximate Equation \eqref{eq:pearsonIntegralRelation} for all pairs of margins. To this end, we implement the approximation scheme introduced by \citet{xiao2019matching}. Briefly, the many double integrals of the form in \eqref{eq:pearsonIntegralRelation} are approximated by weighted sums of Hermite polynomials. Matching coefficients for pairs of continuous distributions is made tractable by this method, but for discrete distributions (especially discrete distributions with large support sets or infinite support), the problem is not so simple or efficient.

Our solution is to approximate discrete distributions by a continuous distribution. The question then becomes that of which distribution to use. We found that Generalized S-Distributions \citep{muino2006gs} solve this problem by approximating a wide range of unimodal distributions, both continuous and discrete. Using the GS-Distribution approximation of discrete distributions in Pearson matching scheme above yields favorable results.

\emph{Step 2}

Once \(R_X\) has been determined, we check admissibility of the adjusted correlation via the steps described above. If \(R_X\) is not a valid correlation matrix, then we compute the nearest correlation matrix. Finding the nearest correlation matrix is a common statistical computing problem. The defacto default function in \texttt{R} is \texttt{Matrix::nearPD}, an alternating projection algorithm due to \citet{higham2002computing}. As implemented the function fails to scale to HD. Instead, we provide the quadratically convergent algorithm based on the theory of strongly semi-smooth matrix functions (\citet{qi2006quadratically}). The nearest correlation matrix problem can be written down as the following convex optimization problem:

\begin{align*}
    \mathrm{min}\quad & \frac{1}{2} \Vert G - X \Vert^2 \\
    \mathrm{s.t.}\quad & X_{ii} = 1, \quad i = 1, \ldots , n, \\
    & X \in S_{+}^{n}
\end{align*}

For nonparametric correlation measures, our algorithm allows the generation of high-dimensional multivariate data with arbitrary marginal distributions with a broad class of admissible Spearman correlation matrices and Kendall \(\tau\) matrices. The admissible classes consist of the matrices that map to a Pearson correlation matrix for a MVN. In particular, if we let \(X\) be MVN with \(d\) components and set

\begin{align*}
\Omega_P &= \{ R_P : R_P \textrm{ is a correlation matrix for } X \} \\
\Omega_K &= \{ R_K : R_K \textrm{ is a correlation matrix for } X \} \\
\Omega_S &= \{ R_S : R_S \textrm{ is a correlation matrix for } X \} \\
\end{align*}

then there are 1-1 mappings between these sets. We conjecture informally that the sets of admissible \(R_S\) and \(R_K\) are not highly restrictive. In particular, \(R_P\) is approximately \(R_S\) for a MVN, suggesting that the admissible set \(\Omega_S\) should be flexible as \(R_P\) can be any PD matrix with 1's along the diagonal. We provide methods to check whether a target \(R_S\) is an element of the \emph{admissible set} \(\Omega_S\) (and similarly for \(R_K\)).

There is an increasing probability of encountering a non-admissible correlation matrix as dimension increases. In our experience, the mapping step for large \(d\) almost always produces a \(R_X\) that is not a correlation matrix. In Section \ref{package} we provide a basic description of how to quantify and control the approximation error. Further, the RNA-seq example in Section \ref{examples} provides an illustration of this in practice.

\emph{Step 3}

The final step implements a NORTA-inspired, Gaussian copula approach to produce the desired margins. Steps 1 and 2 determine the MVN Pearson correlation values that will eventually match the target correlation. Then all that is required is a fast MVN simulator, a standard normal CDF, and well-defined quantile functions for marginals. The MVN is transformed to a copula (distribution with standard uniform margins) by applying the normal CDF \(\phi(\cdot)\). The quantile functions \(F^{-1}\) are applied across the margins to return the desired random vector \({\bf Y}\).

\section[bigsimr-pkg]{The \pkg{bigsimr} R package}\label{package}

\begin{CodeChunk}
\begin{CodeInput}
R> box::use(
+   patchwork[...],
+   dplyr[...],
+   ggplot2[...],
+   tidyr[drop_na],
+   bigsimr[bigsimr_setup, distributions_setup],
+   JuliaCall[julia_call]
+ )
R> 
R> Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
R> bs <- bigsimr_setup(pkg_check = FALSE)
\end{CodeInput}
\begin{CodeOutput}
Julia version 1.5.4 at location /home/alex/julia-1.5.4/bin will be used.
\end{CodeOutput}
\begin{CodeOutput}
Loading setup script for JuliaCall...
\end{CodeOutput}
\begin{CodeOutput}
Finish loading setup script for JuliaCall.
\end{CodeOutput}
\begin{CodeInput}
R> dist <- distributions_setup()
\end{CodeInput}
\end{CodeChunk}

This section describes a low-dimensional (2D) random vector simulation workflow via the \texttt{bigsimr} \texttt{R} package (\url{https://github.com/SchisslerGroup/r-bigsimr}). The package \texttt{bigsimr} provides an interface to the native code written in Julia, registered as the \texttt{Bigsimr} Julia package (\url{https://github.com/adknudson/Bigsimr.jl}). In addition to the native Julia \texttt{Bigsimr} package and \texttt{R} interface \texttt{bigsimr}, we also provide a python interface \texttt{bigsimr} (\url{https://github.com/SchisslerGroup/python-bigsimr/}) that interfaces with the Julia \texttt{Bigsimr} package. The Julia package provides a high-performance implementation of our proposed random vector generation algorithm and associated functions (see Section \ref{algorithms}).

The subsections below describe the basic use of \texttt{bigsimr} by stepping through an example workflow using the data set \texttt{airquality} that contains daily air quality measurements in New York, May to September 1973 \citep{Chambers1983}. This workflow proceeds from setting up the computing environment, to data wrangling, estimation, simulation configuration, random vector generation, and, finally, result visualization.

\hypertarget{bivariate-example-description}{%
\subsection{Bivariate example description}\label{bivariate-example-description}}

We illustrate the use of \texttt{bigsimr} motivated by the New York air quality data set (\texttt{airquality}) included in the R \texttt{datasets} package. First, we load the \texttt{bigsimr} library and a few other convenient data science packages, including the syntactically-elegant \texttt{tidyverse} suite of \texttt{R} packages. The code chunk below prepares the computing environment:

\begin{CodeChunk}
\begin{CodeInput}
R> library("tidyverse")
R> library("bigsimr")
R> # Activate multithreading in Julia
R> Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
R> # Load the Bigsimr and Distributions Julia packages
R> bs <- bigsimr_setup()
R> dist <- distributions_setup()
\end{CodeInput}
\end{CodeChunk}

For simplicity and to provide a minimal working example, we consider bivariate simulation of the two \texttt{airquality} variables \texttt{Temperature}, in degrees Fahrenheit, and \texttt{Ozone} level, in parts per billion.

\begin{CodeChunk}
\begin{CodeInput}
R> df <- airquality %>% select(Temp, Ozone) %>% drop_na()
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> glimpse(df)
\end{CodeInput}
\begin{CodeOutput}
Rows: 116
Columns: 2
$ Temp  <int> 67, 72, 74, 62, 66, 65, 59, 61, 74, 69, 66, 68, 58, 64, 66, 57, ~
$ Ozone <int> 41, 36, 12, 18, 28, 23, 19, 8, 7, 16, 11, 14, 18, 14, 34, 6, 30,~
\end{CodeOutput}
\end{CodeChunk}

Figure \ref{fig:ch030-aq-joint-dist} visualizes the bivariate relationship between Ozone and Temperature. We aim to simulate random two-component vectors mimicking this structure. The margins are not normally distributed; particularly the Ozone level exhibits a strong positive skew.

\begin{CodeChunk}
\begin{CodeInput}
R> p0 <- ggplot(df, aes(Temp, Ozone)) +
+   geom_point(size = 1) +
+   theme(legend.position = "none") + 
+   labs(x = "Temperature")
R> 
R> pTemp <- ggplot(df, aes(Temp)) + 
+   geom_density() +
+   theme(axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         axis.text = element_blank(),
+         axis.ticks = element_blank())
R> 
R> pOzone <- ggplot(df, aes(Ozone)) + 
+     geom_density() +
+   theme(axis.title.y = element_blank(),
+         axis.title.x = element_blank(),
+         axis.ticks = element_blank(),
+         axis.text = element_blank()) +
+   coord_flip()
R> 
R> pTemp + plot_spacer() + p0 + pOzone + 
+   plot_layout(widths = c(3,1), heights = c(1, 3)) 
\end{CodeInput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch030-aq-joint-dist-1} 

}

\caption[Bivariate scatterplot of Ozone vs]{Bivariate scatterplot of Ozone vs. Temp with estimated marginal densities. We model the Ozone data as marginally log-normal and the Temperature data as normal.}\label{fig:ch030-aq-joint-dist}
\end{figure}
\end{CodeChunk}

Next, we specify the marginal distributions and correlation coefficient (both type and magnitude). Here the analyst is free to be creative. For this example, we avoid goodness-of-fit considerations to determine the marginal distributions. But it seems sensible without domain knowledge to estimate these quantities from the data, and \texttt{bigsimr} contains fast functions designed for this task.

\hypertarget{specifying-marginal-distributions}{%
\subsection{Specifying marginal distributions}\label{specifying-marginal-distributions}}

Based on the estimated densities in Figure \ref{fig:ch030-aq-joint-dist}, we assume \texttt{Temp} is normally distributed and \texttt{Ozone} is log-normally distributed, as the latter values are positive and skewed. We use the well-known, unbiased estimators for the normal distribution's parameters and maximum likelihood estimators for the log-normal parameters:

\begin{CodeChunk}
\begin{CodeInput}
R> df %>% select(Temp) %>% 
+   summarise_all(.funs = c(mean = mean, sd = sd))
\end{CodeInput}
\begin{CodeOutput}
   mean    sd
1 77.87 9.485
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> mle_mean <- function(x) mean(log(x))
R> mle_sd <- function(x) mean( sqrt( (log(x) - mean(log(x)))^2 ) )
R> df %>% 
+   select(Ozone) %>% 
+   summarise_all(.funs = c(meanlog = mle_mean, sdlog = mle_sd))
\end{CodeInput}
\begin{CodeOutput}
  meanlog  sdlog
1   3.419 0.6967
\end{CodeOutput}
\end{CodeChunk}

Next, we configure the input marginals for later input into \texttt{rvec}. The marginal distributions are specified using \texttt{Julia}'s \texttt{Distributions} package and stored in a vector.

\begin{CodeChunk}
\begin{CodeInput}
R> margins <- c(dist$Normal(mean(df$Temp), sd(df$Temp)),
+              dist$LogNormal(mle_mean(df$Ozone), mle_sd(df$Ozone)))
\end{CodeInput}
\end{CodeChunk}

\hypertarget{specifying-correlation}{%
\subsection{Specifying correlation}\label{specifying-correlation}}

As mentioned, the user must decide how to describe correlation based on the particulars of the problem. For non-normal data and for improved simulation accuracy/scalability in our scheme, we advocate the use of Spearman's \(\rho\) correlation matrix \(R_S\) and Kendall's \(\tau\) correlation matrix \(R_K\). We also support Pearson correlation coefficient matching, while cautioning the user to check the performance for their parametric multivariate model (see \protect\hyperlink{simulations}{Monte Carlo evaluations} below for evaluation strategies and guidance). These estimation methods are classical approaches, not designed for high-dimensional correlation estimation (see the \href{\%7B\#discussion}{Conclusion and Discussion} sections for more on this).

\begin{CodeChunk}
\begin{CodeInput}
R> (R_S <- bs$cor(as.matrix(df), bs$Spearman))
\end{CodeInput}
\begin{CodeOutput}
      [,1]  [,2]
[1,] 1.000 0.774
[2,] 0.774 1.000
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{checking-target-correlation-matrix-admissibility}{%
\subsection{Checking target correlation matrix admissibility}\label{checking-target-correlation-matrix-admissibility}}

First we use \texttt{cor\_bounds} to ensure that the pairwise target correlation values are valid prior to the mapping step which constructs \(R_X\) for the MVN input into NORTA (see Section \ref{algorithms}). \texttt{cor\_bounds} estimates the pairwise lower and upper correlation bounds using the Generate, Sort, and Correlate algorithm of \citet{DH2011}.

\begin{CodeChunk}
\begin{CodeInput}
R> bounds <- bs$cor_bounds(margins)
R> bounds$lower
\end{CodeInput}
\begin{CodeOutput}
        [,1]    [,2]
[1,]  1.0000 -0.8902
[2,] -0.8902  1.0000
\end{CodeOutput}
\begin{CodeInput}
R> bounds$upper
\end{CodeInput}
\begin{CodeOutput}
       [,1]   [,2]
[1,] 1.0000 0.8876
[2,] 0.8876 1.0000
\end{CodeOutput}
\end{CodeChunk}

Since our single estimated Spearman correlation is within the theoretical bounds, the correlation is valid as input to \texttt{rvec}. But even if the 2-dimensional bounds are satisfied for each pair of margins, this does not guarantee the feasibility of a \(d-\)variate distribution \citep{BF17}.

To provide higher dimensional feasibility/admissibility checking, we begin by mapping using \texttt{cor\_convert} and check admissibility.

\begin{CodeChunk}
\begin{CodeInput}
R> # Step 1. Mapping
R> (R_X <- bs$cor_convert(R_S, bs$Spearman, bs$Pearson))
\end{CodeInput}
\begin{CodeOutput}
       [,1]   [,2]
[1,] 1.0000 0.7886
[2,] 0.7886 1.0000
\end{CodeOutput}
\begin{CodeInput}
R> # Step 2. Check admissibility
R> bs$iscorrelation(R_X)
\end{CodeInput}
\begin{CodeOutput}
[1] TRUE
\end{CodeOutput}
\end{CodeChunk}

The bounds on the Pearson correlation coefficient \(R_P\) between these margins are restricted as seen below in our MC estimated correlation bounds:

\begin{CodeChunk}
\begin{CodeInput}
R> bs$cor_bounds(margins[1], margins[2], bs$Pearson, n_samples = 1e6)
\end{CodeInput}
\begin{CodeOutput}
Julia Object of type NamedTuple{(:lower, :upper),Tuple{Float64,Float64}}.
(lower = -0.8822730883942045, upper = 0.8833146839651883)
\end{CodeOutput}
\end{CodeChunk}

Our MC estimate of the bounds slightly overestimates the theoretic bounds of \((-0.881, 0.881)\). An analytic derivation is presented as an Appendix.

\hypertarget{simulating-random-vectors}{%
\subsection{Simulating random vectors}\label{simulating-random-vectors}}

Finally, we arrive at the main function of \texttt{bigsimr}: \texttt{rvec}. We now simulate \(B=10,000\) random vectors from the assumed joint distribution of Ozone levels and Temp.

\begin{CodeChunk}
\begin{CodeInput}
R> x <- bs$rvec(10000, R_X, margins)
R> df_sim <- as.data.frame(x)
R> colnames(df_sim) <- colnames(df)
\end{CodeInput}
\end{CodeChunk}

Figure \ref{fig:ch030-plot-sim} plots the 10,000 simulated points.

\begin{CodeChunk}
\begin{CodeInput}
R> p1 <- df_sim %>%
+   ggplot(aes(Temp, Ozone)) +
+   geom_density_2d_filled() +
+   theme(legend.position = "none") +
+   labs(x = "Simulated Temperature", y = "Simulated Ozone") +
+   scale_y_continuous(limits = c(0, max(df$Ozone))) +
+   scale_x_continuous(limits = range(df$Temp))
R> 
R> p1Temp <- ggplot(df_sim, aes(Temp)) + 
+   geom_density(alpha = 0.5, fill = "lightseagreen") +
+   theme(axis.title.x = element_blank(),
+         axis.title.y = element_blank(),
+         axis.text = element_blank(),
+         axis.ticks = element_blank())
R> 
R> p1Ozone <- ggplot(df_sim, aes(Ozone)) + 
+   geom_density(alpha = 0.5, fill = "lightseagreen") + 
+   theme(axis.title.y = element_blank(),
+         axis.title.x = element_blank(),
+         axis.ticks = element_blank(),
+         axis.text = element_blank()) +
+   coord_flip() 
R> 
R> p1Temp + plot_spacer() + p1 + p1Ozone + 
+   plot_layout(widths = c(3,1), heights = c(1, 3))
\end{CodeInput}
\begin{CodeOutput}
Warning: Removed 342 rows containing non-finite values (stat_density2d_filled).
\end{CodeOutput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch030-plot-sim-1} 

}

\caption[Contour plot and marginal densities for the simulated bivariate distribution of Air Quality Temperatures and Ozone levels]{Contour plot and marginal densities for the simulated bivariate distribution of Air Quality Temperatures and Ozone levels. The simulated points mimic the observed data with respect to both the marginal characteristics and bivariate association.}\label{fig:ch030-plot-sim}
\end{figure}
\end{CodeChunk}

\hypertarget{simulations}{%
\section{Monte Carlo evaluations}\label{simulations}}

Before applying our methodology to real data simulation, we conduct several Monte Carlo studies to investigate method performance. Since marginal parameter matching in our scheme is essentially a sequence of univariate inverse probability transforms, the challenging aspects are the accuracy of dependency matching and computational efficiency at high dimensions. To evaluate our methods in those respects, we design the following numerical experiments to first assess accuracy of matching dependency parameters in bivariate simulations and then time the procedure in increasingly large dimension \(d\).

\hypertarget{bivariate-experiments}{%
\subsection{Bivariate experiments}\label{bivariate-experiments}}

We select bivariate simulation configurations to ultimately simulate our motivating discrete-valued RNA-seq example, and, so we proceed in increasing complexity, leading to the model in our motivating application in Section \ref{examples}. We begin with empirically evaluating the dependency matching across all three supported correlations --- Pearson's, Spearman's, and Kendall's --- in identical, bivariate marginal configurations. For each pair of identical margins, we vary the target correlation across \(\Omega\), the set of possible admissible values for correlation type, to evaluate the simulation's ability to obtain the theoretic bounds. The simulations progress from bivariate normal, to bivariate gamma (non-normal yet continuous), and bivariate negative binomial (mimicking RNA-seq counts).

Table \ref{tab:sims} lists our identical-marginal, bivariate simulation configurations. We increase the simulate replicates \(B\) to check that our results converge to the target correlations and gauge statistical efficiency. We select distributions beginning with a standard multivariate normal (MVN) as we expect the performance to be exact (up to MC error) for all correlation types. Then, we select a non-symmetric continuous distribution: a standard (rate =1) two-component multivariate gamma (MVG). Finally, we select distributions and marginal parameter values that are motivated by our RNA-seq data, namely values proximal to probabilities and sizes estimated from the data (see \href{examples}{Example applications} for estimation details). Thus we arrive at a multivariate negative binomial (MVNB) \(p_1 = p_2 = 3\times10^{-4}, r_1 = r_2 = 4, \rho \in \Omega\).

Table: \label{tab:sims} Identical margin, bivariate simulation configurations to evaluate correlation matching accuracy and efficiency.

\begin{table}[]
\begin{tabular}{@{}lcr@{}}
\toprule
Simulation Reps $B$ & Correlation Types & Identical-margin 2D distribution \\ \midrule
$1000$ & Pearson ($\rho_P$) & ${\bf Y} \sim MVN( \mu= 0 , \sigma = 1, \rho_i ), i=1,\ldots,100$ \\
$10,000$ & Spearman ($\rho_S$) & ${\bf Y} \sim MVG( shape = 10, rate = 1, \rho_i ), i=1,\ldots,100$ \\
$100,000$ & Kendall ($\tau$) & ${\bf Y} \sim MVNB(p = 3\times10^{-4}, r = 4,\rho_i), i=1,\ldots,100$ \\ \bottomrule
\end{tabular}
\end{table}

For each of the unique 9 simulation configurations described above, we estimate the correlation bounds and vary the correlations along a sequence of 100 points evenly placed within the bounds, aiming to explore \(\Omega\). Specifically, we set correlations \(\{ \rho_1 = ( \hat{l} + \epsilon), \rho_2 = (\hat{l} + \epsilon) + \delta, \ldots, \rho_{100} = (\hat{u} - \epsilon) \}\), with \(\hat{l}\) and \(\hat{u}\) being the estimated lower and upper bounds, respectively, and increment value \(\delta\). The adjustment factor, \(\epsilon=0.01\), is introduced to handle numeric issues when the bound is specified exactly.

\begin{CodeChunk}
\begin{CodeInput}
R> bivariate_normal_sims %>%
+     ggplot(aes(rho, rho_hat, color = type)) +
+     geom_point() +
+     geom_abline(slope = 1) +
+     facet_wrap(~ type + N) + 
+     theme_bw()
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> bivariate_gamma_sims %>%
+     ggplot(aes(rho, rho_hat, color = type)) +
+     geom_point() +
+     geom_abline(slope = 1) +
+     facet_wrap(~ type + N) + 
+     theme_bw()
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> bivariate_nbinom_sims %>%
+     ggplot(aes(rho, rho_hat, color = type)) +
+     geom_point() +
+     geom_abline(slope = 1) +
+     facet_wrap(~ type + N) + 
+     theme_bw()
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> allDat <- bind_rows(
+     select(bivariate_normal_sims, margins, type, N, rho, rho_hat),
+     select(bivariate_gamma_sims, margins, type, N, rho, rho_hat),
+     select(bivariate_nbinom_sims, margins, type, N, rho, rho_hat)
+ ) %>%
+     mutate(margins = factor(margins, levels=c("norm", "gamma", "nbinom")),
+            type = factor(type, levels=c("Pearson", "Spearman", "Kendall")),
+            N = factor(N))
\end{CodeInput}
\end{CodeChunk}

Figure \ref{fig:ch040-bPlot} displays the aggregated bivariate simulation results. Table \ref{tab:ch040-BiError} contains the mean absolute error (MAE) in reproducing the desired dependency measures for the three bivariate scenarios.

\begin{CodeChunk}
\begin{CodeInput}
R> tabMAE <- allDat %>%
+     group_by(N, type, margins) %>%
+     summarize(MAE = mean(abs(rho - rho_hat))) %>%
+     ungroup()
\end{CodeInput}
\begin{CodeOutput}
`summarise()` has grouped output by 'N', 'type'. You can override using the `.groups` argument.
\end{CodeOutput}
\begin{CodeInput}
R> kable(tabMAE, booktabs = TRUE, format = "latex",
+       linesep = c("", "", "\\addlinespace"),
+       col.names = c("No. of random vectors",
+                     "Correlation type",
+                     "Distribution",
+                     "Mean abs. error"),
+       caption = "Average abolute error in matching the target dependency across the entire range of possible correlations for each bivariate marginal.")
\end{CodeInput}
\begin{table}

\caption{\label{tab:ch040-BiError}Average abolute error in matching the target dependency across the entire range of possible correlations for each bivariate marginal.}
\centering
\begin{tabular}[t]{lllr}
\toprule
No. of random vectors & Correlation type & Distribution & Mean abs. error\\
\midrule
1000 & Pearson & norm & 0.0168\\
1000 & Pearson & gamma & 0.0159\\
1000 & Pearson & nbinom & 0.0188\\
\addlinespace
1000 & Spearman & norm & 0.0203\\
1000 & Spearman & gamma & 0.0180\\
1000 & Spearman & nbinom & 0.0165\\
\addlinespace
1000 & Kendall & norm & 0.0118\\
1000 & Kendall & gamma & 0.0099\\
1000 & Kendall & nbinom & 0.0089\\
\addlinespace
10000 & Pearson & norm & 0.0048\\
10000 & Pearson & gamma & 0.0055\\
10000 & Pearson & nbinom & 0.0066\\
\addlinespace
10000 & Spearman & norm & 0.0058\\
10000 & Spearman & gamma & 0.0054\\
10000 & Spearman & nbinom & 0.0060\\
\addlinespace
10000 & Kendall & norm & 0.0034\\
10000 & Kendall & gamma & 0.0031\\
10000 & Kendall & nbinom & 0.0037\\
\addlinespace
1e+05 & Pearson & norm & 0.0016\\
1e+05 & Pearson & gamma & 0.0016\\
1e+05 & Pearson & nbinom & 0.0032\\
\addlinespace
1e+05 & Spearman & norm & 0.0018\\
1e+05 & Spearman & gamma & 0.0019\\
1e+05 & Spearman & nbinom & 0.0013\\
\addlinespace
1e+05 & Kendall & norm & 0.0011\\
1e+05 & Kendall & gamma & 0.0010\\
1e+05 & Kendall & nbinom & 0.0010\\
\bottomrule
\end{tabular}
\end{table}

\end{CodeChunk}

Overall, the studies show that our methodology is generally accurate across the entire range of possible correlation values all three dependency measures, at least in these limited simulation settings for the rank-based correlations. Our Pearson matching performs nearly as well as Spearman or Kendall, except for a slight increase in error for negative binomial case. This is due the particularly large counts generated with our choice of parameters for \(p\) and \(r\).

\begin{CodeChunk}
\begin{CodeInput}
R> # https://www.datanovia.com/en/blog/how-to-change-ggplot-facet-labels/
R> # New facet label names
R> repsLabs <- paste0("B=", c("1,000", "10,000", "100,000") )
R> names(repsLabs) <- c(1000, 10000, 100000)
R> typeLabs <- c("Pearson", "Spearman", "Kendall")
R> names(typeLabs) <- c("Pearson", "Spearman", "Kendall")
R> 
R> # Set colors
R> ## RColorBrewer::display.brewer.all()
R> numColors <- 4
R> numGroups <- length(levels(allDat$margins))
R> myColors <- rev(RColorBrewer::brewer.pal(n = numColors, name = "Greys")[ ((numColors - numGroups) + 1):numColors])
R> 
R> allDat %>%
+     ggplot(aes(x = rho, y = rho_hat, color = margins)) +
+     geom_point(size = 2) +
+     scale_color_manual(values = myColors ) +
+     geom_abline(slope = 1, linetype = "dashed") +
+     labs(x = "Specified Correlation", y = "Estimated Correlation") +
+     facet_wrap(~ type + N, labeller = labeller(N = repsLabs, type = typeLabs)) +
+     theme_bw() +
+     theme(legend.position = "bottom", legend.direction = "horizontal")
\end{CodeInput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch040-bPlot-1} 

}

\caption[Bivariate simulations match target correlations across the entire range of feasible correlations]{Bivariate simulations match target correlations across the entire range of feasible correlations. The horizontal axis plots the specified target correlations for each bivariate margin. Normal margins are plotted in dark dark grey, gamma in medium grey, and negative binomial in light grey. As the number of simulated vectors $B$ increases from left to right, the variation in estimated correlations (vertical axis) decreases. The dashed line indicates equality between the specified and estimated correlations.}\label{fig:ch040-bPlot}
\end{figure}
\end{CodeChunk}

\hypertarget{scale-up-to-high-dimensions}{%
\subsection{Scale up to High Dimensions}\label{scale-up-to-high-dimensions}}

With information of our method's accuracy from a low-dimensional perspective, we now turn to assessing whether \texttt{bigsimr} can scale to larger dimensional problems with practical computation times. Specifically, we ultimately generate \(B=1,000\) random vectors for \(d=\{100, 250, 500, 1000, 2500, 5000, 10000\}\) for each correlation type, \(\{Pearson, Spearman, Kendall\}\) while timing the algorithm's major steps.

To mimic the workflow, we first produce a synthetic ``data set'' by completing the following steps:

\setstretch{1.5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Produce heterogeneous gamma marginals by randomly selecting the \(j^{th}\) gamma shape parameter from \(U_j \sim uniform(1,10), j=1,\ldots,d\) and the \(j^{th}\) rate parameter from \(V_j \sim exp(1/5), j=1,\ldots,d\), with the constant parameters determined arbitrarily.
\item
  Produce a random full-rank Pearson correlation matrix via \texttt{cor\_randPD} of size \(d \times d\).
\item
  Simulate a ``data set'' of \(1,000 \times d\) random vectors via \texttt{rvec} (without matching the Pearson exactly).
  \setstretch{2.0}
\end{enumerate}

With the synthetic data set in hand, we complete and time the following 4 steps:

\setstretch{1.5}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  Estimate the correlation matrix from the ``data'' in the \emph{Compute Correlation} step.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Map the correlations to initialize the algorithm (Pearson to Pearson, Spearman to Pearson, or Kendall to Pearson) in the \emph{Adjust Correlation} step.
\item
  Check whether the mapping produces a valid correlation matrix and, if not, find the nearest PD correlation matrix in the \emph{Check Admissibility} step.
\item
  Simulate \(1,000\) vectors via the Inverse Transform method in the \emph{Simulate Data} step.
  \setstretch{2.0}
\end{enumerate}

The experiments are conducted on a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, with all 16 threads employed during computation. The results for \(d \leq 500\) are fast with all times (except Kendall) under 3 seconds. Table \ref{tab:ch040-moderateDtab} displays the total computation time of the algorithms steps 1 through 3, including estimation step i.

\begin{CodeChunk}
\begin{CodeInput}
R> dat_long <- benchmark_dependences %>%
+   select(-total_time, -n_sim, -needed_near_pd) %>%
+   pivot_longer(cols = c(corr_time:sim_time), names_to = "Step", values_to = "Time") %>%
+   mutate(dim = factor(dim),
+          Step = factor(Step,
+                        levels = c("corr_time", "adj_time", "admiss_time", "sim_time"),
+                        labels = c("Compute Correlation",
+                                   "Adjust Correlation",
+                                   "Check Admissibility",
+                                   "Simulate Data"))) %>%
+     rename(Correlation = corr_type, Dimensions = dim)
R> 
R> tabTimes  <- dat_long %>%
+     filter(Dimensions %in% c(100, 250, 500)) %>%
+     group_by(Dimensions, Correlation) %>%
+     summarize('Total Time(Seconds)' = sum(Time)) %>%
+     ungroup()
\end{CodeInput}
\begin{CodeOutput}
`summarise()` has grouped output by 'Dimensions'. You can override using the `.groups` argument.
\end{CodeOutput}
\begin{CodeInput}
R> tabTimes %>%
+     kable(booktabs = TRUE,
+           linesep = c("", "", "\\addlinespace"),
+           col.names = c("Dimension",
+                         "Correlation type",
+                         "Total Time (Seconds)"),
+           caption = 'Total time to produce 10,000 random vectors with a random correlation matrix and hetereogeneous gamma margins.')
\end{CodeInput}
\begin{table}

\caption{\label{tab:ch040-moderateDtab}Total time to produce 10,000 random vectors with a random correlation matrix and hetereogeneous gamma margins.}
\centering
\begin{tabular}[t]{llr}
\toprule
Dimension & Correlation type & Total Time (Seconds)\\
\midrule
100 & Pearson & 0.080\\
100 & Spearman & 0.036\\
100 & Kendall & 0.116\\
\addlinespace
250 & Pearson & 0.286\\
250 & Spearman & 0.371\\
250 & Kendall & 0.366\\
\addlinespace
500 & Pearson & 2.932\\
500 & Spearman & 0.176\\
500 & Kendall & 2.183\\
\bottomrule
\end{tabular}
\end{table}

\end{CodeChunk}

The results with larger dimensional vectors show scalability to ultra-high dimensions for all three correlation types, although the total times do become much larger.
Figure \ref{fig:ch040-largeDfig} displays computation times for \(d=\{1000, 2500, 5000, 10000\}\). For \(d\) equal to 1000 and 2500, the total time is under a couple of minutes. At \(d\) of 5000 and 10,000, Pearson correlation matching in the \emph{Adjust Correlation} step becomes costly. Interestingly, Pearson is actually faster than Kendall for \(d=10,000\) due to bottlenecks in \emph{Compute Correlation} and \emph{Check Admissibility}. Uniformly, matching Spearman correlations is faster, with total times under 5 minutes for \(d=10,000\), making Spearman correlations the most computationally-friendly dependency type. With this in mind, we scaled the simulation to \(d=20,000\) for the Spearman type and obtained the 1,000 vectors in under an hour (data not shown). In principle, this would enable the simulation of an entire human-derived RNA-seq data set. We note that for a given target correlation matrix and margins, steps i, 1, and 2 only need to be computed once and the third step, \emph{Simulate Data}, is fast under all schemes for all \(d\) under consideration.

\begin{CodeChunk}
\begin{CodeInput}
R> # Set colors
R> numColors <- 5
R> numGroups <- 4
R> myColors <- rev(brewer.pal(n=numColors, name="Greys")[((numColors-numGroups)+1):numColors])
R> 
R> dat_long %>%
+     filter(Dimensions %in% c(1000, 2500, 5000, 10000, 20000)) %>%
+     mutate(`Time (minutes)` = Time / 60) %>%
+     ggplot(aes(Correlation, `Time (minutes)`, fill=Step)) +
+     geom_bar(position = "stack", stat = "identity") +
+     scale_fill_manual(values = myColors ) +
+     scale_y_continuous(breaks = seq(0, 80, 10),
+                      minor_breaks = NULL) +
+     facet_grid(. ~ Dimensions) +
+     theme(axis.text.x = element_text(angle = -90))
\end{CodeInput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch040-largeDfig-1} 

}

\caption[Computation times as d increases]{Computation times as d increases.}\label{fig:ch040-largeDfig}
\end{figure}
\end{CodeChunk}

\emph{Limitations, conclusions, and recommendations}

In the bivariate studies, we chose arbitrary simulation parameters for three distributions, moving from the Gaussian to the discrete and non-normal, multivariate negative binomial. Under these conditions, the simulated random vectors sample the desired bivariate distribution across the entire range of pairwise correlations for the three dependency measures. The simulation results could differ for other choices of simulation settings. Specifying extreme correlations near the boundary or Frechet bounds could result in poor simulation performance. Fortunately, it is straightforward to evaluate simulation performance by using strategies similar to those completed above. We expect our random vector generation to perform well for the vast majority of NORTA-feasible correlation matrices, but advise to check the performance before making inferences/further analyses.

Somewhat surprising, Kendall estimation and nearest PD computation scale poorly compared to Spearman and, even, approximate Pearson matching. In our experience, Kendall computation times are sensitive to the number of cores, benefiting from multi-core parallelization. This could mitigate some of the current algorithmic/implementation shortcomings. Despite this, Kendall matching is still feasible for most high-dimensional data sets. Finally, we note that one could use our single-pass algorithm \texttt{cor\_fastPD} to produce a `close' (not nearest PD) to scale to even higher dimensions with greater loss of accuracy.

\hypertarget{examples}{%
\section{RNA-seq data applications}\label{examples}}

This section demonstrates how to simulate multivariate data using \texttt{bigsimr}, aiming to replicate the structure of high-dimensional dependent count data. In an illustration of our proposed methodology, we seek to simulate RNA-sequencing data by producing simulated random vectors mimicking the observed data and its generating process. Modeling RNA-seq using multivariate probability distributions is natural as inter-gene correlation is an inherent part of biological processes \citep{Wang2009b}. And yet, many models do not account for this, leading to major disruptions to the operating characteristics of statistical estimation, testing, and prediction. See \citet{Efron2012} for a detailed discussion with related methods and \citet{Wu2012b}, \citet{Schissler2018}; \citet{Schissler2019} for applied examples. The following subsections apply \texttt{bigsimr}'s methods to real RNA-seq data, including replicating an estimated parametric structure, MC probability estimation, and MC evaluation of correlation estimation efficiency.

\hypertarget{simulating-high-dimensional-rna-seq-data}{%
\subsection{Simulating High-Dimensional RNA-seq data}\label{simulating-high-dimensional-rna-seq-data}}

\begin{CodeChunk}
\begin{CodeInput}
R> d <- 1000
R> brca1000 <- example_brca %>%
+   select(all_of(1:d)) %>%
+   mutate(across(everything(), as.double))
\end{CodeInput}
\end{CodeChunk}

We begin by estimating the structure of the TCGA BRCA RNA-seq data set (see \href{background}{Background}). Ultimately, we will simulate \(B=10,000\) random vectors \({\bf Y}=(Y_1, \ldots, Y_d)^\top\) with \(d=1000\). We assume a multivariate negative binomial (MVNB) model as RNA-seq counts are often over-dispersed and correlated. Since all \(d\) selected genes exhibit over-dispersion (data not shown), we proceed to estimate the NB parameters \((r_i, p_i), i=1,\ldots,d\), to determine the target marginal PMFs \(f_i\). To complete specification of the simulation algorithm inputs, we estimate the Spearman correlation matrix \({ \bf R}_{S}\) to characterize dependency.

With this goal in mind, we first estimate the desired correlation matrix using the fast implementation provided by \texttt{bigsimr}:

\begin{CodeChunk}
\begin{CodeInput}
R> # Estimate Spearman's correlation on the count data
R> R_S <- bs$cor(as.matrix(brca1000), bs$Spearman)
\end{CodeInput}
\end{CodeChunk}

Next, we estimate the marginal parameters. We use the method of moments (MoM) to estimate the marginal parameters for the multivariate negative binomial model. While, marginal distributions are from the same probability family (NB), they are heterogeneous in terms of the parameters probability and size \((p_i, n_i)\) for \(i,\ldots,d\). The functions below support this estimation for later use in \texttt{rvec}.

\begin{CodeChunk}
\begin{CodeInput}
R> make_nbinom_margins <- function(sizes, probs) {
+   margins <- lapply(1:length(sizes), function(i) {
+     dist$NegativeBinomial(sizes[i], probs[i])
+   })
+   do.call(c, margins)
+ }
\end{CodeInput}
\end{CodeChunk}

We apply these estimators to all 1000 genes across the 878 patients:

\begin{CodeChunk}
\begin{CodeInput}
R> nbinom_fit <- apply(brca1000, 2, mom_nbinom)
R> sizes <- nbinom_fit["size",]
R> probs <- nbinom_fit["prob",]
R> nb_margins <- make_nbinom_margins(sizes, probs)
\end{CodeInput}
\end{CodeChunk}

Notably, the estimated marginal NB probabilities \(\{ \hat{p}_i \}\) are small --- ranging in the interval \([\ensuremath{7.5305\times 10^{-7}} , \ensuremath{5.6592\times 10^{-4}}]\). This gives rise to highly variable counts and, typically, less restriction on potential pairwise correlation pairs. Once the functions are defined/executed to complete marginal estimation, we specify targets and generate the desired random vectors using \texttt{rvec}. Now we check admissibility of the specified correlation matrix.

\begin{CodeChunk}
\begin{CodeInput}
R> # 1. Mapping step first
R> R_X <- bs$cor_convert(R_S, bs$Spearman, bs$Pearson)
R> # 2a. Check admissibility
R> (is_valid_corr <- bs$iscorrelation(R_X))
\end{CodeInput}
\begin{CodeOutput}
[1] FALSE
\end{CodeOutput}
\begin{CodeInput}
R> # 2b. compute nearest correlation
R> if (!is_valid_corr) {
+   R_X_pd <- bs$cor_nearPD(R_X)
+   ## Quantify the error
+   targets      <- R_X[lower.tri(R_X, diag = FALSE)]
+   approximates <- R_X_pd[lower.tri(R_X_pd, diag = FALSE)]
+   R_X          <- R_X_pd
+ }
R> summary(abs(targets - approximates))
\end{CodeInput}
\begin{CodeOutput}
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
0.000000 0.000201 0.000432 0.000534 0.000753 0.006931 
\end{CodeOutput}
\end{CodeChunk}

While the exact \(d \times d\) Spearman correlation matrix is not strictly admissible in our scheme (as seen by the non-positive definite result above), the approximation is close with a maximum absolute error of 0.0069 and average absolute error of \ensuremath{5.3366\times 10^{-4}} across the \ensuremath{4.995\times 10^{5}} correlations.

\begin{CodeChunk}
\begin{CodeInput}
R> sim_nbinom <- bs$rvec(10000, R_X, nb_margins) 
R> colnames(sim_nbinom) <- colnames(brca1000)
\end{CodeInput}
\end{CodeChunk}

Figure \ref{fig:ch050-simDataFig} displays the simulated counts and pairwise relationships for our example genes from Table \ref{tab:ch010-realDataTab}.
Simulated counts roughly mimic the observed data but with a smoother appearance due to the assumed parametric form and with less extreme points then the observed data in Figure \ref{fig:ch010-realDataFig}. Figure \ref{fig:ch050-figBRCA} displays the aggregated results of our simulation by comparing the specified target parameter (horizontal axes) with the corresponding quantities estimated from the simulated data (vertical axes). The evaluation shows that the simulated counts approximately match the target parameters and exhibit the full range of estimated correlation from the data.

\begin{CodeChunk}
\begin{CodeInput}
R> set.seed(2020-02-25)
R> num_genes <- 3
R> gene_sample <- sample(example_genes[1:1000], num_genes)
R> 
R> ggpairs(data = as.data.frame(sim_nbinom[, gene_sample]),
+         upper = list(continuous = wrap('cor', method = "spearman"))) + 
+   theme_bw()
\end{CodeInput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{CodeOutput}
Warning in cor.test.default(x, y, method = method, use = use): Cannot compute
exact p-value with ties
\end{CodeOutput}
\begin{figure}

{\centering \includegraphics{article_bigsimr_files/figure-latex/ch050-simDataFig-1} 

}

\caption[Simulated data for three selected high-expressing genes generally replicates the estimated data structure]{Simulated data for three selected high-expressing genes generally replicates the estimated data structure. The data do not exhibit outlying points, but do possess the desired Spearman correlations, central tendencies, and discrete values.}\label{fig:ch050-simDataFig}
\end{figure}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> include_graphics('fig/ch050-figBRCA.png')
\end{CodeInput}
\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/ch050-figBRCA} 

}

\caption[Simulated random vectors from a multivariate negative binomial replicate the estimated structure from an RNA-seq data set]{Simulated random vectors from a multivariate negative binomial replicate the estimated structure from an RNA-seq data set. The dashed red lines indicate equality between estimated parameters from simulated data (vertical axes) and the specified target parameters (horizontal axes).}\label{fig:ch050-figBRCA}
\end{figure}
\end{CodeChunk}

\emph{Limitations, conclusions, and recommendations}

The results show overall good simulation performance for our choice of parameters settings. Our settings were motivated by modeling high-expressing genes from the TCGA BRCA data set. In general, the ability to match marginal and dependence parameters depends on the particular joint probability model. We recommend to evaluate and tune your simulation until you can be assured of the accuracy.

\hypertarget{simulation-based-joint-probability-calculations}{%
\subsection{Simulation-based joint probability calculations}\label{simulation-based-joint-probability-calculations}}

Many statistical tasks require evaluation of a joint probability mass (or density) function:

\[
P( {\bf Y} = {\bf y} ), \: y_i \in \chi_i.
\]

where \(\chi_i\) is the sample space for the \(i^{th}\) component of the random vector \(\bf{Y}\). Compact representations with convenient computational forms are rare for high-dimensional constructions, especially with heterogeneous, correlated marginal distributions (or margins of mixed data types). Given a large number of simulated vectors as produced above, estimated probabilities are readily given by counting the proportion of simulated vectors meeting the desired condition. In our motivating application, one may ask what is the probability that all genes expressed greater than a certain threshold value \({\bf y}_0\). This can be estimated as

\[
\hat{P}( {\bf Y} \ge {\bf y_0 } ) = \frac{1}{B} \sum_{b=1}^B I( {\bf Y}^{ (b) } \ge {\bf y_0} ),
\]

where \({\bf Y}^{(b)}\) is the \(b^{th}\) simulated vector from a total of \(B\) simulation replicates and \(I(\cdot)\) is the indicator function. For example, we can estimate from our \(B=10,000\) simulated vectors that the probability of all genes expressing (i.e., \({\bf y}_i \geq 1, \forall \; i\)) is \(0.5979\).

\begin{CodeChunk}
\begin{CodeInput}
R> d <- ncol(sim_nbinom)
R> B <- nrow(sim_nbinom)
R> y0 <- 1
R> pHat <- mean(apply(sim_nbinom, 1, function(Y) {
+   all(Y >= y0)
+ }))
R> pHat
\end{CodeInput}
\begin{CodeOutput}
[1] 0.5979
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{evaluation-of-correlation-estimation-efficiency}{%
\subsection{Evaluation of correlation estimation efficiency}\label{evaluation-of-correlation-estimation-efficiency}}

MC methods are routinely used in many statistical inferential tasks including estimation, hypothesis testing, error rates, and empirical interval coverage rates. To conclude the example applications, we demonstrate how \texttt{bigsimr} can be used to evaluate estimation efficiency. In particular, we wish to assess the error in our correlation estimation above. We used a conventional method, based on classical statistical theory which was not designed for high-dimensional data. Indeed, high-dimensional covariance estimation (and precision matrices) is an active area of statistical science, .e.g., \citep{Won2013g, VanWieringen2016}.

In this small example, we simulate \(m=30\) data sets with the number of simulated vectors matching the number of patients in the BRCA data set, \(N=878\). Since our simulation is much faster for the Pearson correlation type (see Figure \ref{fig:ch040-largeDfig}), we only convert the Spearman correlation matrix once (and ensure it is positive definite). At each iteration, we estimate the quadratic loss (residual sum of squared errors) from the specified \({\bf R}_{S}\), producing a distribution of loss values.

\begin{CodeChunk}
\begin{CodeInput}
R> # Simulate random vectors equal to the sample size
R> N <- nrow(example_brca)
R> # create m random vectors and estimate correlation
R> simRho <- replicate(n = m, expr = {
+   tmpSim <- bs$rvec(N , R_X, nb_margins)
+   bs$cor(tmpSim, bs$Spearman)
+ }, simplify = FALSE)
R> # Evaluate the residual sum of squared error
R> sapply(simRho, function(R) sum((R - R_S)^2))
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
R> frobenius_loss <- sapply(simRho, function(R) sum((R - R_S)^2))
\end{CodeInput}
\end{CodeChunk}

The \texttt{R} summary function supplies the mean-augmented five-number summary of the quadratic loss distribution computed above.

\begin{CodeChunk}
\begin{CodeInput}
R> summary(frobenius_loss)
\end{CodeInput}
\begin{CodeOutput}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    691     829     890     925     986    1224 
\end{CodeOutput}
\end{CodeChunk}

This distribution could be compared to high-dimensional designed covariance estimators for guidance on whether the additional complexity and computation time are warranted.

\hypertarget{discussion}{%
\section{Conclusion and discussion}\label{discussion}}

We have introduced a general-purpose high-dimensional multivariate simulation algorithm and provide a user-friendly, high-performance \texttt{R} package {[}\texttt{bigsimr}{]}. The random vector generation method is inspired by NORTA \citep{Cario1997} and Gaussian copula-based approaches \citep[\citet{BF17}, \citet{Xia17}]{MB13}. The major contributions of this work are methods and software for flexible, scalable simulation of HD multivariate probability distributions with broad potential data analytic applications for modern, big-data statistical computing. For example, one could simulate high-resolution time series data, such as those consistent with an auto-regressive moving average model exhibiting a specified Spearman structure. Or our methods could be used to simulate sparsely correlated data, as many HD methods assume, via specifying a \emph{spiked correlation matrix}.

It is customary to compare new tools and algorithms directly to existing competing methods and software. In this study, however, we only employ our proposed methodology, as our previous work has shown that existing \texttt{R} tools are simply not designed to meet our high-dimensional goal (see \citet{Li2019gpu} for evaluations of the \texttt{R} \texttt{copula} package and others). For the bivariate simulations, existing packages such as \href{https://github.com/superdesolator/NORTARA}{\texttt{nortaRA}} work well to match Pearson correlations exactly.

There are limitations to the methodology and implementation. We could only investigate selected multivariate distributions in our Monte Carlo studies. There may be instances that the methods do not perform well. Along those lines, we expect that correlation values close to the boundary of the feasible region could result in algorithm failure. Another issue is that for discrete distributions, we use continuous approximations when the support set is large. This could limit the scalability/accuracy for particular discrete/mixed multivariate distributions. Our method would also benefit computationally from faster Kendall estimation and more finely tuned nearest PD calculation for non-admissible Kendall matrices.

\hypertarget{misc}{%
\section*{Supplementary Materials}\label{misc}}
\addcontentsline{toc}{section}{Supplementary Materials}

We provide an open-source implementation of our methodology as the \texttt{Bigsimr.jl} Julia package, hosted on github at \url{https://github.com/adknudson/Bigsimr.jl}. Additionally we provide R and Python interfaces to \texttt{Bigsimr}, respectively at \url{https://github.com/SchisslerGroup/r-bigsimr} and \url{https://github.com/SchisslerGroup/python-bigsimr}. \texttt{Bigsimr.jl} is an ongoing project, and feature requests or issues can be submitted to \url{https://github.com/adknudson/Bigsimr.jl/issues}.

\begin{CodeChunk}
\begin{CodeInput}
R> knitr::include_graphics('images/hex-bigsimr.png')
\end{CodeInput}


\begin{center}\includegraphics[width=0.05\linewidth]{images/hex-bigsimr} \end{center}

\end{CodeChunk}

\hypertarget{acknowledgments}{%
\section*{Acknowledgment(s)}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgment(s)}

The authors gratefully acknowledge Heather Knudson's graphic design for the \texttt{bigsimr} R Package. The results published here are in whole or part based upon data generated by the TCGA Research Network: \url{https://www.cancer.gov/tcga}.

\hypertarget{coi}{%
\section*{Disclosure statement}\label{coi}}
\addcontentsline{toc}{section}{Disclosure statement}

The authors report no conflict of interest.

\hypertarget{funding}{%
\section*{Funding}\label{funding}}
\addcontentsline{toc}{section}{Funding}

Research reported in this publication was supported by grants from the National Institutes of General Medical Sciences (5 U54 GM104944, GM103440) from the National Institutes of Health.

\hypertarget{appendix}{%
\section*{Appendix}\label{appendix}}
\addcontentsline{toc}{section}{Appendix}

\noindent Consider a bivariate example where we have a correlated \((Y_1, Y_2)^\top\) with \(Y_1\sim N(\mu_1, \sigma_1^2)\) (normal) and \(Y_2\sim LN(\mu_2, \sigma_2^2)\) (lognormal). First, let us note that when we get such a vector from the \texttt{bigsimr} package then in fact it can be represented as

\begin{equation}
(Y_1, Y_2)^\top \stackrel{d}{=} \left(X_1, e^{X_2}\right)^\top,
\label{eq:kram1}
\end{equation}

where \((X_1, X_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)\), which is bivariate normal with mean vector \(\boldsymbol \mu = (\mu_1, \mu_2)^\top\) and variance-covariance matrix

\begin{equation}
\boldsymbol \Sigma = 
\begin{bmatrix}
\sigma_1^2 & \rho \sigma_1\sigma_2\\
\rho \sigma_1\sigma_2 & \sigma_2^2
\end{bmatrix}
\label{eq:kram2}
\end{equation}

To see this, consider the three steps of the NORTA construction:

\begin{enumerate}

\item Generate $(Z_1, Z_2)^\top \sim N_2(\boldsymbol 0, \boldsymbol R)$, where 

\begin{equation}
\boldsymbol R = 
\left[
\begin{array}{cc}
1 & \rho \\
\rho & 1
\end{array}
\right].
\label{eq:kram3}
\end{equation}


\item Transform $(Z_1, Z_2)^\top$ to $(U_1, U_2)^\top$ viz. $U_i =\Phi(Z_i)$,  $i=1,2$, where $\Phi(\cdot)$ is the standard normal CDF. 

\item Return $(Y_1, Y_2)^\top$, where $Y_i=F_i^{-1}(U_i)$, $i=1,2$, and $F_i(\cdot)$ is the CDF of $Y_i$ and $F_i^{-1}(\cdot)$ is its inverse (the quantile function). 

\end{enumerate}

In this example, \(F_1\) is the CDF of \(N(\mu_1, \sigma_1^2)\) while \(F_2\) is the CDF of \(LN(\mu_2, \sigma_2^2)\), so that the two required quantile functions are

\begin{equation}
F_1^{-1}(u) = \mu_1+\sigma_1 \Phi^{-1}(u)\,\,\, \mbox{and} \,\,\, F_2^{-1}(u) = e^{\mu_2+\sigma_2 \Phi^{-1}(u)}, 
\label{eq:kram4}
\end{equation}

as can be seen by standard calculation. Thus, when we apply Step 3 of the NORTA algorithm, we get

\begin{equation}
F_1^{-1}(U_1) = \mu_1+\sigma_1 \Phi^{-1}(\Phi(Z_1)) =  \mu_1+\sigma_1 Z_1 \,\,\, \mbox{and} \,\,\, F_2^{-1}(U_2) = e^{\mu_2+\sigma_2 \Phi^{-1}(\Phi(Z_2))} = e^{\mu_2+\sigma_2 Z_2}, 
\label{eq:kram5}
\end{equation}

where \((X_1, X_2)^\top = (\mu_1+\sigma_1 Z_1, \mu_2+\sigma_2 Z_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)\). Consequently, the vector \((Y_1, Y_2)^\top\) obtained in Step 3 of the algorithm has the structure provided by (\ref{kram1}).

\vspace{0.1in}

\noindent Next, we provide the exact covariance structure of the random vector \((Y_1, Y_2)^\top\) given by (\ref{kram1}), and relate the correlation of \(Y_1\) and \(Y_2\) to \(\rho\), which is the correlation of the normal variables \(Z_1\), \(Z_2\) (and also \(X_1\) and \(X_2\)). A straightforward albeit somewhat tedious algebra produces the following result.

\begin{lemma}
Let ${\bf Y} = (Y_1, Y_2)^\top$ admit the stochastic representation (\ref{kram1}), where ${\bf X} = (X_1, X_2)^\top \sim N_2(\boldsymbol \mu, \boldsymbol \Sigma)$ with $\boldsymbol \mu$ and $\boldsymbol \Sigma$ as above. Then the mean vector and the variance-covariance matrix of ${\bf Y}$ are given by 

\begin{equation}
\boldsymbol \mu_{{\bf Y}} = \left(\mu_1, e^{\mu_2+\sigma_2^2/2}\right)^\top
\label{eq:kram6}
\end{equation}

and 

\begin{equation}
\boldsymbol \Sigma = 
\begin{bmatrix}
\sigma_1^2 & \rho \sigma_1\sigma_2  e^{\mu_2+\sigma_2^2/2}  \\
\rho \sigma_1\sigma_2 e^{\mu_2+\sigma_2^2/2} & \left[ e^{\mu_2+\sigma_2^2/2}\right]^2 \left[e^{\sigma_2^2} -1 \right],
\end{bmatrix}
\label{eq:kram7}
\end{equation}

respectively. 
\end{lemma}

\begin{proof}
The values of the means and the variances are obtained immediately from normal marginal distribution of $Y_1$ and lognormal marginal distribution of $Y_2$. It remains to establish the covariance of $Y_1$ and $Y_2$, 

\begin{equation}
\mbox{Cov}(Y_1, Y_2) = \mathbb E(Y_1 Y_2) - \mathbb E(Y_1)\mathbb E(Y_2), 
\label{eq:covy1y2}
\end{equation}

and in particular the first expectation on the right-hand-side above. By using the tower property of expectations, the latter expectation can be expressed as 

\begin{equation}
\mathbb E(Y_1 Y_2)  =  \mathbb E\left(X_1 e^{X_2}\right)  =  \mathbb E \left\{ \mathbb E \left(X_1 e^{X_2} | X_1 \right) \right\} = \mathbb E \left\{ X_1 \mathbb E \left(e^{X_2} | X_1 \right) \right\}.
\label{eq:kre1}
\end{equation}

Further, by using the fact that the conditional distribution of $X_2$ given $X_1=x_1$ is normal with mean $\mathbb E(X_2|X_1=x_1) = \mu_2+\rho\sigma_2(x_1-\mu_1)/\sigma_1$ and variance $\sigma_2^2(1-\rho^2)$, one can relate the inner expectation on the far right in (\ref{kre1}) to the moment generating function $M(t)$ of this conditional distribution evaluated at $t=1$, leading to 

\begin{equation}
E \left(e^{X_2} | X_1 \right) = e^{ \mu_2+\rho\sigma_2(x_1-\mu_1)/\sigma_1 + \sigma_2^2(1-\rho^2)/2},
\label{eq:kre2}
\end{equation}

so that 

\begin{equation}
\mathbb E(Y_1 Y_2)  =  e^{ \mu_2+ \frac{1}{2} \sigma_2^2(1-\rho^2)} \mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\}.
\label{eq:kre3}
\end{equation}

Since $X_1$ is normal with mean $\mu_1$ and variance $\sigma_1^2$, the expectation in (\ref{kre3}) becomes 

\begin{equation}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = \int_{-\infty}^\infty x_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (x_1-\mu_1)} \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{1}{2\sigma_1^2}(x_1-\mu_1)^2}.
\label{eq:kre4}
\end{equation}

Upon substituting $u=x_1-\mu_1$, followed by some algebra, we arrive at 

\begin{equation}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = e^{\frac{1}{2}\sigma_2^2\rho^2} \int_{-\infty}^\infty (u+\mu_1)  \frac{1}{\sqrt{2\pi}\sigma_1} e^{-\frac{1}{2\sigma_1^2}(u-\rho\sigma_1\sigma_2)^2}.
\label{eq:kre5}
\end{equation}

Since the integral in (\ref{kre5}) is the expectation $\mathbb E(X+\mu_1)$, where $X$ is normal with mean $\rho\sigma_1\sigma_2$ and variance $\sigma_1^2$, we conclude that 

\begin{equation}
\mathbb E \left\{ X_1 e^{ \rho\frac{\sigma_2}{\sigma_1} (X_1-\mu_1)} \right\} = e^{\frac{1}{2}\sigma_2^2\rho^2} (\rho\sigma_1\sigma_2 +\mu_1),
\label{eq:kre6}
\end{equation}

which, in view of (\ref{kre3}), leads to 

\begin{equation}
\mathbb E (Y_1Y_2) = e^{\mu_2 + \frac{1}{2}\sigma_2^2} (\rho\sigma_1\sigma_2 +\mu_1).
\label{eq:kre7}
\end{equation}

Finally, (\ref{covy1y2}), along with the expressions for the means of $Y_1$ and $Y_2$, produce the covariance of $Y_1$ and $Y_2$:

\begin{equation}
\mbox{Cov}(Y_1, Y_2) = e^{\mu_2 + \frac{1}{2}\sigma_2^2} (\rho\sigma_1\sigma_2 +\mu_1) - \mu_1 e^{\mu_2 + \frac{1}{2}\sigma_2^2} = \rho\sigma_1\sigma_2 e^{\mu_2 + \frac{1}{2}\sigma_2^2}.
\label{eq:kre8}
\end{equation}

This concludes the proof
\end{proof}

Using the above result, we can directly relate the correlation coefficient of \(Y_1\) and \(Y_2\) with that of \(X_1\) and \(X_2\),

\begin{equation}
\rho_{Y_1, Y_2} = \rho \frac{\sigma_2}{\sqrt{e^{\sigma_2^2} -1}},
\label{eq:kram8}
\end{equation}

where \(\rho\) is the correlation of \(X_1\) and \(X_2\). The above result is useful when studying the possible range of correlation of \(Y_1\) and \(Y_2\) in the above example. It can be shown that the factor on the far right in (\ref{kram8}) is a monotonically decreasing function of \(\sigma_2\) on \((0,\infty)\), with the limits of 1 and 0 at zero and infinity, respectively. Thus, in principle, the range of correlation of \(Y_1\) and \(Y_2\) is the same as that of \(X_1\) and \(X_2\), as the factor on the far right in (\ref{kram8}) can be made arbitrarily close to 1. However, by changing this factor we may affect the marginal distributions of \(Y_1\) and \(Y_2\). It can be shown that if the marginal distributions of \(Y_1\) and \(Y_2\) are fixed, then the relation (\ref{kram8}) becomes

\begin{equation}
\rho_{Y_1, Y_2} = \rho \sqrt{\frac{\log(1+c_2^2)}{c_2^2}},
\label{eq:kram9}
\end{equation}

where \(c_2=\sigma_{Y_2}/\mu_{Y_2}\) is the \emph{coefficient of variation} (CV) of the variable \(Y_2\). Thus, the range of possible correlations in this model is not affected by the distribution of \(Y_1\), and is determined by the CV of \(Y_2\) as follows:

\begin{equation}
- \sqrt{\frac{\log(1+c_2^2)}{c_2^2}} \leq \rho_{Y_1, Y_2} \leq \sqrt{\frac{\log(1+c_2^2)}{c_2^2}}. 
\label{eq:kram10}
\end{equation}

Plugging in the estimates of these quantities from Section \ref{package}, we see that

\begin{equation}
\frac{\hat{\sigma}_2}{\sqrt{e^{\hat{\sigma}_2^2} -1}} = 0.881.
\label{eq:kram11}
\end{equation}

Thus, the possible range of correlation becomes \((-0.881, 0.881)\). This approximately agrees with the MC results provided.

As an invited speaker at CMStatistics 2020, I was delighted to learn of the opportunity to have our work included in the first issue of CSDA Annals of Statistical Data Science.

On behalf of my co-authors, I'm pleased to submit our manuscript entitled \emph{Simulating High-Dimensional Multivariate Data using the \texttt{Bigsimr} Package} for your consideration.
We feel that the work is well suited to the indicated themes and hope that it will be considered a valuable contribution to this prestigious journal.

The entire article can be reproduced from code and is housed at \url{https://github.com/SchisslerGroup/article_bigsimr}, which we are glad to make available upon request to the reviewers and, after acceptance, the general public.

I hope that everything is in order. If not, or if you have any additional needs, please do not hesitate to contact me. Thank you and all those involved in this process.

\bibliography{bigsimr.bib,packages.bib,alex.bib}


\end{document}
