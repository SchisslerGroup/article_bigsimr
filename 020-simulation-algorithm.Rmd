# Simulation algorithm

## Algorithm description

This section describes the method for simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$. Each $Y_i$ has a specified marginal
distribution function $F_i$ and its inverse. To characterize dependency, every pair $(Y_i, Y_j)$ has either a specified Pearson correlation \@ref(eq:pearson), (rescaled) Spearman correlation \@ref(eq:spearmanRescaled) , or Kendall's $\tau$ (Equation \@ref(eq:tau) ). The method only approximately matches the Pearson correlation in general, whereas the rank-based methods are exact.

The method is best understand as a **parallelized Gaussian copula** (see Equation \@ref(eq:gauss). We shall see that constructing continuous joint distributions that match a target Spearman or Kendall's correlations computes easily when employing Gaussian copulas, since this measures are invariant under the monotone transformations involved [refs]. To do this, we take advantage of a closed form relationship [ref?] between Kendall's $\tau$ and Pearson's correlation coefficient for bivariate normal random variables:

\begin{equation}
(\#eq:convertKendall)
r_{Pearson} = sin \left( \tau_{Kendall} \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's $\rho$ [@K58],

\begin{equation}
(\#eq:convertSpearman)
\rho_{Pearson} = 2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right).
\end{equation}

For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components from Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In contrast the rank-based correlations, matching specified Pearson correlation coefficients exactly is computational intense in this scheme. In general, there is no closed form correspondence and involving computing or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X|r}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d$. For accurate numeric approximation of these integrals, the functions must be evaluated hundreds of times. Others have used efficient Monte Carlo integration schemes (see @Chen2001), but scale poorly to large dimension in reasonable times (property **S2**). Despite all this, if one does desire to characterize dependency using Pearson correlations, we often see in practice --- and it is theoretically justified under certain conditions (@Song00) --- that simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution.

## Simulation Algorithm

Putting the together the facts provided in the equations above, we come the following proposed simulation algorithm to produce a random vector ${\bf Y}$ with specified Spearman's correlation and marginal distributions. Note that all computational steps can be parallelized as each operation can be done or either the $d$ marginals or the $\binom{d}{2}$ pairs for the correlation values. Even the generation of multivariate normal random vectors is parallelized through optimized matrix multiplication/decomposition routines. 

### Algorithm 1: Matching Pearson approximately

We aim to simulate multivariate data with a Pearson correlation
matrix ${\bf R}$.

(i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$;  
(ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
(iii) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

* Where $F_i^{-1}$ is the inverse CDF for the $i^{th}$ gene's marginal
distribution.  
* And ${\bf R}$ is the target Pearson correlation matrix.  
* Pearson's correlation is **not** invariant under the montonically increasing
  transformations in steps 3, 4.  
* So the resulting simulated random vectors will have the **exact** margins but **approximate** ${\bf R}$.  

### Algorithm 2: Matching Spearman's exactly

(i) Convert ${\bf R}_{Spearman}$ into ${\bf R}_{Pearson}$ via $\rho_{Pearson} =
  2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right)$;  
(ii) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf} R_{Pearson})$;  
(iii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
(iv) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

- Spearman's $\rho$ relates to the difference in the ranks of pairs of values.
- We take advantage of a closed form correspondence between Pearson and Spearman
  for *normal random variables* in step 1 [@BF17].
- Since Spearman's $\rho$ is invariant under the monotonically increasing
  transformations in steps 3 and 4, the resulting simulated random vectors will
  have the **exact** margins and **exact** Spearman correlation (up to Monte Carlo/numeric error).

### Algorithm 3: Matching Kendall's exactly

(i) Convert ${\bf R}_{Kendall}$ into ${\bf R}_{Pearson}$ via $\rho_{Pearson} = sin \left( \tau \times \frac{\pi}{2} \right)$;  
(ii) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R}_{Pearson})$;  
(iii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
(iv) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

- Kendall's $\tau$ is the difference in the probabilities of concordant and discordant pairs.
- We use the 1-1 relationship between Pearson and Kendall's $\tau$ for *normal
  rvs* in step 1.
- Since Kendall's $\rho$ is invariant under the montonically increasing
  transformations in steps 3 and 4,  
 the resulting simulated random vectors will have the **exact** margins and
 **exact** Kendall's $\tau$  (up to Monte Carlo/numeric error).

## Algorithm notes

- discuss the consequences of the transformations to obtain the corresponding Pearson matrix and describe the quadratic algorithm implemented to address this (Alex).
- Discrete margins pose some difficulties and matching *unadjusted* correlation
  measures is not exact (empirically approximate).
- Exact Pearson solutions are possible but require computing $d \choose 2$
   double integrals $EY_iY_j = \int \int y_i y_j f_{X|r}(F_i^{-1}(\Phi(z_i)),
   F_j^{-1}(\Phi(z_j))dy_idy_j$. We have yet to accelerate this process.
- We GPU accelerate certain steps (Alex which steps?) possible during our proposed High-Performance NORTA
  algorithm.
- One thorny problem when dealing with high-dimensional correlation matrices is
they can become non-positive definite through either during estimation or
bivariate transformations.
- When converting nonparametric correlations to Pearson (first step above) the
  resultant correlation may not be positive definite (PD).
- In that case, we replace ${\bf R}_{Pearson}$ with a "close" PD matrix ${\bf \tilde{R}}_{Pearson}$.
- In practice, this loss in accuracy typically has little impact on performance,
  but the algorithm needs acceleration (`Matrix::nearPD()` violates property S1).

test workflow again
test after a long while
