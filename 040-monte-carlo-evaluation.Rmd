# Monte Carlo evaluations {#simulations}


```{r ch040-LoadLib040, include=FALSE, cache=FALSE}
box::use(
    dplyr[...],
    ggplot2[...],
    patchwork[...],
    tidyr[pivot_longer],
    bigsimr[bigsimr_setup, distributions_setup],
    knitr[kable],
    RColorBrewer[brewer.pal]
)

load("data/bivariate_normal_sims.rda")
load("data/bivariate_gamma_sims.rda")
load("data/bivariate_nbinom_sims.rda")
load("data/benchmark_dependences.rda")

Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())
bs <- bigsimr_setup(pkg_check = FALSE)
dist <- distributions_setup()
prob <- 3e-04
```


In this section, we conduct MC studies to investigate method performance. Marginal parameter matching is straightforward in our scheme as it is a sequence of univariate inverse probability transforms. This process is both fast and exact. On the other hand, accurate and computationally efficent dependency matching presents a challenge, especially in the HD setting. To evaluate our methods in those respects, we design the following numerical experiments to first assess accuracy of matching dependency parameters in bivariate simulations and then time the procedure in increasingly large dimension $d$.


## Bivariate experiments

We select bivariate simulation configurations to ultimately simulate our motivating discrete-valued RNA-seq example. Thus, we proceed in increasing departure from normality, leading to a multivariate negative binomial (MVNB) model for our motivating data. We begin with empirically evaluating the dependency matching across all three supported correlations --- Pearson, Spearman, and Kendall --- in identical, bivariate marginal configurations. For each pair of identical margins, we vary the target correlation across $\Omega$, the set of possible admissible values for each correlation type, to evaluate the simulation's ability to simulate all possible correlations. The simulations progress from bivariate normal, to bivariate gamma (non-normal yet continuous), and bivariate negative binomial (non-normal and discrete).

Table \@ref(tab:sims) lists our identical-marginal, bivariate simulation configurations. We increase the simulate replicates $B$ to visually ensure that our results converge to the target correlations and gauge efficiency. We select distributions beginning with a standard multivariate normal (MVN) as we expect the performance to be exact (up to MC error) for all correlation types. Then, we select a non-symmetric continuous distribution: a standard (rate =1), two-component multivariate gamma (MVG). Finally, we select distributions and marginal parameter values that are motivated by our RNA-seq data, namely values proximal to probabilities and sizes estimated from the data (see [RNA-seq data application](examples) for estimation details). Specifically, we assume a MVNB with $p_1 = p_2 = 3\times10^{-4}, r_1 = r_2 = 4, \rho \in \Omega$.

\begin{table}[]
\centering
\caption{ \label{tab:sims} Identical margin, bivariate simulation configurations to evaluate matching accuracy.}
\begin{tabular}{@{}lcr@{}}
\toprule
Simulation Reps $B$ & Correlation Types & Identical-margin 2D distribution \\ \midrule
$1,000$ & Pearson ($\rho_P$) & ${\bf Y} \sim MVN( \mu= 0 , \sigma = 1, \rho_i ), i=1,\ldots,100$ \\
$10,000$ & Spearman ($\rho_S$) & ${\bf Y} \sim MVG( shape = 10, rate = 1, \rho_i ), i=1,\ldots,100$ \\
$100,000$ & Kendall ($\tau$) & ${\bf Y} \sim MVNB(p = 3\times10^{-4}, r = 4,\rho_i), i=1,\ldots,100$ \\ \bottomrule
\end{tabular}
\end{table}

For each of the unique 9 simulation configurations described above, we estimate the correlation bounds and vary the correlations along a sequence of 100 points evenly placed within the bounds, aiming to explore $\Omega$. Specifically, we set correlations $\{ \rho_1 = ( \hat{l} + \epsilon), \rho_2 = (\hat{l} + \epsilon) + \delta, \ldots, \rho_{100} = (\hat{u} - \epsilon) \}$, with $\hat{l}$ and $\hat{u}$ being the estimated lower and upper bounds, respectively, with increment value $\delta$. The adjustment factor, $\epsilon=0.01$, is introduced to handle numeric issues when the bound is specified exactly.


```{r ch040-biNormPlot, eval=FALSE, fig.height=5, fig.width=8, fig.cap="`bigsimr` recovers the Pearson specified correlations for MVN."}
bivariate_normal_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-biGammaPlot, eval=FALSE, fig.height=5, fig.width=8, fig.cap="`bigsimr` recovers the Pearson specified correlations for Bivariate Gamma."}
bivariate_gamma_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-biNegBinPlot, eval=FALSE, fig.width=8, fig.height=5, fig.cap="`bigsimr` recovers the correlations for bivariate negative binomial only approximately for Pearson but (nearly) exactly for the rank-based correlations."}
bivariate_nbinom_sims %>%
    ggplot(aes(rho, rho_hat, color = type)) +
    geom_point() +
    geom_abline(slope = 1) +
    facet_wrap(~ type + N) + 
    theme_bw()
```


```{r ch040-combineBiSims, cache=FALSE}
allDat <- bind_rows(
    select(bivariate_normal_sims, margins, type, N, rho, rho_hat),
    select(bivariate_gamma_sims, margins, type, N, rho, rho_hat),
    select(bivariate_nbinom_sims, margins, type, N, rho, rho_hat)
) %>%
    mutate(margins = factor(margins, levels=c("norm", "gamma", "nbinom")),
           type = factor(type, levels=c("Pearson", "Spearman", "Kendall")),
           N = factor(N))
```


Figure \@ref(fig:ch040-bPlot) displays the aggregated bivariate simulation results. Table \@ref(tab:ch040-BiError) contains the mean absolute error (MAE) in reproducing the desired dependency measures for the three bivariate scenarios. Overall, the studies show that our methodology is generally accurate across the entire range of possible correlation for all three dependency measures. Our Pearson matching performs nearly as well as Spearman or Kendall, except for a slight increase in error for the negative binomial case.



```{r ch040-BiError}
tabMAE <- allDat %>%
    group_by(N, type, margins) %>%
    summarize(MAE = mean(abs(rho - rho_hat))) %>%
    ungroup()

kable(tabMAE, booktabs = TRUE, format = "latex",
      linesep = c("", "", "\\addlinespace"),
      col.names = c("No. of random vectors",
                    "Correlation type",
                    "Distribution",
                    "Mean abs. error"),
      caption = "Average abolute error in matching the target dependency across the entire range of possible correlations for each bivariate marginal.")
```

```{r ch040-bPlot, cache=FALSE, fig.asp=1.30, fig.weight=4, fig.cap="Bivariate simulations match target correlations across the entire range of feasible correlations. The horizontal axis plots the specified target correlations for each bivariate margin. Normal margins are plotted in dark dark grey, gamma in medium grey, and negative binomial in light grey. As the number of simulated vectors $B$ increases from left to right, the variation in estimated correlations (vertical axis) decreases. The dashed line indicates equality between the specified and estimated correlations."}
# https://www.datanovia.com/en/blog/how-to-change-ggplot-facet-labels/
# New facet label names
repsLabs <- paste0("B=", c("1,000", "10,000", "100,000") )
names(repsLabs) <- c(1000, 10000, 100000)
typeLabs <- c("Pearson", "Spearman", "Kendall")
names(typeLabs) <- c("Pearson", "Spearman", "Kendall")

# Set colors
## RColorBrewer::display.brewer.all()
numColors <- 4
numGroups <- length(levels(allDat$margins))
myColors <- rev(RColorBrewer::brewer.pal(n = numColors, name = "Greys")[ ((numColors - numGroups) + 1):numColors])

allDat %>%
    ggplot(aes(x = rho, y = rho_hat, color = margins)) +
    geom_point(size = 2) +
    scale_color_manual(values = myColors ) +
    geom_abline(slope = 1, linetype = "dashed") +
    labs(x = "Specified Correlation", y = "Estimated Correlation") +
    facet_wrap(~ type + N, labeller = labeller(N = repsLabs, type = typeLabs)) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal")
```


## Scale up to High Dimensions


With information of our method's accuracy from a low-dimensional perspective, we now assess whether `bigsimr` can scale to larger dimensional problems with practical computation times. We ultimately generate $B=1,000$ random vectors for $d=\{100, 250, 500, 1000, 2500, 5000, 10000\}$ for each correlation type, {Pearson, Spearman, Kendall} while timing the algorithm's major steps. We begin by producing a synthetic "data set" by completing the following steps:

\setstretch{1.5}
1. Produce heterogeneous gamma marginals by randomly selecting the $j^{th}$ gamma shape parameter from $U_j \sim uniform(1,10), j=1,\ldots,d$ and the $j^{th}$ rate parameter from $V_j \sim exp(1/5), j=1,\ldots,d$, with the constant parameters determined arbitrarily.
2. Produce a random full-rank Pearson correlation matrix via `cor_randPD` of size $d \times d$.
3. Simulate a "data set" of $1,000 \times d$ random vectors via `rvec`.
\setstretch{2.0}


With the synthetic data set in hand, we complete and time the following four steps involved in a typical workflow (including correlation estimation).

\setstretch{1.5}
1. Estimate the correlation matrix from the "data" in the *Compute Correlation* step.
2. Map the correlations to initialize the algorithm (Pearson to Pearson, Spearman to Pearson, or Kendall to Pearson) in the *Adjust Correlation* step.
3. Check whether the mapping produces a valid correlation matrix and, if not, find the nearest PD correlation matrix in the *Check Admissibility* step.
4. Simulate $1,000$ vectors in the *Simulate Data* step.
\setstretch{2.0}

The experiments are conducted on a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, with all 16 threads employed during computation. Table \@ref(tab:ch040-moderateDtab) displays the total computation time for moderate dimesions ($d \leq 500$). In every simullation setting, the 1,000 vectors are generated rapidly, executing in under 2 seconds.

```{r ch040-moderateDtab}
dat_long <- benchmark_dependences %>%
  select(-total_time, -n_sim, -needed_near_pd) %>%
  pivot_longer(cols = c(corr_time:sim_time), names_to = "Step", values_to = "Time") %>%
  mutate(dim = factor(dim),
         Step = factor(Step,
                       levels = c("corr_time", "adj_time", "admiss_time", "sim_time"),
                       labels = c("Compute Correlation",
                                  "Adjust Correlation",
                                  "Check Admissibility",
                                  "Simulate Data"))) %>%
    rename(Correlation = corr_type, Dimensions = dim)

tabTimes  <- dat_long %>%
    filter(Dimensions %in% c(100, 250, 500)) %>%
    group_by(Dimensions, Correlation) %>%
    summarize('Total Time(Seconds)' = sum(Time)) %>%
    ungroup()

tabTimes %>%
    kable(booktabs = TRUE,
          linesep = c("", "", "\\addlinespace"), format="latex",
          col.names = c("Dimension",
                        "Correlation type",
                        "Total Time (Seconds)"),
          caption = 'Total time to produce 1,000 random vectors with a random correlation matrix and hetereogeneous gamma margins.')
```


The results for $d > 500$ show scalability to ultra-high dimensions for all three correlation types, although the total times do become much larger. 
Figure \@ref(fig:ch040-largeDfig) displays computation times for $d=\{1000, 2500, 5000, 10000\}$. For $d$ equal to 1000 and 2500, the total time is under a couple of minutes. At $d$ of 5000 and 10,000, Pearson correlation matching in the *Adjust Correlation* step becomes costly. Interestingly, Pearson is actually faster than Kendall for $d=10,000$ due to bottlenecks in *Compute Correlation* and *Check Admissibility*. Uniformly, matching Spearman correlations is faster, with total times under 5 minutes for $d=10,000$, making Spearman the most computationally-friendly dependency type for our package. With this in mind, we scaled the simulation to $d=20,000$ for the Spearman type and obtained the 1,000 vectors in under an hour (data not shown). In principle, this would enable the simulation of an entire human-derived RNA-seq data set. We note that for a given target correlation matrix and margins, steps 1, 2, and 3 only need to be computed once and the fourth step, *Simulate Data*, is nearly instaneous for all settings considered.

```{r ch040-largeDfig, fig.cap="Computation times for HD multivariate gamma simulation."}
# Set colors
numColors <- 5
numGroups <- 4
myColors <- rev(brewer.pal(n=numColors, name="Greys")[((numColors-numGroups)+1):numColors])

dat_long %>%
    filter(Dimensions %in% c(1000, 2500, 5000, 10000, 20000)) %>%
    mutate(`Time (minutes)` = Time / 60) %>%
    ggplot(aes(Correlation, `Time (minutes)`, fill=Step)) +
    geom_bar(position = "stack", stat = "identity") +
    scale_fill_manual(values = myColors ) +
    scale_y_continuous(breaks = seq(0, 80, 10),
                     minor_breaks = NULL) +
    facet_grid(. ~ Dimensions) +
    theme(axis.text.x = element_text(angle = -90))
```

*Limitations, conclusions, and recommendations*

In the bivariate studies, we chose arbitrary simulation parameters for three distributions, moving from the Gaussian to discrete and non-normal MVNB. Under these conditions, the simulated random vectors sample the desired bivariate distribution across the entire range of pairwise correlations for the three dependency measures. The simulation results could differ for other choices of simulation settings. Specifying extreme correlations near the boundary or Frechet bounds could result in poor simulation performance. Fortunately, it is straightforward to evaluate simulation performance by using strategies similar to those completed above. We expect our random vector generation to perform well for the vast majority of NORTA-feasible correlation matrices, but advise to check the performance before making inferences/further analyses. 

Somewhat surprising, Kendall estimation and nearest PD computation scale poorly compared to Spearman and, even, approximate Pearson matching. In our experience, Kendall computation times are sensitive to the number of cores, benefiting from multi-core parallelization. This could mitigate some of the current algorithmic/implementation shortcomings. Despite this, Kendall matching is still feasible for most HD data sets. Finally, we note that one could use our single-pass algorithm `cor_fastPD` to produce a 'close' (not nearest PD) to scale to even higher dimensions with some loss of accuracy.
