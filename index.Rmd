--- 
title: "Simulating Ultra High-Dimensional Multivariate Data Using the bigsimr R Package"
author: [A.G. Schissler, A. Knudson]
date: "`r format(Sys.Date(), '%A, %B %d, %Y')`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: article
bibliography: [bigsimr.bib, packages.bib]
biblio-style: apalike
link-citations: yes
abstract: |
  In this era of Big Data, it is critical to realistically simulate data to conduct informative Monte Carlo studies. This is problematic when data are inherently multivariate while at the same time are (ultra-) high dimensional. This situation appears frequently in observational data found on online and in high-throughput biomedical experiments (e.g., RNA-sequencing). Due to the difficulty in simulating realistic correlated data points, researchers often resort to simulation designs that posit independence --- greatly diminishing the insight into the empirical operating characteristics of any proposed methodology.  Major challenges lie in the computational complexity involved in simulating these massive random vectors. We propose a fairly general, scalable procedure to simulate high-dimensional multivariate distributions with pre-specified marginal characteristics and dependency characteristics. As a motivating example, we use our methodology to study large-scale statistical inferential procedures applied to cancer-related RNA-sequencing data sets. The proposed algorithm is implemented as the `bigsimr` R package.
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
library(ggplot2)
library(tidyverse)
library(knitr)
library(dplyr)
set.seed(06082020)
```

```{bash copyBib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
cp /Users/alfred/Dropbox/bib/bigsimr.bib ./.
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Introduction

Massive high-dimensional data sets are commonplace in many areas of scientific inquiry. As new methods are developed for these data, a fundamental challenge lies in designing and conducting simulation studies to assess operating characteristics of proposed methodology, such as false positive rates, statistical power, and robustness --- often in comparison to existing methods. Another application lies in the discovering the sampling distribution of test statistic under hypothesized null models in complex scenarios. Such Monte Carlo studies in the high-dimensional setting often become difficult to conduct with existing algorithms and tools, or require expertise in computing on clusters. This is particularly true when simulating massive multivariate, non-normal distributions. 

Along those lines, simulating high-dimension dependent negative binomial data motivates this work --- in pursuit of modeling RNA-sequencing data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. This labroratory procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. Often researchers posit a negative binomial model (refs) as counts are often over (or under) expressed that a Poisson model would suggest. Yet due to inherent biological processes, gene expression data exhibits correlation (coexpression) across genes [@BE07; @Schissler2018]. RNA-sequencing analysis has garnered much interest in the statistical literature (refs). Often researchers simulate independently or from their proposed model in order condunct Monte Carlo studies (refs). An alternavitive strategy is to think *generatively* about the data collection process and scientific domain knowledge and design simulations with probabilitistic models that reflect those processes. In our motivating example, we'll study the consequences correlation in the simulation study replicating test statistics from an illustrative, classical large-scale hypothesis testing study [see @Efron2004].

With our application in mind and seeking a general-purpose algorithm with broad applications, our purposed methodology should possess the following properities (adapted from criteria from @Nik13a):

* P1: Wide range of dependence, allowing both positive and negative dependence
* P2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* P3: Flexible marginal modeling, generating hetereogenous data --- possibly from differing probability families.
* P4: Computationally feasible cumulative distribution function (cdf) for likelihood estimation.

Moreover, in simulation method must scale to high dimensions:

* S1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* S2: Procedure must scale to high dimensions while maintaining accuracy.

As others @MB13 have noted, however, simulating dependent data can be challenging. A central issue lies characterizing dependency between components in the high-dimensional random vector. The choice of correlation in typical practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g. non-normal, discrete, finite support, etc). Here we propose a scheme that ignores the analytic goal --- and, instead, aims to match the empirically-derived estimated correlation data and marginal characteristics. For normal data, the Pearson product-moment correlation describes the dependency completely. As we will see, however, simulating abrirtary random vectors with a given Pearson correlation matrix is computationally intense [@Chen2001, @Xia17], violating property **S1**. On the other hand, an analyst may consider the use of non-parametric correlation measures, such as Spearman's $\rho$ and Kendall's $\tau$, particularly when modeling non-normal data as in our application. Adding to the complexity is the well-known, marginal-dependent constraints on the possible bivariate correlation (a consequence of the bounds of bivariate distribution functions --- the *Frechet-Hoeffding bounds* [@Nelsen2007]). Denote the pointwise upper bound as $M(y_i, y_{i^\prime})$ These bounds give strict restraints on the possible bounds and cannot be overcome through algorithm design. Yet not all multivariate simulation approaches obtain these bounds [for example REF]. Computationally, algorithms that rely on serial computation scale poorly to high dimensions (refs - hierarchy) and fail to make use of modern high-performance computing including parallelization and graphical processing unit acceleration [@Li2019gpu].

To meet the desired properties in the face of the challenging setting, we present a general-purpose, scalable multivariate simulation algorithm. The crux of the method lies in the construction of a Gaussian copula [refs]. The idea that many multivariate and marginally heterogenous distribution can be constructed in such a manner is well known [refs]. This article's contriubition lies in it's application to high-dimenisonal data through a high-performance implementation (`bigsimr`) and speed-focused algorithm design. The algorithm design relies on useful properties of non-standard correlation measures, namely Kendall's $\tau$ and Spearman's $\rho$. We'll show that quality of simulation does not require the use of the standard Pearson correlation matrix, even for normally distributed data. The purpose of our method is to generate random vectors that match exactly specified any possible Kendall's $\tau$ or Spearman's $\rho$, and approximately Pearson product-moment correlation.

The study proceeds with more details on our motivating example in RNA-sequencing breast cancer data. Then we describe and justify our simulation methodology, followed by extensive Monte Carlo studies under various distributional assumputions --- emphasizing normal and non-normal scenarios. In these studies, we evaluate the accuracy and speed of our approach in comparison to other readily availble software. After evaluating *in silico*, we revisit our motivating
example by employing our method and compare our simulations to empirically-derived statistics. Finally, we'll make concluding remarks regarding the method's utility and future directions.

Intergene correlation [@Schissler2019].

# Multivariate correlation bounds

# Simulation methodology

## General algorithm

## Rank-based dependence measures

## Pearson correlation

Difficulties

# The `bigsimr` R package

Short description and key features, including GPU acceleration.

```{r setup, eval=TRUE}
## Using conda
reticulate::use_condaenv("py37")
library(bigsimr)
```

## Basic use

short description

### Specifying marginals

As stated earlier, to generate multivariate data, we need a list of marginals (and their parameters), and a correlation structure (matrix). The marginal distributions can be built up as a list of lists, where each sublist contains the information for the target distribution.

```{r}
margins = list(
  list("norm", mean = 3.14, sd = 0.1),
  list("beta", shape1 = 1, shape2 = 4),
  list("nbinom", size = 10, prob = 0.75)
)
```

The things to point out here are that in each sublist (marginal), the first item is an unnamed character string with the R name of the distribution *without a letter prefix*. E.g. instead of `rnorm`, we pass in just `"norm"`. The second thing to note is that the remaining items are *named* arguments that go along with the distribution. A full list of built-in distributions is found in the appendix.

### Specify correlation

The next step is to define a correlation structure for the multivariate distribution. This correlation matrix can either come from observed data, or we can set it ourselves, or we can generate a random correlation matrix via `bigsimr::rcor`. Let's create a simple correlation matrix where all off-diagonal elements are 0.5. Since we have 3 marginals, we need a $3\times 3$ matrix.

```{r}
rho <- matrix(0.5, nrow = 3, ncol = 3)
diag(rho) <- 1.0
rho
```

Finally we can generate a random vector with our specified marginals and correlation structure. The last argument, `type`, is looking to know what kind of correlation matrix it is receiving. Right now it can handle Pearson, Spearman, or Kendal.

### Small sample

```{r}
x = rvec(10, rho = rho, params = margins, type = "pearson")
```

On my machine, there is no dedicated GPU, so I would see the following warning message once per session.

```{r, echo=FALSE}
warning("warning.warn('No GPU/TPU found, falling back to CPU.')")
```

Taking a look at our random vector, we see that it hs 10 rows and 3 columns, one column for each marginal.

```{r}
x
```

We can simulate many more samples and then check the histogram of each margin, as well as the estimated correlation between the columns.

### Scaling up N

```{r, fig.width=7}
x = rvec(10000, rho = rho, params = margins, type = "pearson")

par(mfrow=c(1,3))
hist(x[,1], breaks = 30, xlab = "", main = "Normal")
hist(x[,2], breaks = 30, xlab = "", main = "Beta")
hist(x[,3], breaks = 30, xlab = "", main = "Negative Binomial")
```

### Evaluation

```{r}
cor(x)
```

We can see that even wih 10,000 samples, the estimated correlation of the simulated data is not exactly the same as the target correlation. This can be explained by the fact that some correlations are simply not possible due to the discrete nature of certain distributions. Another possibility is that the copula algorith is biased and needs correction. 

### List of supported distributions

```{r, eval=FALSE}
all_dists <- list(
  list(dist = "beta", shape1, shape2),
  list(dist = "binom", size, prob),
  list(dist = "cauchy", location, scale),
  list(dist = "chisq", df),
  list(dist = "exp", rate),
  list(dist = "f", df1, df2),
  list(dist = "gamma", shape, rate),
  list(dist = "geom", prob),
  list(dist = "hyper", m, n, k),
  list(dist = "logis", location, scale),
  list(dist = "lnorm", meanlog, sdlog),
  list(dist = "nbinom", size, prob),
  list(dist = "norm", mean, sd),
  list(dist = "pois", lambda),
  list(dist = "t", df),
  list(dist = "unif", min, max),
  list(dist = "weibull", shape, scale),
  list(dist = "wilcox", m, n),
  list(dist = "signrank", n)
)
```

## Comparsions to other software

### Speed 

- `copula`
- `Genord`
- `Multiord`
- other?
- CPU benchmarking
- GPU benchmarking

### Accuracy

blah

### Other comparisons

blah

# Some applications

## Simulation-based correlation checking

A multivariate approach?

Or all the pairs?

Generate, sort, and correlate algorithm @Demirtas2011 

```{r, computeFrechetBounds, echo = TRUE, eval=TRUE}

## simulation based
## devtools::install_github("adknudson/bigsimr", ref="develop")
library(bigsimr)
## ?rvec

d <- 5
rho <- rcor(d)
margins <- list(
    list("nbinom", size = 5, prob = 0.3),
    list("exp", rate = 4),
    list("binom", size = 5, prob = 0.7),
    list("norm", mean = 10, sd = 3),
    list("pois", lambda = 10)
)

margins2 <- list(
    list("nbinom", 5, 0.3),
    list("exp", 4),
    list("binom", 5, 0.7),
    list("norm", 10, 3),
    list("pois", 10)
)
 
params = margins
rho = rho
## cores = parallel::detectCores() - 1
cores = 2
reps = 1e3
type = "spearman"

## help(package="bigsimr")
rho_bounds <- computeCorBounds(params = margins, cores = cores, type = "spearman", reps = reps)
rho_bounds
```

## Simulating RNA-seq data

We apply our methodology to simulate RNA-sequencing data sets based on samples derived from breast cancer patients' tumors (the BRCA data set in TCGA). The data are freely available as part of the BRCA data set within the The Cancer Genome Atlas data warehouse and were downloaded on BLAH using the Harvard's Broad Institute's Firehouse interface. For each of the 1093 patients, the gene expression (abundance of messenger-RNA) from 20501 genes were counted using RNA-sequencing. The resultant data set can be represented as a matrix of size 1093 x 20501 with discrete values aligned to HUGO gene symbols for each patient. In this illustrative case study, we model the genewise marginal distributions as hetereogeneous negative binomial with pmf given by Equation \@ref(eq:nb) with two parameters, $p$ and $r$ (OR SWITCH TO mu and delta). Modeling RNA-seq counts as negative binomially distributed random variables is commonplace (e.g., @Zhao2018) since overdispersion is often observed for most expressed genes. We begin by estimating the marginal negative binomial parameters for each group using a simple method of moments approach by matching the negative binomial parameters with the sample mean and variance. Table/Figure XXX summarizes the estimated means, variances, and NB parameters as described in Equation \@ref(eq:nb). Notably some genes show underdisperation compared to a Poisson random varible. For simplicity we exclude these cases, but nothing in the later described simulation method prevents using a different marginal distribution in those cases (such as a Maxwell distribution REF Sellers).

\begin{equation} 
  g(n) = \mathbb P(Y=n) = \frac{\Gamma(n+r)}{\Gamma(r)n!} p^r(1-p)^n, \,\,\, n\in
\mathbb N_0.
  (\#eq:nb)
\end{equation} 

## Simulation-based correlation testing

## Simulation-based probability calculations

# Conclusion/Discussion

- Although we recommend to use the rank-based measures
- discuss the omission (or inclusion) of high dimensional covariance estimators?
- We've shown that the multivariate NB simulations outperform the Independent sims. But the agreement is still poor. This could motivate other more appropriate models for high-expressing RNA-seq data than the negative binomial distribution. This algorithm lends itself to exploration of flexible probability models to inspire model selection in the developemnt novel RNA-seq analytic methodology.

We've introduced a general-purpose high-dimensional multivariate simulation algorithm and provide a high-performance implementation called `bigsimr` (github url). The parallelized (multi-core) algorithm is simple and easy to understand as an application of Gaussian copulas. This method is largely inspired by @MB13, but with contributions with regard to scaleability, implementation, and application in high-throughput biomedical data (RNA-sequencing). We advocate the use of Kendall's $\tau$ as it better captures correlation among components of non-normal, as well as non-linear patterns of association. Moreover, the matching Kendall's $\tau$ is nearly trival due to the invariant of monotone transformations, after an adjustment to the input correlation matrix. Interestingly, our simulation studies show the use of Kendall's $\tau$ to works as well as using Pearson correlation coefficient when simulating from multivariate normal distributions.

We also show utility in our methodology through an application to differential gene expression analysis from RNA-sequencing data. The application results show that correlations indeed matter in the large-scale hypothesis testing, as many others have noted (for example, see @BE07, @Wu2012b). We also hope that the application provides an example workflow --- and other strategy to use in simulation design. Even the best-performing simulations we provide show a gap from the empirical distribution. To keep the demonstration straightforward, we did not attempt to match the empirical distribution of test statistics as precisely as possible. Yet our methodology could be more creatively applied to meet that goal. One could consider marginal distributions from different families, such as Poisson for a subset of genes. One could imagine finding a best fitting probability distribution among a class of distributions for each gene and this could perhaps a better fit. One could imagine additonal structures/features in the data that an analyset could model to improve the correspondence with the emprical values, for example row correlations and high-dimensional covariance estimators (see @Won2013g).

There are of course limitations to the methodology and implementation. The algorithm requires invertible marginal cdfs. This leaves some restriction to the available marginal probability distributions (DOES IT? FOR EXAMPLE?) An additional source of error is after converting dependencies (step i; ${\bf R_{Kendall}})$) the resultant matrix may not be positive definite (PD). Then the nearest PD matrix is used. From a practical computing standpoint, the user's computing resource provides the ultimate limit on how large a dimension can be simulated using the `bigsimr` R package. A user must consider carefully the available memory and cores when conducting a Monte Carlo experiment. The algorithm does amends itself well to parallelization and graphical processing unit acceleration [@Li2019gpu] and advanced users may take advantage of those techniques.

Future work includes developing scalable algorithms to match the Pearson correlation matrix exactly. Further, more sophicated approaches could involve simulation algorithms that are "estimation awarene" and so could explore the role of covariance estimation within the simulation. Lastly, these may reveal new approaches to estimating and evaluating high dimensional regression and Bayesian models. On the implementation side, employing graphical process unit acceleration could produce substaintial speedups over our multi-core approach.

# Conclusions 

# Supplementary Materials

We provide an open-source implementation of our methology as the `bigsimr` R package, hosted on github. 

# Acknowledgement(s)

The authors gratefully acknowledge the helpful discussions with Professor Walter W. Piegorsch and Professor Edward J. Bedrick during this project's conception.

# Disclosure statement

The authors report no conflict of interest. DO WE?

# Funding

FUNDING FROM EVERYONE HERE. CP3 grant.

# Nomenclature/Notation

# Notes

Mention Python?



