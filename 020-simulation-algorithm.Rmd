# Algorithms {#algorithms}


This section describes our methods involved in simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,\ldots,d$. Each $Y_i$ has a specified marginal CDF $F_i$ and its inverse $F^{-1}_i$. To characterize dependency, every pair $(Y_i, Y_j)$ has a given Pearson correlation $\rho_P$, Spearman correlation $\rho_S$, and/or Kendall's $\tau$. The method can be described as a *high-performance Gaussian copula* (Equation \@ref(eq:gauss)) providing a high-dimensional NORTA-inspired algorithm.


## NORmal To Anything (NORTA)


The well-known NORTA algorithm [@Cario1997] simulates a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$. Specifically, NORTA algorithm proceedings as follows:


1. Simulate a random vector $\bf Z$ with $d$ *independent and identically distributed* (iid) standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ such that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_i^{-1}[\Phi(X_i)], \; i=1,...,d$.


With modern parallel computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multi-core and/or graphical-processing-unit (GPU) accelerated algorithms --- providing fast scalability using readily-available hardware.


Matching specified Pearson correlation coefficients exactly (step 2 above), however, is problematic. In general, there is no closed-form correspondence between the components of the input $\Sigma_{\bf Z}$ and target $\Sigma_{\bf Y}$. Matching the correlations involves evaluating or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,\ldots, d, \; i \neq j$. For high-dimensional data, these evaluations are often too costly to enable feasible simulation studies. For low-dimensional problems, however, methods and tools exist to match Pearson correlations precisely [@Xia17] and the publicly available `nortaRA` R package [@Chen2001].


To maintain scalability (SP1), our solution is to essentially avoid this complication in Pearson matching. Since our goal is to simulate non-normal marginals, we greatly prefer the use of rank-based measures, such as $\rho_S$ and $\tau$, from a modeling standpoint. Further, $\rho_S$ and $\tau$'s invariance under monotone transformation (see [Background](#background)), preserves the correlation coefficients through steps 3, 4, and 5 in the NORTA algorithm above. This eliminates the need for computing the $\binom{d}{2}$ integrals to match exactly (nothing is for free, however, as discussed below in Section \@ref(rand-vec-gen)).


Now, if one does desire to characterize dependency using Pearson correlations, simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution [@Song00] in many practical applications. The quality of this approximation depends on the setting, but in practice, for high-dimensional count data we find the accuracy to be adequate. Later, we study robustness of our method to this limitation in selected [Monte Carlo evaluations]({#simulations}).


*NORTA in higher dimensions*. 
Sklar's theorem provides a useful characterization of multivariate distributions through copulas. Yet the choice of copula-based simulation algorithm affects which joint distributions may be simulated. Even in low-dimensional spaces (e.g., $d=3$), there exist valid multivariate distributions with *feasible* Pearson correlation matrices that NORTA cannot match exactly [@LH75]. This occurs when the bivariate transformations are applied to find the input correlation matrix, yet when combined the resultant matrix is indefinite. These situations do occur, even using exact analytic calculations. Such problematic target correlation matrices are termed *NORTA defective*.


@GH02 conducted a Monte Carlo study to estimate the probability of encountering NORTA defective matrices while increasing the dimension $d$. They found for what is now considered low-to-moderate dimensions ($d \approx 20$) that almost *all* feasible matrices are NORTA defective. This stems from the concentration of measure near the boundary of the space of all possible correlation matrices as dimension increases. Unfortunately, it is precisely near this boundary that NORTA defective matrices reside.


There is hope, however, as @GH02 also showed that replacing an indefinite input correlation matrix with a close proxy will give approximate matching to the target --- with adequate performance for moderate $d$. This provides evidence that our nearest positive definite (PD) augmented approach will maintain reasonable accuracy if our input matching scheme returns an indefinite matrix, at least for the rank-based matching scheme described above.


## Random vector generator {#rand-vec-gen}


We now describe our algorithm to generate random vectors, which mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:


1. Mapping step
    (i) Convert the target Spearman correlation matrix $R_S$ to the corresponding MVN Pearson correlation $R_X$.
    
        or

    (i') Convert the target Kendall $\tau$ matrix $R_K$ to the corresponding MVN Pearson correlation $R_X$.
2. Check admissibility and, if needed, compute the nearest correlation matrix.
    (i) Check that $R_X$ is a correlation matrix, a positive definite matrix with 1's along the diagonal.   
	(ii) If $R_X$ is a correlation matrix, the input matrix is *admissible* in this scheme.
	Otherwise,
    (ii$^\prime$) Replace $R_X$ with the nearest correlation matrix $\tilde{R}_X$, in the Frobenius norm.
3. NORTA step
    (i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, R_X)$.  
    (ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$.
    (iii) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$.


The "mapping step" takes advantage of the closed-form relationships between $\rho_S$ and $\tau$ with $\rho_P$ for bivariate normal random variables via Equations \@ref(eq:convertKendall) or \@ref(eq:convertSpearman), respectively (implemented as `cor_covert`).


Our algorithm allows the generation of high-dimensional multivariate data with arbitrary marginal distributions with a broad class of admissible Spearman correlation matrices and Kendall $\tau$ matrices. The admissible classes consist of the matrices that map to a Pearson correlation matrix for a MVN.


In particular, if we let $X$ be MVN with $d$ components and set


\begin{align*}
\Omega_P &= \{ R_P : R_P \textrm{ is a correlation matrix for } X \} \\
\Omega_K &= \{ R_K : R_K \textrm{ is a correlation matrix for } X \} \\
\Omega_S &= \{ R_S : R_S \textrm{ is a correlation matrix for } X \} \\
\end{align*}


then there are 1-1 mappings between these sets. We conjecture informally that the sets of admissible $R_S$ and $R_K$ are not highly restrictive. In particular, $R_P$ is approximately $R_S$ for a MVN, suggesting that the admissible set $\Omega_S$ should be flexible as $R_P$ can be any PD matrix with 1's along the diagonal. We provide methods to check whether a target $R_S$ is an element of the *admissible set* $\Omega_S$ (and similarly for $R_K$) and also for checking that each pairwise correlation value is admissible (via the MC estimation of the 2-dimensional Frechet bounds). 


Yet this scheme still suffers similar issues as the original Pearson-matching NORTA algorithm regarding the increasing probability of encountering a non-admissible correlation matrix as dimension increases. In our experience, the mapping step for large $d$ almost always produces a $R_X$ that is not in $\Omega_P$ (or even a correlation matrix). The second step handles this issue by computing the nearest correlation matrix to $R_X$. In Section \@ref(package) we provide a basic description of how to quantify and potentially control the approximation error. Further, the RNA-seq example in Section \@ref(examples) provides an illustration of this in practice.


The final step implements a high-performance NORTA algorithm. Specifically, step 3i uses a fast multi-core multivariate normal simulator, and steps 3ii, 3iii are distributed over CPU cores.


## Algorithm details


*Pearson Matching*


Given a desired target correlation $\rho_Y$ between a pair of marginal distributions, the correlation coefficient $\rho_Z$ used in the Gaussian copula must be specified so as to satisfy equation \@ref(eq:pearsonIntegralRelation).


\begin{equation}
    \mathrm{E}\left[Y_i Y_j\right] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} F_i^{-1}\left[\Phi(z_i)\right] F_j^{-1}\left[\Phi(z_j)\right] \phi(z_i, z_j, \rho_z) dz_i dz_j
    (\#eq:pearsonIntegralRelation)
\end{equation}


where $\phi(\cdot)$ is the joint probability distribution function of two correlated standard normal variables. In general, the double integral in \@ref(eq:pearsonIntegralRelation) is difficult or impossible to solve analytically, so numerical methods are required. Our implementation is based off the work of @xiao2019matching where the marginal transformation is approximated by a weighted sum of Hermite polynomials.


*Nearest Correlation Matrix*


Finding the nearest correlation matrix to an *almost* correlation matrix is a problem that has been researched since the 1990's, and improved upon in the 2000's by Nick Higham who presented an alternating projection algorithm (@higham2002computing) which is used by R (`Matrix::nearPD`), and then by Qi and Sun who developed a quadratically convergent algorithm based on the theory of strongly semismooth matrix functions (@qi2006quadratically). Our algorithm is transcription of the `R` code shared by Sun to the Julia language. The nearest correlation matrix problem can be written down as the following convex optimization problem:


\begin{align*}
    \mathrm{min}\quad & \frac{1}{2} \Vert G - X \Vert^2 \\
    \mathrm{s.t.}\quad & X_{ii} = 1, \quad i = 1, \ldots , n, \\
    & X \in S_{+}^{n}
\end{align*}
