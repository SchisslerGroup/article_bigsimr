--- 
title: "Simulating Ultra High-Dimensional Multivariate Data Using the bigsimr R Package"
author: [A.G. Schissler, A. Knudson]
date: "`r format(Sys.Date(), '%A, %B %d, %Y')`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook:
    base_format: rticles::tf_article
documentclass: article
bibliography: [bigsimr.bib, packages.bib]
biblio-style: apalike
link-citations: yes
abstract: |
  In this era of Big Data, it is critical to realistically simulate data to conduct informative Monte Carlo studies. This is problematic when data are inherently multivariate while at the same time are (ultra-) high dimensional. This situation appears frequently in observational data found on online and in high-throughput biomedical experiments (e.g., RNA-sequencing). Due to the difficulty in simulating realistic correlated data points, researchers often resort to simulation designs that posit independence --- greatly diminishing the insight into the empirical operating characteristics of any proposed methodology.  Major challenges lie in the computational complexity involved in simulating these massive random vectors. We propose a fairly general, scalable procedure to simulate high-dimensional multivariate distributions with pre-specified marginal characteristics and dependency characteristics. As a motivating example, we use our methodology to study large-scale statistical inferential procedures applied to cancer-related RNA-sequencing data sets. The proposed algorithm is implemented as the `bigsimr` R package.
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
library(ggplot2)
library(tidyverse)
library(knitr)
library(dplyr)
set.seed(06082020)
```

```{bash copyBib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
cp /Users/alfred/Dropbox/bib/bigsimr.bib ./.
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Introduction

Massive high-dimensional data sets are commonplace in many areas of scientific inquiry. As new methods are developed for these data, a fundamental challenge lies in designing and conducting simulation studies to assess operating characteristics of proposed methodology, such as false positive rates, statistical power, and robustness --- often in comparison to existing methods. Another application lies in the discovering the sampling distribution of test statistic under hypothesized null models in complex scenarios. Such Monte Carlo studies in the high-dimensional setting often become difficult to conduct with existing algorithms and tools, or require expertise in computing on clusters. This is particularly true when simulating massive multivariate, non-normal distributions. 

Along those lines, simulating high-dimension dependent negative binomial data motivates this work --- in pursuit of modeling RNA-sequencing data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. This labroratory procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. Often researchers posit a negative binomial model (refs) as counts are often over (or under) expressed that a Poisson model would suggest. Yet due to inherent biological processes, gene expression data exhibits correlation (coexpression) across genes [@BE07; @Schissler2018]. RNA-sequencing analysis has garnered much interest in the statistical literature (refs). Often researchers simulate independently or from their proposed model in order condunct Monte Carlo studies (refs). An alternavitive strategy is to think *generatively* about the data collection process and scientific domain knowledge and design simulations with probabilitistic models that reflect those processes. In our motivating example, we'll study the consequences correlation in the simulation study replicating test statistics from an illustrative, classical large-scale hypothesis testing study [see @Efron2004].

## Ultra High Dimensional multivariate modeling and simulation desired properties

With our application in mind and seeking a general-purpose algorithm with broad applications, our purposed methodology should possess the following properities (adapted from criteria from @Nik13a):

* P1: Wide range of dependence, allowing both positive and negative dependence
* P2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* P3: Flexible marginal modeling, generating hetereogenous data --- possibly from differing probability families.

Moreover, the simulation method must scale to high dimensions:

* S1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* S2: Procedure must scale to high dimensions while maintaining accuracy.

As others @MB13 have noted, however, simulating dependent data can be challenging. A central issue lies characterizing dependency between components in the high-dimensional random vector. The choice of correlation in typical practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g. non-normal, discrete, finite support, etc). Here we propose a scheme that ignores the analytic goal --- and, instead, aims to match the empirically-derived estimated correlation data and marginal characteristics. For normal data, the Pearson product-moment correlation describes the dependency completely. As we will see, however, simulating abrirtary random vectors with a given Pearson correlation matrix is computationally intense [@Chen2001, @Xia17], violating property **S1**. On the other hand, an analyst may consider the use of non-parametric correlation measures, such as Spearman's $\rho$ and Kendall's $\tau$, particularly when modeling non-normal data as in our application. Adding to the complexity is the well-known, marginal-dependent constraints on the possible bivariate correlation (a consequence of the bounds of bivariate distribution functions --- the *Frechet-Hoeffding bounds* [@Nelsen2007]). Denote the pointwise upper bound as $M(y_i, y_{i^\prime})$ These bounds give strict restraints on the possible bounds and cannot be overcome through algorithm design. Yet not all multivariate simulation approaches obtain these bounds [for example REF]. Computationally, algorithms that rely on serial computation scale poorly to high dimensions (refs - hierarchy) and fail to make use of modern high-performance computing including parallelization and graphical processing unit acceleration [@Li2019gpu].

To meet the desired properties in the face of the challenging setting, we present a general-purpose, scalable multivariate simulation algorithm. The crux of the method lies in the construction of a Gaussian copula [refs]. The idea that many multivariate and marginally heterogenous distribution can be constructed in such a manner is well known [refs]. This article's contriubition lies in it's application to high-dimenisonal data through a high-performance implementation (`bigsimr`) and speed-focused algorithm design. The algorithm design relies on useful properties of non-standard correlation measures, namely Kendall's $\tau$ and Spearman's $\rho$. We'll show that quality of simulation does not require the use of the standard Pearson correlation matrix, even for normally distributed data. The purpose of our method is to generate random vectors that match exactly specified any possible Kendall's $\tau$ or Spearman's $\rho$, and approximately Pearson product-moment correlation.

The study proceeds with more details on our motivating example in RNA-sequencing breast cancer data. Then we describe and justify our simulation methodology, followed by extensive Monte Carlo studies under various distributional assumputions --- emphasizing normal and non-normal scenarios. In these studies, we evaluate the accuracy and speed of our approach in comparison to other readily availble software. After evaluating *in silico*, we revisit our motivating
example by employing our method and compare our simulations to empirically-derived statistics. Finally, we'll make concluding remarks regarding the method's utility and future directions.

# Background and notation

## Gaussian copulas

We present an general-purpose, scalable multivaritae simulation algorithm. The crux of the method is construction of a Gaussian copula. This idea is well known [refs]. A copula is a distribution function on $[0,1]^d$ describing a random vector with standard uniform marginals. Moreover, for any random vector ${\bf X}=(X_1, \ldots, X_d)$ with cumulative distribution function (CDF) $F$ and marginal CDFs $F_i$ there is a copula function 
$C(u_1, \ldots, u_d)$ so that 

\[
F(x_1, \ldots,x_d) = \mathbb P(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), \,\,\, x_i\in \mathbb R, i=1,\ldots,d. 
\]  

A Gaussian copula is the case where all marginal CDFs $F_i$ are the standard normal cdf, $\Phi$. The Gaussian copula is the one that corresponds to a multivariate normal distribution with standard normal marginal distributions and covariance matrix ${\bf R}$. (Since the marginals are standard normal, this ${\bf R}$ is also the correlation matrix). If $F_{{\bf R}}$ is the CDF of such multivariate normal distribution, then the corresponding Gaussian copula $C_{{\bf R}}$ is defined through

\begin{equation}
(\#eq:gauss)
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

where $\Phi(\cdot)$ is the standard normal CDF. Note that the copula $C_{{\bf R}}$ is simply the CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where  $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$. 

Sklar's Theorem (ref) guarantee's that any random vector with computational feasible inverse CDFs (P4 above) and obtainable correlation matrix (within the Frechet bounds) can be obtained via transformations involving copula functions. Namely, to simulate a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$, we can construct a Gaussian copula ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$. When an $Y_i$ is discrete, some care must be taken to define $F_i$. Letting 

\begin{equation}
F_{i}^{-1} = inf\{y:F_{i}(y) \geq u \}
(\#eq:inverseCDF)
\end{equation}

\noindent ensures that $Y_i \sim F_i$.

Simulation of a general multivariate random vector ${\bf T}$ with margins $F_i$ based on this copula is quite simple. Ensuring that one obtains a certian dependence among the marginal distributions, however, proves challenging in our proposed Gaussian-copula-based scheme. Two core issues: 1) marginal characteristics induce bounds on the possible bivariate correlations and 2) montone transformations deform the Pearson correlation which no closed form expression exists in general (ref? Chen2001). The Frechet bounds are well-known.

## Measures of dependency

The population correlation coefficient, commonly called the Pearson (product-moment) correlation coefficient, describes the linear association between two random variables $X$ and $Y$ and is given by

\begin{equation}
(\#eq:pearson)
\rho(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ var(X)var(Y)\right]^{1/2}} 
\end{equation}

As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector the Pearson correlation adequately describes the dependency between the components. $\rho$'s utility in non-normal or non-linear associations is lacking (@MK01). Rank-based (ordinal) approaches performance better in these settings, such as Spearman's (denoted $\rho_s$) and Kendall's $\tau$. Define 

\begin{equation}
(\#eq:spearman)
\rho_s(X,Y) = 3 \left[ P\left[ (X_1 - X_2)(Y_1-Y_3) > 0 \right] - P\left[ (X_1 - X_2)(Y_1-Y_3) < 0 \right] \right]
\end{equation}

\noindent where $(X_1, Y_1) \overset{d}{=} (X,Y), X_2 \overset{d}{=} X, Y_3 \overset{d}{=} Y$ with $X_2$ and $Y_3$ are independent of one other and of $(X_1, Y_1)$. For continuous marginals, the measure works well due to zero probability of ties. For discrete marginals, however, one could perform a rescaled version of $\rho_s$ for random variables $X,Y$ with pmfs (or pdfs) $p(x)$ and $q(y)$, respectively. 

\begin{equation}
(\#eq:spearmanRescaled)
\rho_{RS}(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}
\end{equation}

\noindent Kendall's $\tau$ is the probability of concordant pairs minus the probability of discordant pairs and is given compactly as

\begin{equation}
  (\#eq:tau)
  \tau(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}
\end{equation}

## Marginal-dependent bivariate correlation bounds

The pairwise correlation between two correlated random variables cannot in general obtain the full range of possibile values, $[-1,-1]$. The range, called the Frechet(-Hoeffding) bounds, is a well-known function of the marginal distributions and are given by [@BF17]:

\begin{equation}
(\#eq:frechet)
\rho^{max} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho^{min} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where $U$ is a uniform random variable in $(0,1)$, and $F^{-1}_1, F^{-1}_2$ are the inverse cdf of random variables $X_1$ and $X_2$, respectively. For discrete random variables, define $F^{-1}$ as in Equation \@ref(eq:inverseCDF).

# Simulation algorithm

## Algorithm description

This section describes the method for simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$. Each $Y_i$ has a specified marginal
distribution function $F_i$ and its inverse. To characterize dependency, every pair $(Y_i, Y_j)$ has either a specified Pearson correlation \@ref(eq:pearson), (rescaled) Spearman correlation \@ref(eq:spearmanRescaled) , or Kendall's $\tau$ (Equation \@ref(eq:tau) ). The method only approximately matches the Pearson correlation in general, whereas the rank-based methods are exact.

The method is best understand as a **parallelized Gaussian copula** (see Equation \@ref(eq:gauss). We shall see that contructing continuous joint distributions that match a target Spearman or Kendall's correlations computes easily when employing Gaussian copulas, since this measures are invariant under the monotone transformations involved [refs]. To do this, we take advantage of a closed form relationship [ref?] between Kendall's $\tau$ and Pearson's correlation coefficient for bivariate normal random variables:

\begin{equation}
(\#eq:convertKendall)
r_{Pearson} = sin \left( \tau_{Kendall} \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's $\rho$ [@K58],

\begin{equation}
(\#eq:convertSpearman)
\rho_{Pearson} = 2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right).
\end{equation}

For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components rom Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similiar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In contrast the rank-based correlations, matching specified Pearson correlation coefficients exactly is computational intense in this scheme. In general, there is no closed form correspondence and involving computing or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X|r}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d$. For accurate numeric approximation of these integrals, the functions must be evaluated hundreds of times. Others have used efficient Monte Carlo integration schemes (see @Chen2001), but scale poorly to large dimension in reasonable times (property **S2**). Despite all this, if one does desire to characterize dependency using Pearson correlations, we often see in practice --- and it is theoretically justified under certain conditions (@Song00) --- that simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution.

## Simulation Algorithm

Putting the together the facts provided in the equations above, we come the following proposed simulation algorithm to produce a random vector ${\bf Y}$ with specified Spearman's correlation and marginal distributions. Note that all computational steps can be parallelized as each operation can be done or either the $d$ marginals or the $\binom{d}{2}$ pairs for the correlation values. Even the generation of multivariate normal random vectors is parallelized through optimized matrix multiplication/decomposition routines. 

### Inputs
(1) Marginal characteristics including the same of distributional family and parameter values, denotes as $F_i$ for marginal component random variable $Y_i$ for $i=1,\ldots,d$.
(2) A target (specified) Spearman's correlation matrix ${\bf R_{Spearman}}$ with
each $\rho_{Spearman}$ within the Frechet limits. Alternatively, the rescaled
Spearman's correlation (see Equation \@ref(eq:convertKendallDiscrete) ) can be provided.
(3) An error tolerance $\epsilon$ for finding the nearest positive definite matrix a transformed correlation matrix (see step iii below). Alternatively, a maximum number of iterations can be supplied.

### Algorithm
(i) Compute Spearman's $\rho_{rs}$ (see Equation
\@ref(eq:convertKendallDiscrete)) from the specified $\rho_{spearman}$ for each
pair of marginal random variables $(Y_i,Y_{i^\prime})$ with $i \neq i^\prime$
when either marginal is discrete. In such large scale computations, it may be
that numerically $\rho_{rs}$ is be larger/smaller than the Frechet bounds (or
even $\pm 1$). To guard aganist this, set $\rho_{rs} = min( \rho_{rs}, M)$,
where $M$ is the upper Frechet bound (see [Section Introduction]) and similarly set $\rho_{rs} = max( \rho_{rs}, W)$, where $W$ is the lower Frechet bound for this pair of distributions. Gather these $\rho_{rs} \lq s$ into a new input correlation matrix ${\bf R_{rs}}$.  
(ii) Convert ${\bf R_{Spearman}}$ into ${\bf R_{Pearson}}$ via $\rho_{Pearson} = 2 \times sin \left( \rho_{rs} \times \frac{\pi}{6} \right)$.   
(iii) Finally, ensure that ${\bf R_{Pearson}}$ is positive definite, by finding the nearest positive definite correlation matrix ${\bf R}$ --- using the `R` routine `nearPD` in the `Matrix` package --- within an error tolerance of $\epsilon$.  
(iv) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$;  
(v) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
(vi) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

# The `bigsimr` R package

Short description and key features, including GPU acceleration.

The most computationally extensive lies in the second step of our algorithm. We
use a optimized and parallelized multivariate normal simulator within the `R`
package `mvnfast`. The rest of the code is also parallelized, but these steps
run rapidly in serial computation except for extremely large dimension.

```{r setup, eval=TRUE}
## Using conda
reticulate::use_condaenv("py37")
library(bigsimr)
## help(package = 'bigsimr')
```

## Basic use

short description

### Specifying marginals

As stated earlier, to generate multivariate data, we need a list of marginals (and their parameters), and a correlation structure (matrix). The marginal distributions can be built up as a list of lists, where each sublist contains the information for the target distribution.

```{r}
margins = list(
  list("norm", mean = 3.14, sd = 0.1),
  list("beta", shape1 = 1, shape2 = 4),
  list("nbinom", size = 10, prob = 0.75)
)
```

The things to point out here are that in each sublist (marginal), the first item is an unnamed character string with the R name of the distribution *without a letter prefix*. E.g. instead of `rnorm`, we pass in just `"norm"`. The second thing to note is that the remaining items are *named* arguments that go along with the distribution. A full list of built-in distributions is found in the appendix.

### Specify correlation

The next step is to define a correlation structure for the multivariate distribution. This correlation matrix can either come from observed data, or we can set it ourselves, or we can generate a random correlation matrix via `bigsimr::rcor`. Let's create a simple correlation matrix where all off-diagonal elements are 0.5. Since we have 3 marginals, we need a $3\times 3$ matrix.

```{r}
rho <- matrix(0.5, nrow = 3, ncol = 3)
diag(rho) <- 1.0
rho
```

Finally we can generate a random vector with our specified marginals and correlation structure. The last argument, `type`, is looking to know what kind of correlation matrix it is receiving. Right now it can handle Pearson, Spearman, or Kendal.

### Small sample

```{r}
x = rvec(10, rho = rho, params = margins, type = "pearson")
```

On my machine, there is no dedicated GPU, so I would see the following warning message once per session.

```{r, echo=FALSE}
warning("warning.warn('No GPU/TPU found, falling back to CPU.')")
```

Taking a look at our random vector, we see that it hs 10 rows and 3 columns, one column for each marginal.

```{r}
x
```

We can simulate many more samples and then check the histogram of each margin, as well as the estimated correlation between the columns.

### Scaling up N

```{r, fig.width=7}
x = rvec(10000, rho = rho, params = margins, type = "pearson")

par(mfrow=c(1,3))
hist(x[,1], breaks = 30, xlab = "", main = "Normal")
hist(x[,2], breaks = 30, xlab = "", main = "Beta")
hist(x[,3], breaks = 30, xlab = "", main = "Negative Binomial")
```

### Evaluation

```{r}
cor(x)
```

We can see that even wih 10,000 samples, the estimated correlation of the simulated data is not exactly the same as the target correlation. This can be explained by the fact that some correlations are simply not possible due to the discrete nature of certain distributions. Another possibility is that the copula algorith is biased and needs correction. 


# Monte Carlo evaluation and Comparsions to other software

Before applying our methodology to real data simulation, we conduct several Monte Carlo studies to investigate method performance in comparison to other existing implementations. We focus the numerical experiments on assessing how well the procedure scales to high dimension with respect to reasonable computation times (property S1 above) and accurately matching marginal and dependency paramaters. The simulations will proceed in increasing complexity --- leading up to the setting in our motivating example. We begin by exploring simple exchangeable (constant) correlation structures under a large number of simulation replicates while applying the algorithm above to first continous then discrete marginal distributions. We conclude the simulation studies with $10^5$ replicated data sets with sample sizes and distributional characteristics corresponding the estimates from the motivating example data.

The CPU-based computation times we report were from runs using eight dual-threaded 3.9GHz
Xeon Gold processors on a linux workstation, allowing for parallel instructions
over 16 cores. We choos 15 cores for our experiments.

## Other multivariate simulation software

Describe the competing software and algorithms briefly.

- `copula`
- `Genord`
- `Multiord`
- `nortaRA` only does Pearson matching.

## Simulation I: Bivariate simulation using rank-based correlation

### Continuous example: Bivariate Exponential

Both Spearman's and Kendall's measures of dependency are invariant under
strictly monotone transformations (ref). This in turn provides exact simulation
of continuos marginals $F_i$ $i=1,\ldots,d$ since their corresponding quantile
function are increasing montonically over their support. The results below
provide computations times for $N=10^5$ under increasingly large dimension $d$.
For simplicity we assume multivariate Normal with varying parameters and to
additionally demonstrate the utility of Kendall's $\tau$ even in the Gaussian
setting (as opposed to the nature choice of Pearson's correlation). To layer on complexity, we now assume a multivariate gamma distribution with fixed marginal parameters.

Compare speed and accuracy.

```{r tabGamma, echo = TRUE}
## computation times normal

```
### Discrete example: Bivariate Geometric

Compare speed and accuracy.

## Simulation II: Ultra HD multivariate simulation with identical margins and exchangeable rank-based correlation. 

### UHD Continuous example: Multivariate Gamma

Compare speed and accuracy.

### UHD Discrete example: Multivariate Negative Binomial

Compare speed and accuracy.

## Simulation III: Ultra HD multivariate simulatino with heterogenous margins and exchangeable rank-based correlation. 

### UHD Heteregeous continuous example: Multivariate Gamma

Compare speed and accuracy.

### UHD Heteregeousdiscrete example: Multivariate Negative Binomial

Compare speed and accuracy.

# Some applications

## Simulating Ultra High Dimensional RNA-seq data

We apply our methodology to simulate RNA-sequencing data sets based on samples derived from breast cancer patients' tumors (the BRCA data set in TCGA). The data are freely available as part of the BRCA data set within the The Cancer Genome Atlas data warehouse and were downloaded on BLAH using the Harvard's Broad Institute's Firehouse interface. For each of the 1093 patients, the gene expression (abundance of messenger-RNA) from 20501 genes were counted using RNA-sequencing. The resultant data set can be represented as a matrix of size 1093 x 20501 with discrete values aligned to HUGO gene symbols for each patient. In this illustrative case study, we model the genewise marginal distributions as hetereogeneous negative binomial with pmf given by Equation \@ref(eq:nb) with two parameters, $p$ and $r$ (OR SWITCH TO mu and delta). Modeling RNA-seq counts as negative binomially distributed random variables is commonplace (e.g., @Zhao2018) since overdispersion is often observed for most expressed genes. We begin by estimating the marginal negative binomial parameters for each group using a simple method of moments approach by matching the negative binomial parameters with the sample mean and variance. Table/Figure XXX summarizes the estimated means, variances, and NB parameters as described in Equation \@ref(eq:nb). Notably some genes show underdisperation compared to a Poisson random varible. For simplicity we exclude these cases, but nothing in the later described simulation method prevents using a different marginal distribution in those cases (such as a Maxwell distribution REF Sellers).

Intergene correlation [@Schissler2019].

\begin{equation} 
  g(n) = \mathbb P(Y=n) = \frac{\Gamma(n+r)}{\Gamma(r)n!} p^r(1-p)^n, \,\,\, n\in
\mathbb N_0.
  (\#eq:nb)
\end{equation} 

```{r brca, echo=TRUE, eval=FALSE}
## full workflow simulating RNA-seq data

```

The data are overdispersed:

```{r figNBratio, echo = FALSE, eval=FALSE, message=FALSE, warning=FALSE, fig.path='fig/plot-', out.width= '75%', fig.align='center', fig.ext='pdf', fig.cap="The genes display extreme overdispersion. Using group-specific sample means and variances for each of the d=4777 genes, we display the distributions oflog of variance to mean ratio (log(sigma2/mu))."}
## http://www.sthda.com/english/wiki/ggplot2-histogram-easy-histogram-graph-with-ggplot2-r-package
## devtools::install_github("kassambara/easyGgplot2")
library(easyGgplot2)
## p0 <- ggplot(data = nb_dat, aes(x = log(nb_var / nb_mean), y = stat(density), fill=group))
## p0 + geom_histogram(bins = 30) + labs(x="log of variance to mean ratio") + theme_bw()
nb_dat$logRatio <- log(nb_dat$nb_var / nb_dat$nb_mean)
ggplot2.histogram(data=nb_dat, xName='logRatio',
    groupName='group', legendPosition="right",
    alpha=0.5, addDensity=TRUE,
    addMeanLine=TRUE, meanLineColor="white", meanLineSize=1.5)
```

We see that the all genes studies display overdispersion (variance greater than mean) compared to a Poisson model for the gene counts. The genes are highly hetereogeneous even when dealing with highly expressed genes after filtering. The facts taken together motivate the negative binomial model for the marginal distributions. The genes are slightly more variable within the deceased group (expected with smaller sample size and potentially due to biomedical considerations).

Next we perform some essential pre-processing of the data. Since our goal is to simulate meaningful multivariate constructions, we must first find a subset of 20501 genes that are expressed. In order words, we'll filter out the low expressing genes to allow greater range of possible $d$-variate correlations (see @NK10 for details OR SHOULD I EXPLAIN THIS MORE CAREFULLY?). Notably, many RNA-seq analytic workflows filter out low expressing genes (see \cite{Conesa2016b}) and this, admittedly, is usually done in an \emph {ad hoc} fashion. In this study, we prefer an inclusion criterion that is theoretically motivated. Based on the results in Section BLAH, we filter genes with a sample mean less than 12. In this way, we remove 4762 (23.2\%) of the genes, retaining 15,739 genes for further analysis. The filtered low-expressing genes will be simulated as independent negative binomial random variables to form a complete simulated transcriptome.

RNA-sequencing data derived from breast cancer patients (n=1093), with 20501 genes measured (from TCGA). We filter to genes with average expression greater than 10000 counts (d = 4777) across both groups. Consider basic differentially expressed genes (DEG) analysis between surviving (n0 = 941) and deceased (n1 = 152) patients. This setting can be described as large-scale simultaneous hypothesis testing, under correlation. We should evaluate existing and new methodology using simulations that reflect dependency among variables. But multivariate simulation tools often scale Tpoorly to high dimension or do not model the full range of dependency. Perform a two-sample $t$ test for each 4777 genes on the observed RNA-seq counts ($empirical$). Call a "Z score" for the $i^{th}$ gene $Z_{i} = \Phi^{-1}(t_{\approx 272}(t_i))$.

Our method requires some measure of that the covariance structure is pre-specified. In practice, however, high-dimensional covariance estimation is not that easy. Indeed there is much recent interest in this area (cite a few recent high visibility articles --- see the GPU-NORTA pub). Here we do not provide a comprehensive review on the topic. Instead we only seek to use covariance estimator that only guarantees postive-semidefiniteness while maintaining adequate estimation properties. To this end we chose to use the condition-number-regularized (\textsc{condreg}) approach of \cite{Won2013g}. This covariance estimation procedure restricts the ratio of the largest eigenvalue to the smallest eigenvalue to improve numerical results. But the method employs a penalized Gaussian likelihood. And so, rather than estimating the covariance on the discrete counts directly, we first perform a $log_{2}(x +1)$ transformation to better suit this modeling assumption. \textsc{condreg} requires a parameter, $\kappa_{max}$, that controls the largest condition number of the resultant estimated covariance matrix. Here we aim to allow a large number of nonzero pairwise correlations and so arbitrary set $\kappa_{max}=10^{4}$. The routine ran without issue on a MacBook Pro carrying BLAH in XXX units of time.

Most pairwise correlations are nearly zero with XXX \% less than |0.1|. Yet there are dense subsets of correlated genes. And even small correlations among many variables can disrupt the operating characteristics of commonly used statistical inferential procedures \cite{Schissler2018}.

In this case study in the large-scale hypothesis testing in pursuit of detecting differentially expressed genes, we aim to simulate RNA-seq counts using a multivariate negative binomial. To evaluate the simulation's utility, we systematically compare the simulated results to the empirical results. In short we computed t-statistics and p-values for each of $d=4777$ genes between two groups of breast cancer patients (1=deceased, 0=alive; $n_0 = 152, n_1=941$). Further details on the empirical results are provided in Section BLAH. We compare two aspects in the simulation design 1) Choice of correlation measure while modeling marginal distributions consistently and 2) how well does the best-performing simulation agree with the empirical results. Informed by the simulation studies in Section BLAH, we anticipate a far amount of variation at such small sample sizes and large $d$ and so replicate the entire study 100 times. 

To recap from the discussion above (Section BLAH), we estimated negative binomial parameters for each of the $d=4777$ genes and also their Pearson, Spearman, and Kendall's correlation coefficients from n=1093 breast cancer patients, within each vital status group (0=deceased, 1=alive). We generated 100 sythentic data sets for each group, with the number of $d$-dimensional random vectors produced equal to the corresponding sample sizes ($n_0 = 152, n_1=941$). These 100 samples are used to understand uncertainty in this setting and we will use the mean simulated values to compare to the empirical (and also explore the worst-case scenarios).

Model genes as marginally negative binomial with heterogenous genewise parameters. Compute sample correlations (using desired dependency measure) for the 11,407,476 genes pairs, for each group. To simulate gene expression counts, our goal is to produce a random vector ${\bf Y}=(Y_1, \ldots, Y_d)$ with **correlated** NB components. To do that, we start with a sequence of **independent** Poisson processes $N_i(t)$, $i=1,\ldots,d$, where the rate of the process $N_i(t)$ is $\lambda_i=(1-p_i)/p_i>0$ (so that $p_i=1/(1+\lambda_i)$). Now, we let ${\bf T} = (T_1, \ldots T_d)$ have a multivariate distribution on $\mathbb R_+^d$ with the PDF $f_{{\bf T}}({\bf t})$. Then, we define 

We rescaled the target Spearman correlation to account for the probability of ties via Equation \@ref(eq:convertSpearmanDiscrete). The serial computation took approximately 10 minutes on XXX. Notably, the difference between target and adjusted matrices was very small (mean relative difference: 0.708) in this particular configuration of negative binomial parameters. For problems with smaller counts and higher probabilities of ties, the adjustment can be substantial (as observed in the simulation studies above in Section BLAH).

- Kendall's $\tau$ was estimated efficently using the `pcaPP::cor.fk` function in `R`.

## Ultra High-Dimensional simulation-based correlation hypothesis testing

```{r brcaSigmaTesting, echo=TRUE, eval=FALSE}
## full workflow simulating RNA-seq data

```

## Simulation-based computation of correlation bounds

Using the Generate, sort, and correlate algorithm @Demirtas2011 

```{r, computeBounds, echo = TRUE, eval=FALSE}

## simulation based
## devtools::install_github("adknudson/bigsimr", ref="develop")
library(bigsimr)
## ?rvec

d <- 5
rho <- rcor(d)
margins <- list(
    list("nbinom", size = 5, prob = 0.3),
    list("exp", rate = 4),
    list("binom", size = 5, prob = 0.7),
    list("norm", mean = 10, sd = 3),
    list("pois", lambda = 10)
)
 
params = margins
rho = rho
## cores = parallel::detectCores() - 1
cores = 2
reps = 1e3
type = "spearman"

## help(package="bigsimr")
rho_bounds <- computeCorBounds(params = margins, cores = cores, type = type, reps = reps)
rho_bounds
```

## Simulation-based joint probability calculations

```{r densityEvaluation, echo=TRUE, eval=FALSE}
## full workflow simulating RNA-seq data

```

# Conclusion/Discussion

- Although we recommend to use the rank-based measures
- discuss the omission (or inclusion) of high dimensional covariance estimators?
- We've shown that the multivariate NB simulations outperform the Independent sims. But the agreement is still poor. This could motivate other more appropriate models for high-expressing RNA-seq data than the negative binomial distribution. This algorithm lends itself to exploration of flexible probability models to inspire model selection in the developemnt novel RNA-seq analytic methodology.

We've introduced a general-purpose high-dimensional multivariate simulation algorithm and provide a high-performance implementation called `bigsimr` (github url). The parallelized (multi-core) algorithm is simple and easy to understand as an application of Gaussian copulas. This method is largely inspired by @MB13, but with contributions with regard to scaleability, implementation, and application in high-throughput biomedical data (RNA-sequencing). We advocate the use of Kendall's $\tau$ as it better captures correlation among components of non-normal, as well as non-linear patterns of association. Moreover, the matching Kendall's $\tau$ is nearly trival due to the invariant of monotone transformations, after an adjustment to the input correlation matrix. Interestingly, our simulation studies show the use of Kendall's $\tau$ to works as well as using Pearson correlation coefficient when simulating from multivariate normal distributions.

We also show utility in our methodology through an application to differential gene expression analysis from RNA-sequencing data. The application results show that correlations indeed matter in the large-scale hypothesis testing, as many others have noted (for example, see @BE07, @Wu2012b). We also hope that the application provides an example workflow --- and other strategy to use in simulation design. Even the best-performing simulations we provide show a gap from the empirical distribution. To keep the demonstration straightforward, we did not attempt to match the empirical distribution of test statistics as precisely as possible. Yet our methodology could be more creatively applied to meet that goal. One could consider marginal distributions from different families, such as Poisson for a subset of genes. One could imagine finding a best fitting probability distribution among a class of distributions for each gene and this could perhaps a better fit. One could imagine additonal structures/features in the data that an analyset could model to improve the correspondence with the emprical values, for example row correlations and high-dimensional covariance estimators (see @Won2013g).

There are of course limitations to the methodology and implementation. The algorithm requires invertible marginal cdfs. This leaves some restriction to the available marginal probability distributions (DOES IT? FOR EXAMPLE?) An additional source of error is after converting dependencies (step i; ${\bf R_{Kendall}})$) the resultant matrix may not be positive definite (PD). Then the nearest PD matrix is used. From a practical computing standpoint, the user's computing resource provides the ultimate limit on how large a dimension can be simulated using the `bigsimr` R package. A user must consider carefully the available memory and cores when conducting a Monte Carlo experiment. The algorithm does amends itself well to parallelization and graphical processing unit acceleration [@Li2019gpu] and advanced users may take advantage of those techniques.

Future work includes developing scalable algorithms to match the Pearson correlation matrix exactly. Further, more sophicated approaches could involve simulation algorithms that are "estimation awarene" and so could explore the role of covariance estimation within the simulation. Lastly, these may reveal new approaches to estimating and evaluating high dimensional regression and Bayesian models. On the implementation side, employing graphical process unit acceleration could produce substaintial speedups over our multi-core approach.

# Supplementary Materials

We provide an open-source implementation of our methology as the `bigsimr` R package, hosted on github. 

# Acknowledgement(s)

The authors gratefully acknowledge the helpful discussions with Professor Walter W. Piegorsch and Professor Edward J. Bedrick during this project's conception.

# Disclosure statement

The authors report no conflict of interest. DO WE?

# Funding

FUNDING FROM EVERYONE HERE. CP3 grant.

# Nomenclature/Notation

# Notes

### List of supported distributions

```{r, eval=FALSE}
all_dists <- list(
  list(dist = "beta", shape1, shape2),
  list(dist = "binom", size, prob),
  list(dist = "cauchy", location, scale),
  list(dist = "chisq", df),
  list(dist = "exp", rate),
  list(dist = "f", df1, df2),
  list(dist = "gamma", shape, rate),
  list(dist = "geom", prob),
  list(dist = "hyper", m, n, k),
  list(dist = "logis", location, scale),
  list(dist = "lnorm", meanlog, sdlog),
  list(dist = "nbinom", size, prob),
  list(dist = "norm", mean, sd),
  list(dist = "pois", lambda),
  list(dist = "t", df),
  list(dist = "unif", min, max),
  list(dist = "weibull", shape, scale),
  list(dist = "wilcox", m, n),
  list(dist = "signrank", n)
)
```

Mention Python?



