Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{K58,
abstract = {Ordinally invariant, i.e., rank, measures of association for bivariate populations are discussed, with emphasis on the probabilistic and operational interpretations of their population values. The three measures considered at length are the quadrant measure, Kendall's tau, and Spearman's rho. Relationships between these measures are discussed, as are connections between these measures and certain measures of association for cross classifications. Sampling theory is surveyed with special attention to the motivation for sample values of the measures. The historical development of ordinal measures of association is outlined.},
author = {Kruskal, William H.},
doi = {10.1080/01621459.1958.10501481},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Kruskal/Kruskal - 1958 - Ordinal Measures of Association.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {284},
pages = {814--861},
title = {{Ordinal Measures of Association}},
volume = {53},
year = {1958}
}
@article{Scott2016g,
abstract = {A useful definition of ‘big data' is data that is too big to process comfortably on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated), so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine, and then averaging individual Monte Carlo draws across machines. Depending on the model, the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single-machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available, for large single-layer hierarchical models, and for Bayesian additive regression trees (BART).},
author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/17509653.2016.1142191},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Scott et al/Scott et al. - 2016 - Bayes and big data The consensus monte carlo algorithm(2).pdf:pdf},
issn = {17509661},
journal = {International Journal of Management Science and Engineering Management},
keywords = {@Read{\_}review,Bayesian inference,Big data,Distributed computing,Embarrassingly parallel,Markov chain Monte Carlo,Pro:NSF{\_}proposal,Projects:GPU{\_}NORTA},
mendeley-tags = {@Read{\_}review,Pro:NSF{\_}proposal,Projects:GPU{\_}NORTA},
number = {2},
pages = {78--88},
title = {{Bayes and big data: The consensus monte carlo algorithm}},
volume = {11},
year = {2016}
}
@article{Lewandowski2009g,
abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276-294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177-2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
doi = {10.1016/j.jmva.2009.04.008},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Lewandowski, Kurowicka, Joe/Lewandowski, Kurowicka, Joe - 2009 - Generating random correlation matrices based on vines and extended onion method.pdf:pdf},
isbn = {0047-259X},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation,Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
number = {9},
pages = {1989--2001},
publisher = {Elsevier Inc.},
title = {{Generating random correlation matrices based on vines and extended onion method}},
url = {http://dx.doi.org/10.1016/j.jmva.2009.04.008},
volume = {100},
year = {2009}
}
@article{Schissler2019,
author = {Schissler, Alfred Grant and Aberasturi, Dillon and Kenost, Colleen and Lussier, Yves A.},
doi = {10.3389/fgene.2019.00414},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Schissler et al/Schissler et al. - 2019 - A Single-Subject Method to Detect Pathways Enriched With Alternatively Spliced Genes.pdf:pdf},
issn = {1664-8021},
journal = {Frontiers in Genetics},
keywords = {Alternative splicing,Hellinger distance,Isoform,Local false discovery rate,Pathways,Precision medicine,RNA-Seq,Systems biology},
month = {may},
number = {414},
title = {{A Single-Subject Method to Detect Pathways Enriched With Alternatively Spliced Genes}},
url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00414/full},
volume = {10},
year = {2019}
}
@article{Song00,
author = {Song, Peter Xue-kun},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Song/Song - 2000 - Multivariate Dispersion Models Generated from Gaussian Copula.pdf:pdf},
journal = {Scandinavian Journal of Statistics},
keywords = {copula,dependence,dispersion model,general-,generalized estimating equation},
number = {2},
pages = {305--320},
title = {{Multivariate Dispersion Models Generated from Gaussian Copula}},
volume = {27},
year = {2000}
}
@article{Zhang2017g,
abstract = {For big data analysis, high computational cost for Bayesian methods often limits their applications in practice. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov chain Monte Carlo methods, namely, Hamiltonian Monte Carlo. The key idea is to explore and exploit the structure and regularity in parameter space for the underlying probabilistic model to construct an effec-tive approximation of its geometric properties. To this end, we build a surrogate function to approximate the target distribution using properly chosen random bases and an effi-cient optimization process. The resulting method provides a flexible, scalable, and efficient sampling algorithm, which converges to the correct target distribution. We show that by choosing the basis functions and optimization process differ-ently, our method can be related to other approaches for the construction of surrogate functions such as generalized addi-tive models or Gaussian process models. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the-art methods.},
author = {Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai and Zhang, B Cheng},
doi = {10.1007/s11222-016-9699-1},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Zhang et al/Zhang et al. - 2017 - Hamiltonian Monte Carlo acceleration using surrogate functions with random bases.pdf:pdf},
journal = {Statistics and Computing},
keywords = {Hamiltonian dynamics,Markov chain Monte Carlo,Projects:GPU{\_}NORTA,Projects:Stan{\_}talk,Random bases,Surrogate method},
mendeley-tags = {Projects:GPU{\_}NORTA,Projects:Stan{\_}talk},
pages = {1473--1490},
title = {{Hamiltonian Monte Carlo acceleration using surrogate functions with random bases}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs11222-016-9699-1.pdf},
volume = {27},
year = {2017}
}
@techreport{Cario1997,
abstract = {We describe a model for representing random vectors whose component random variables have arbitrary marginal distributions and correlation matrix, and describe how to generate data based upon this model for use in a stochastic simulation. The central idea is to transform a multivariate normal random vector into the desired random vector, so we refer to these vectors as having a NORTA (NORmal To Anything) distribution. NORTA vectors are most useful when the marginal distributions of the component random variables are neither identical nor from the same family of distributions, and they are particularly valuable when the dimension of the random vector is greater than two. Several numerical examples are provided.},
author = {Cario, Marne C. and Nelson, Barry L.},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Cario, Nelson/Cario, Nelson - 1997 - Modeling and generating random vectors with arbitrary marginal distributions and correlation matrix.pdf:pdf},
keywords = {Projects:GPU{\_}NORTA,copulas,correlation matrix,input modeling,random vector,simulation},
mendeley-tags = {Projects:GPU{\_}NORTA},
pages = {1--19},
publisher = {Technical Report},
title = {{Modeling and generating random vectors with arbitrary marginal distributions and correlation matrix}},
year = {1997}
}
@article{Anders2010g,
abstract = {High-throughput sequencing assays such as RNA-Seq, ChIP-Seq or barcode counting provide quantitative readouts in the form of count data. To infer differential signal in such data correctly and with good statistical power, estimation of data variability throughout the dynamic range and a suitable error model are required. We propose a method based on the negative binomial distribution, with variance and mean linked by local regression and present an implementation, DESeq, as an R/Bioconductor package.},
author = {Anders, Simon and Huber, W},
doi = {10.1186/gb-2010-11-10-r106},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Anders, Huber/Anders, Huber - 2010 - Differential expression analysis for sequence count data.pdf:pdf},
isbn = {1474-7596},
issn = {1465-6906},
journal = {Genome Biology},
pmid = {20979621},
title = {{Differential expression analysis for sequence count data}},
url = {http://www.biomedcentral.com/content/pdf/gb-2010-11-10-r106.pdf},
year = {2010}
}
@article{BE07,
abstract = {Large-scale hypothesis testing problems, with hundreds or thousands of test statistics zi to consider at once, have become familiar in current practice. Applications of popular analysis methods, such as false discovery rate techniques, do not require independence of the zi's, but their accuracy can be compromised in high-correlation situations. This article presents computational and theoretical methods for assessing the size and effect of correlation in large-scale testing. A simple theory leads to the identification of a single omnibus measure of correlation for the zi's order statistic. The theory relates to the correct choice of a null distribution for simultaneous significance testing and its effect on inference. Large-scale hypothesis testing problems, with hundreds or thousands of test statistics zi to consider at once, have become familiar in current practice. Applications of popular analysis methods, such as false discovery rate techniques, do not require independence of the zi's, but their accuracy can be compromised in high-correlation situations. This article presents computational and theoretical methods for assessing the size and effect of correlation in large-scale testing. A simple theory leads to the identification of a single omnibus measure of correlation for the zi's order statistic. The theory relates to the correct choice of a null distribution for simultaneous significance testing and its effect on inference.},
author = {Efron, Bradley},
doi = {10.1198/016214506000001211},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Efron/Efron - 2007 - Correlation and large-scale simultaneous significance testing.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Correlated processes,Empirical null,False discovery rate,Microarray},
number = {477},
pages = {93--103},
title = {{Correlation and large-scale simultaneous significance testing}},
volume = {102},
year = {2007}
}
@article{Hardin2013g,
abstract = {Simulating sample correlation matrices is important in many ar-eas of statistics. Approaches such as generating Gaussian data and finding their sample correlation matrix or generating random uniform [−1, 1] deviates as pairwise correlations both have drawbacks. We de-velop an algorithm for adding noise, in a highly controlled manner, to general correlation matrices. In many instances, our method yields results which are superior to those obtained by simply simulating Gaussian data. Moreover, we demonstrate how our general algorithm can be tailored to a number of different correlation models. Using our results with a few different applications, we show that simulating correlation matrices can help assess statistical methodology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1106.5834v4},
author = {Hardin, Johanna and Garcia, Stephan Ramon and Golan, David},
doi = {10.1214/13-AOAS638},
eprint = {arXiv:1106.5834v4},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Hardin, Garcia, Golan/Hardin, Garcia, Golan - 2013 - A METHOD FOR GENERATING REALISTIC CORRELATION MATRICES.pdf:pdf},
journal = {The Annals of Applied Statistics},
keywords = {@Read{\_}review,Correlation matrix,Projects:GPU{\_}NORTA,Toeplitz matrix,Weyl inequalities,eigenvalues,simulating matrices},
mendeley-tags = {@Read{\_}review,Projects:GPU{\_}NORTA},
number = {3},
pages = {1733--1762},
title = {{A method for generating realistic correlation matrices}},
url = {https://arxiv.org/pdf/1106.5834.pdf},
volume = {7},
year = {2013}
}
@article{Lee2010g,
abstract = {We present a case study on the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. Graphics cards, containing multiple Graphics Processing Units (GPUs), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers and can be thought of as prototypes of the next generation of many-core processors. For certain classes of population-based Monte Carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multicore processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. On a canonical set of stochastic simulation examples including population-based Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we find speedups from 35- to 500-fold over conventional single-threaded computer code. Our findings suggest that GPUs have the potential to facilitate the growth of st...},
archivePrefix = {arXiv},
arxivId = {0905.2441},
author = {Lee, Anthony and Yau, Christopher and Giles, Michael B. and Doucet, Arnaud and Holmes, Christopher C.},
doi = {10.1198/jcgs.2010.10039},
eprint = {0905.2441},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Lee et al/Lee et al. - 2009 - On the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods.pdf:pdf},
isbn = {1061-8600},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {1 south parks road,9 alfred street,General purpose computation on graphics processing,Many-core architecture,Parallel processing,Population-based Markov chain Monte Carlo,Projects:GPU{\_}NORTA,Stochastic simulation,ac,and university of oxford,carlo,department of statistics,email,general purpose com-,lee,many-core architecture,ox,oxford ox1 3tg,oxford ox1 4eh,oxford-man institute,parallel processing,population-based markov chain monte,putation on graphics processing,sequential monte carlo,stats,stochastic simulation,uk,units},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {jan},
number = {4},
pages = {769--789},
pmid = {22003276},
publisher = {Taylor {\&} Francis},
title = {{On the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods}},
url = {http://arxiv.org/abs/0905.2441{\%}0Ahttp://dx.doi.org/10.1198/jcgs.2010.10039 http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.10039},
volume = {19},
year = {2010}
}
@article{Preis2009g,
abstract = {The compute unified device architecture (CUDA) is a programming approach for performing scientific calculations on a graphics processing unit (GPU) as a data-parallel computing device. The programming interface allows to implement algorithms using extensions to standard C language. With continuously increased number of cores in combination with a high memory bandwidth, a recent GPU offers incredible resources for general purpose computing. First, we apply this new technology to Monte Carlo simulations of the two dimensional ferromagnetic square lattice Ising model. By implementing a variant of the checkerboard algorithm, results are obtained up to 60 times faster on the GPU than on a current CPU core. An implementation of the three dimensional ferromagnetic cubic lattice Ising model on a GPU is able to generate results up to 35 times faster than on a current CPU core. As proof of concept we calculate the critical temperature of the 2D and 3D Ising model using finite size scaling techniques. Theoretical results for the 2D Ising model and previous simulation results for the 3D Ising model can be reproduced.},
author = {Preis, Tobias and Virnau, Peter and Paul, Wolfgang and Schneider, Johannes J.},
doi = {10.1016/J.JCP.2009.03.018},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Preis et al/Preis et al. - 2009 - GPU accelerated Monte Carlo simulation of the 2D and 3D Ising model.pdf:pdf},
issn = {0021-9991},
journal = {Journal of Computational Physics},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {jul},
number = {12},
pages = {4468--4477},
publisher = {Academic Press},
title = {{GPU accelerated Monte Carlo simulation of the 2D and 3D Ising model}},
url = {http://www.sciencedirect.com/science/article/pii/S0021999109001387},
volume = {228},
year = {2009}
}
@article{Nelsen1987,
abstract = {Using convex linear combinations of the probability functions for the discrete Fr{\'{e}}chet boundary distributions and the probability function for independent random variables, we construct bivariate probability functions for dependent discrete random variables with arbitrary marginals and any correlation between the theoretical minimum and maximum. The technique is elementary and the results can be adapted readily for simulation. {\textcopyright} 1987, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Nelsen, Roger B.},
doi = {10.1080/03610918708812585},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Nelsen/Nelsen - 1987 - Discrete bivariate distributions with given marginals and correlation.pdf:pdf},
isbn = {0361091870881},
issn = {0361-0918},
journal = {Communications in Statistics-Simulation and Computation},
keywords = {bivariate Poisson distribution,dependent random variables,discrete Fr{\'{e}}chet bounds,simulation},
number = {1},
pages = {199--208},
publisher = {Taylor {\&} Francis},
title = {{Discrete bivariate distributions with given marginals and correlation}},
volume = {16},
year = {1987}
}
@article{Chung2015g,
abstract = {When fitting hierarchical regression models, maximum likelihood (ML) estimation has computational (and, for some users, philosophical) advantages compared to full Bayesian inference, but when the number of groups is small, estimates of the covariance matrix ({\{}Sigma{\}}) of group-level varying coefficients are often degenerate. One can do better, even from a purely point estimation perspective, by using a prior distribution or penalty function. In this article, we use Bayes modal estimation to obtain positive definite covariance matrix estimates. We recommend a class of Wishart (not inverse-Wishart) priors for {\{}Sigma{\}} with a default choice of hyperparameters, that is, the degrees of freedom are set equal to the number of varying coefficients plus 2, and the scale matrix is the identity matrix multiplied by a value that is large relative to the scale of the problem. This prior is equivalent to independent gamma priors for the eigenvalues of {\{}Sigma{\}} with shape parameter 1.5 and rate parameter close to 0. It is also equivalent to independent gamma priors for the variances with the same hyperparameters multiplied by a function of the correlation coefficients. With this default prior, the posterior mode for {\{}Sigma{\}} is always strictly positive definite. Furthermore, the resulting uncertainty for the fixed coefficients is less underestimated than under classical ML or restricted maximum likelihood estimation. We also suggest an extension of our method that can be used when stronger prior information is available for some of the variances or correlations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chung, Y. and Gelman, A. and Rabe-Hesketh, S. and Liu, J. and Dorie, V.},
doi = {10.3102/1076998615570945},
eprint = {arXiv:1011.1669v3},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Chung et al/Chung et al. - 2015 - Weakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models.pdf:pdf},
isbn = {1076998615570},
issn = {1076-9986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
number = {2},
pages = {136--157},
pmid = {25246403},
title = {{Weakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models}},
url = {http://jeb.sagepub.com/cgi/doi/10.3102/1076998615570945},
volume = {40},
year = {2015}
}
@article{Beam2016g,
abstract = {In recent years, the Hamiltonian Monte Carlo (HMC) algorithm has been found to work more efficiently compared to other popular Markov Chain Monte Carlo (MCMC) methods (such as random walk Metropolis-Hastings) in generating samples from a posterior distribution. A general framework for HMC based on the use of graphical processing units (GPUs) is shown to greatly reduce the computing time needed for Bayesian inference. The most expensive computational tasks in HMC are the evaluation of the posterior kernel and computing its gradient with respect to the parameters of interest. One of primary goals of this article to show that by expressing each of these tasks in terms of simple matrix or element-wise operations and maintaining persistent objects in GPU memory, the computational time can be drastically reduced. By using GPU objects to perform the entire HMC simulation, most of the latency penalties associated with transferring data from main to GPU memory can be avoided. Thus, the proposed computational framework is conceptually very simple, but also is general enough to be applied to most problems that use HMC sampling. For clarity of exposition, the effectiveness of the proposed approach is demonstrated in the high-dimensional setting on a standard statistical model - multinomial regression. Using GPUs, analyses of data sets that were previously intractable for fully Bayesian approaches due to the prohibitively high computational cost are now feasible.},
archivePrefix = {arXiv},
arxivId = {1402.4089},
author = {Beam, Andrew L. and Ghosh, Sujit K. and Doyle, Jon},
doi = {10.1080/10618600.2015.1035724},
eprint = {1402.4089},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Beam, Ghosh, Doyle/Beam, Ghosh, Doyle - 2016 - Fast Hamiltonian Monte Carlo Using GPU Computing.pdf:pdf},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {GPU,Hamiltonian Monte Carlo,MCMC,Multinomial regression,Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
number = {2},
pages = {536--548},
title = {{Fast Hamiltonian Monte Carlo Using GPU Computing}},
volume = {25},
year = {2016}
}
@article{Ren2010g,
abstract = {As the most accurate model for simulating light propagation in heterogeneous tissues, Monte Carlo (MC) method has been widely used in the field of optical molecular imaging. However, MC method is time-consuming due to the calculations of a large number of photons propagation in tissues. The structural complexity of the heterogeneous tissues further increases the computational time. In this paper we present a parallel implementation for MC simulation of light propagation in heterogeneous tissues whose surfaces are constructed by different number of triangle meshes. On the basis of graphics processing units (GPU), the code is implemented with compute unified device architecture (CUDA) platform and optimized to reduce the access latency as much as possible by making full use of the constant memory and texture memory on GPU. We test the implementation in the homogeneous and heterogeneous mouse models with a NVIDIA GTX 260 card and a 2.40GHz Intel Xeon CPU. The experimental results demonstrate the feasibility and efficiency of the parallel MC simulation on GPU.},
annote = {could be used to show gpu in mc, not multivariate!},
author = {Ren, Nunu and Liang, Jimin and Qu, Xiaochao and Li, Jianfeng and Lu, Bingjia and Tian, Jie},
doi = {10.1364/OE.18.006811},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Ren et al/Ren et al. - 2010 - GPU-based Monte Carlo simulation for light propagation in complex heterogeneous tissues(3).pdf:pdf},
issn = {1094-4087},
journal = {Optics Express},
keywords = {Light propagation in tissues,Parallel processing,Photon migration,Projects:GPU{\_}NORTA,Turbid media},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {mar},
number = {7},
pages = {6811},
publisher = {Optical Society of America},
title = {{GPU-based Monte Carlo simulation for light propagation in complex heterogeneous tissues}},
url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-18-7-6811},
volume = {18},
year = {2010}
}
@article{Won2013g,
author = {Won, Joong-Ho and Lim, Johan and Kim, Seung-Jean and Rajaratnam, Bala},
doi = {10.1111/j.1467-9868.2012.01049.x},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Won et al/Won et al. - 2013 - Condition-number-regularized covariance estimation.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Condition number,Convex optimization,Covariance estimation,Cross‐validation,Eigenvalue,Portfolio optimization,Projects:GPU{\_}NORTA,Regularization,Risk comparisons,Shrinkage},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {jun},
number = {3},
pages = {427--450},
publisher = {Blackwell Publishing Ltd},
title = {{Condition-number-regularized covariance estimation}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2012.01049.x},
volume = {75},
year = {2013}
}
@article{Suchard2010g,
abstract = {This article describes advances in statistical computation for large-scale data analy-sis in structured Bayesian mixture models via graphics processing unit (GPU) pro-gramming. The developments are partly motivated by computational challenges arising in fitting models of increasing heterogeneity to increasingly large datasets. An example context concerns common biological studies using high-throughput technologies gen-erating many, very large datasets and requiring increasingly high-dimensional mixture models with large numbers of mixture components. We outline important strategies and processes for GPU computation in Bayesian simulation and optimization approaches, give examples of the benefits of GPU implementations in terms of processing speed and scale-up in ability to analyze large datasets, and provide a detailed, tutorial-style expo-sition that will benefit readers interested in developing GPU-based approaches in other statistical models. Novel, GPU-oriented approaches to modifying existing algorithms software design can lead to vast speed-up and, critically, enable statistical analyses that presently will not be performed due to compute time limitations in traditional computa-tional environments. Supplemental materials are provided with all source code, example data, and details that will enable readers to implement and explore the GPU approach in this mixture modeling context.},
author = {Suchard, Marc A. and Wang, Quanli and Chan, Cliburn and Frelinger, Jacob and Cron, Andrew and West, Mike},
doi = {10.1198/jcgs.2010.10016},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Suchard et al/Suchard et al. - 2010 - Understanding GPU Programming for Statistical Computation Studies in Massively Parallel Massive Mixtures.pdf:pdf},
isbn = {1061-8600 (Electronic)$\backslash$r1061-8600 (Linking)},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {jan},
number = {2},
pages = {419--438},
pmid = {20877443},
title = {{Understanding GPU Programming for Statistical Computation: Studies in Massively Parallel Massive Mixtures}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.10016},
volume = {19},
year = {2010}
}
@article{Li2019gpu,
author = {Li, Xiang and Schissler, A. Grant and Wu, Rui and Barford, Lee and {Harris, Fredrick C.}, Jr. and Harris, Frederick C.},
chapter = {46},
doi = {10.1007/978-3-030-14070-0_46},
edition = {800},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Li et al/Li et al. - 2019 - A graphical processing unit accelerated NORmal-To-Anything algorithm for high dimensional multivariate simulation.pdf:pdf},
journal = {Advances in Intelligent Systems and Computing},
pages = {339--345},
title = {{A Graphical Processing Unit Accelerated NORmal to Anything Algorithm for High Dimensional Multivariate Simulation}},
year = {2019}
}
@article{Ledoit2004g,
abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
author = {Ledoit, Olivier and Wolf, Michael},
doi = {10.1016/S0047-259X(03)00096-4},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Ledoit, Wolf/Ledoit, Wolf - 2004 - A well-conditioned estimator for large-dimensional covariance matrices.pdf:pdf},
issn = {0047-259X},
journal = {Journal of Multivariate Analysis},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {feb},
number = {2},
pages = {365--411},
publisher = {Academic Press},
title = {{A well-conditioned estimator for large-dimensional covariance matrices}},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X03000964},
volume = {88},
year = {2004}
}
@article{Bouezmarni2009,
abstract = {We study Kendall's tau and Spearman's rho concordance measures for discrete variables. We mainly provide their best bounds using positive dependence properties. These bounds are difficult to write down explicitly in general. Here, we give the explicit formula of the best bounds in a particular Fr{\'{e}}chet space in order to understand the behavior of the ranges of these measures. Also, based on the empirical copula which is viewed as a discrete distribution, we propose a new estimator of the copula function. Finally, we give useful dependence properties of the bivariate Poisson distribution and show the relationship between parameters of the Poisson distribution and both tau and rho. {\textcopyright} 2009 Taoufik Bouezmarni et al.},
author = {Bouezmarni, Taoufik and Mesfioui, Mhamed and Tajar, Abdelouahid},
doi = {10.1155/2009/895742},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Bouezmarni, Mesfioui, Tajar/Bouezmarni, Mesfioui, Tajar - 2009 - On concordance measures for discrete data and dependence properties of poisson model.pdf:pdf},
issn = {1687952X},
journal = {Journal of Probability and Statistics},
title = {{On concordance measures for discrete data and dependence properties of poisson model}},
volume = {2009},
year = {2009}
}
@article{Chen2001,
author = {Chen, Huifen},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Chen/Chen - 2001 - Initialization for NORTA Generation of Random Vectors with Specified Marginals and Correlations.pdf:pdf},
journal = {INFORMS Journal on Computing},
number = {4},
pages = {312--331},
title = {{Initialization for NORTA: Generation of Random Vectors with Specified Marginals and Correlations}},
volume = {13},
year = {2001}
}
@article{Yan2007,
abstract = {Copulas have become a popular tool in multivariate modeling successfully applied in many fields. A good open-source implementation of copulas is much needed for more practitioners to enjoy the joy of copulas. This article presents the design, features, and some implementation details of the R package copula. The package provides a carefully designed and easily extensible platform for multivariate modeling with copulas in R. S4 classes for most frequently used elliptical copulas and Archimedean copulas are imple- mented, with methods for density/distribution evaluation, random number generation, and graphical display. Fitting copula-based models with maximum likelihood method is provided as template examples. With the classes and methods in the package, the package can be easily extended by user-defined copulas and margins to solve problems.},
author = {Yan, Jun},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Yan/Yan - 2007 - Enjoy the Joy of Copulas With a Package copula.pdf:pdf},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {copula,multivariate analysis,r},
number = {4},
pages = {1--21},
title = {{Enjoy the Joy of Copulas : With a Package copula}},
url = {http://www.jstatsoft.org/v21/i04},
volume = {21},
year = {2007}
}
@article{Komura2012g,
abstract = {We present the GPU calculation with the common unified device architecture (CUDA) for the Wolff single-cluster algorithm of the Ising model. Proposing an algorithm for a quasi-block synchronization, we realize the Wolff single-cluster Monte Carlo simulation with CUDA. We perform parallel computations for the newly added spins in the growing cluster. As a result, the GPU calculation speed for the two-dimensional Ising model at the critical temperature with the linear size L=4096 is 5.60 times as fast as the calculation speed on a current CPU core. For the three-dimensional Ising model with the linear size L=256, the GPU calculation speed is 7.90 times as fast as the CPU calculation speed. The idea of quasi-block synchronization can be used not only in the cluster algorithm but also in many fields where the synchronization of all threads is required.},
author = {Komura, Yukihiro and Okabe, Yutaka},
doi = {10.1016/J.JCP.2011.09.029},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Komura, Okabe/Komura, Okabe - 2012 - GPU-based single-cluster algorithm for the simulation of the Ising model.pdf:pdf},
issn = {0021-9991},
journal = {Journal of Computational Physics},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {feb},
number = {4},
pages = {1209--1215},
publisher = {Academic Press},
title = {{GPU-based single-cluster algorithm for the simulation of the Ising model}},
url = {http://www.sciencedirect.com/science/article/pii/S0021999111005882},
volume = {231},
year = {2012}
}
@article{K38,
abstract = {In psychological work the problem of comparing two different rankings of the same set of individuals may be divided into two types. In the first type the individuals have a given order A which is objectively defined with reference to some quality, and a characteristic question is: if an observer ranks the individuals in an order B, does a comparison of B with A suggest that he possesses a reliable judgment of the quality, or, alternatively, is it probable that B could have arisen by chance? In the second type no objective order is given. Two observers consider the individuals and rank them in orders A and B. The question now is, are these orders sufficiently alike to indicate similarity of taste in the observers, or, on the other hand, are A and B incompatible within assigned limits of probability 1 An example of the first type occurs in the familiar experiments wherein an observer has to arrange a known set of weights in ascending order of weight; the second type would arise if two observers had to rank a set of musical compositions in order of preference. The measure of rank correlation proposed in this paper is capable of being applied to both problems, which are, in fact, formally very much the same. For purposes of simplicity in the exposition it has, however, been thought convenient to preserve a distinction between them.},
author = {Kendall, M. G.},
doi = {10.2307/2332226},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {81--93},
title = {{A New Measure of Rank Correlation}},
volume = {30},
year = {1938}
}
@book{Rizzo2007,
abstract = {This book covers the traditional core material of computational statistics, with an emphasis on using the R language via an examples-based approach. Suitable for an introductory course in computational statistics or for self-study, it includes R code for all examples and R notes to help explain the R programming concepts.},
author = {Rizzo, Maria L.},
booktitle = {Statistical Computing with R},
doi = {10.1201/9781420010718},
title = {{Statistical Computing with R}},
year = {2007}
}
@incollection{Ubeda-Flores2017,
author = {{\'{U}}beda-Flores, Manuel and Fern{\'{a}}ndez-S{\'{a}}nchez, Juan},
booktitle = {Copulas and Dependence Models with Applications},
doi = {10.1007/978-3-319-64221-5_15},
title = {{Sklar's theorem: The cornerstone of the Theory of Copulas}},
year = {2017}
}
@article{Nik13b,
abstract = {The continuous extension of a discrete random variable is amongst the computational methods used for estimation of multivariate normal copula-based models with discrete margins. Its advantage is that the likelihood can be derived conveniently under the theory for copula models with continuous margins, but there has not been a clear analysis of the adequacy of this method. We investigate the asymptotic and small-sample efficiency of two variants of the method for estimating the multivariate normal copula with univariate binary, Poisson, and negative binomial regressions, and show that they lead to biased estimates for the latent correlations, and the univariate marginal parameters that are not regression coefficients. We implement a maximum simulated likelihood method, which is based on evaluating the multidimensional integrals of the likelihood with randomized quasi-Monte Carlo methods. Asymptotic and small-sample efficiency calculations show that our method is nearly as efficient as maximum likelihood for fully specified multivariate normal copula-based models. An illustrative example is given to show the use of our simulated likelihood method. {\textcopyright} 2013 Elsevier B.V.},
author = {Nikoloulopoulos, Aristidis K.},
doi = {10.1016/j.jspi.2013.06.015},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Continuous extension,Jitters,Multivariate normal copula,Rectangle probabilities,Simulated likelihood},
title = {{On the estimation of normal copula discrete regression models using the continuous extension and simulated likelihood}},
year = {2013}
}
@article{LH75,
abstract = {This correspondence presents a procedure for generating correlated random variables with specified non-Gaussian probability distribution functions (pdf 's) such as might be required for Monte Carlo simulation studies. Specifically, a method is presented for generating an arbitrary number of pseudorandom numbers each with a prescribed probability distribution and with a prescribed correlation coefficient matrix for the collection of random numbers. Collections of typical numbers generated with the method are evaluated with chi-squared tests for the distribution functions and with confidence intervals for the correlation coefficients derived from maximum likelihood estimates. In all cases tested the generated numbers passed the tests. {\textcopyright} 1975, IEEE. All rights reserved.},
author = {Li, Shing Ted and Hammond, Joseph L.},
doi = {10.1109/TSMC.1975.5408380},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
pages = {557--61},
title = {{Generation Of Pseudorandom Numbers With Specified Univariate Distributions And Correlation Coefficients}},
volume = {5},
year = {1975}
}
@article{MD07,
abstract = {In this study we compare two techniques for simulating count-valued random n-vectors Y with specified mean and correlation structure. The first technique is to use a lognormal-Poisson hierarchy (L-P method). A vector of correlated normals Z is generated and transformed to a vector of lognormals X. Then, Y is generated as conditionally independent Poissons with means X-i. The L-P method is simple, fast, and familiar to many researchers. However, the method requires each Y-i to be overdispersed (i.e., sigma(2) {\textgreater} mu), and only low correlations are possible with this method when the variables have small means. We develop a second technique to generate the elements of Y as overlapping sums (OS) of independent X-j's (OS method). For example, suppose X, X-1, and X-2 are independent. If Y-1 = X + X-1 and Y-2 = X + X-2, then Y-1 and Y-2 are correlated because they share the common component X. A generalized version of the OS method for simulating n-vectors of two-Parameter count-valued distributions is presented. The OS method is shown to address some of the shortcomings of the L-P method. In particular, underdispersed random variables can be simulated, and high correlations are feasible even when the means are small. However, negative correlations cannot be simulated with the OS method, and when n {\textgreater} 3, the OS method is more complicated to implement than the L-P method.},
author = {Madsen, L. and Dalthorp, D.},
doi = {10.1007/s10651-007-0008-1},
issn = {13528505},
journal = {Environmental and Ecological Statistics},
keywords = {Generate correlated discrete,Lognormal-Poisson,Negative binomial,Spatial dynamics,Spatial ecology,Taylor's power law},
title = {{Simulating correlated count data}},
year = {2007}
}
@article{DH2011,
abstract = {Correlations among variables are typically not free to vary between -1 and 1, with bounds determined by the marginal distributions. Computing upper and lower limits of correlations given the marginal characteristics often raises theoretical and computational challenges. We propose a simple sorting technique that is predicated upon a little-known consequence of a well-established fact from statistical distribution theory to obtain approximate correlation bounds. This approach works regardless of the data type or distribution. We believe that it has practical value in appropriately specifying the correlation structure in simulation studies.},
author = {Demirtas, Hakan and Hedeker, Donald},
doi = {10.1198/tast.2011.10090},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Demirtas, Hedeker/Demirtas, Hedeker - 2011 - A practical way for computing approximate lower and upper correlation bounds.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Correlation limits,Simulation,Sorting},
number = {2},
pages = {104--109},
title = {{A practical way for computing approximate lower and upper correlation bounds}},
volume = {65},
year = {2011}
}
@article{Li2011c,
abstract = {Background: RNA-Seq is revolutionizing the way transcript abundances are measured. A key challenge in transcript quantification from RNA-Seq data is the handling of reads that map to multiple genes or isoforms. This issue is particularly important for quantification with de novo transcriptome assemblies in the absence of sequenced genomes, as it is difficult to determine which transcripts are isoforms of the same gene. A second significant issue is the design of RNA-Seq experiments, in terms of the number of reads, read length, and whether reads come from one or both ends of cDNA fragments.Results: We present RSEM, an user-friendly software package for quantifying gene and isoform abundances from single-end or paired-end RNA-Seq data. RSEM outputs abundance estimates, 95{\%} credibility intervals, and visualization files and can also simulate RNA-Seq data. In contrast to other existing tools, the software does not require a reference genome. Thus, in combination with a de novo transcriptome assembler, RSEM enables accurate transcript quantification for species without sequenced genomes. On simulated and real data sets, RSEM has superior or comparable performance to quantification methods that rely on a reference genome. Taking advantage of RSEM's ability to effectively use ambiguously-mapping reads, we show that accurate gene-level abundance estimates are best obtained with large numbers of short single-end reads. On the other hand, estimates of the relative frequencies of isoforms within single genes may be improved through the use of paired-end reads, depending on the number of possible splice forms for each gene.Conclusions: RSEM is an accurate and user-friendly software tool for quantifying transcript abundances from RNA-Seq data. As it does not rely on the existence of a reference genome, it is particularly useful for quantification with de novo transcriptome assemblies. In addition, RSEM has enabled valuable guidance for cost-efficient design of quantification experiments with RNA-Seq, which is currently relatively expensive. {\textcopyright} 2011 Li and Dewey; licensee BioMed Central Ltd.},
author = {Li, Bo and Dewey, Colin N.},
doi = {10.1186/1471-2105-12-323},
issn = {14712105},
journal = {BMC Bioinformatics},
pmid = {21816040},
title = {{RSEM: Accurate transcript quantification from RNA-Seq data with or without a reference genome}},
year = {2011}
}
@article{Wu2012b,
abstract = {Competitive gene set tests are commonly used in molecular pathway analysis to test for enrichment of a particular gene annotation category amongst the differential expression results from a microarray experiment. Existing gene set tests that rely on gene permutation are shown here to be extremely sensitive to inter-gene correlation. Several data sets are analyzed to show that inter-gene correlation is non-ignorable even for experiments on homogeneous cell populations using genetically identical model organisms. A new gene set test procedure (CAMERA) is proposed based on the idea of estimating the inter-gene correlation from the data, and using it to adjust the gene set test statistic. An efficient procedure is developed for estimating the inter-gene correlation and characterizing its precision. CAMERA is shown to control the type I error rate correctly regardless of inter-gene correlations, yet retains excellent power for detecting genuine differential expression. Analysis of breast cancer data shows that CAMERA recovers known relationships between tumor subtypes in very convincing terms. CAMERA can be used to analyze specified sets or as a pathway analysis tool using a database of molecular signatures.},
author = {Wu, Di and Smyth, Gordon K.},
doi = {10.1093/nar/gks461},
isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
month = {sep},
number = {17},
pages = {e133--e133},
pmid = {22638577},
publisher = {Oxford University Press},
title = {{Camera: A competitive gene set test accounting for inter-gene correlation}},
url = {https://academic.oup.com/nar/article/40/17/e133/2411151},
volume = {40},
year = {2012}
}
@book{Rizzo2019,
address = {Boca Raton},
author = {Rizzo, Maria L},
edition = {2nd},
isbn = {1466553332},
publisher = {CRC Press},
title = {{Statistical computing with R}},
year = {2019}
}
@article{Smith2020,
abstract = {Variational methods are attractive for computing Bayesian inference for highly parametrized models and large datasets where exact inference is impractical. They approximate a target distribution - either the posterior or an augmented posterior - using a simpler distribution that is selected to balance accuracy with computational feasibility. Here we approximate an element-wise parametric transformation of the target distribution as multivariate Gaussian or skew-normal. Approximations of this kind are implicit copula models for the original parameters, with a Gaussian or skew-normal copula function and flexible parametric margins. A key observation is that their adoption can improve the accuracy of variational inference in high dimensions at limited or no additional computational cost. We consider the Yeo-Johnson and G{\&}H transformations, along with sparse factor structures for the scale matrix of the Gaussian or skew-normal. We also show how to implement efficient reparametrization gradient methods for these copula-based approximations. The efficacy of the approach is illustrated by computing posterior inference for three different models using six real datasets. In each case, we show that our proposed copula model distributions are more accurate variational approximations than Gaussian or skew-normal distributions, but at only a minor or no increase in computational cost.},
archivePrefix = {arXiv},
arxivId = {1904.07495},
author = {Smith, Michael Stanley and Loaiza-Maya, Rub{\'{e}}n and Nott, David J.},
doi = {10.1080/10618600.2020.1740097},
eprint = {1904.07495},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
title = {{High-dimensional Copula Variational Approximation through Transformation}},
year = {2020}
}
@article{ElKaroui2008g,
abstract = {Estimating the eigenvalues of a population covariance matrix from a sample covariance matrix is a problem of fundamental importance in multivariate statistics; the eigenvalues of covariance matrices play a key role in many widely used techniques, in particular in principal component analysis (PCA). In many modern data analysis problems, statisticians are faced with large datasets where the sample size, n, is of the same order of magnitude as the number of variables p. Random matrix theory predicts that in this context, the eigenvalues of the sample covariance matrix are not good estimators of the eigenvalues of the population covariance. We propose to use a fundamental result in random matrix theory, the Marˇcenko–Pastur equation, to better estimate the eigenvalues of large dimensional covariance matrices. The Marˇcenko–Pastur equation holds in very wide generality and under weak assumptions. The estimator we obtain can be thought of as “shrinking” in a nonlinear fashion the eigenvalues of the sample covariance matrix to estimate the population eigenvalues. Inspired by ideas of random matrix theory, we also suggest a change of point of view when thinking about estimation of high-dimensional vectors: we do not try to estimate directly the vectors but rather a probability measure that describes them. We think this is a theoretically more fruitful way to think about these problems. Our estimator gives fast and good or very good results in extended simulations. Our algorithmic approach is based on convex optimization. We also show that the proposed estimator is consistent.},
author = {{El Karoui}, Noureddine},
doi = {10.1214/07-AOS581},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {@Read{\_}review,Convex optimization,Covariance matrices,Eigenvalues of co-variance matrices,High-dimensional inference,Mar??enko-Pastur equation,Principal component analysis,Projects:GPU{\_}NORTA,Random matrix theory,Stieltjes transform},
mendeley-tags = {@Read{\_}review,Projects:GPU{\_}NORTA},
title = {{Spectrum estimation for large dimensional covariance matrices using random matrix theory}},
year = {2008}
}
@book{MK01,
author = {Mari, Dominique Drouet and Kotz, Samuel},
isbn = {1860942644},
publisher = {World Scientific},
title = {{Correlation and dependence}},
year = {2001}
}
@article{Wilkins1994,
abstract = {Two artifical neural network classifiers, the well-known Multi- layer Perceptron (MLP) (also known as the 'backpropagation network'), and the more recently developed Radial Basis Function (RBF) network, were evaluated and compared for their ability to identify multivariate flow cytometric data from Jive North Sea plankton groups (Dinoflagellidae, Bacillariophyceae, Prymnesiomonadida, Cryptomonadida, and other flagellates). RBF networks generally performed similarly to MLPs, and slightly better in cases where the data M!ere markedly multimodal; RBF networks also have much shorter training times. The performance of MLPs was improved greatly by the use of a symmetrical bipolar 'transfer function' as opposed to the commonly-used asymmetric form. The issues of network optimisation and computational efficiency in use are discussed.},
author = {Wilkins, Malcolm F. and Morris, C W and Boddy, Lynne},
doi = {10.1093/bioinformatics/10.3.285},
isbn = {0266-7061},
issn = {1367-4803},
journal = {Computer Applications in the Biosciences},
month = {jun},
number = {3},
pages = {285--294},
publisher = {Oxford University Press},
title = {{A Comparison of Radial Basis Function and Backpropagation Neural Networks for Identification of Marine-Phytoplankton from Multivariate Flow-Cytometry Data}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/10.3.285},
volume = {10},
year = {1994}
}
@article{XZ19,
abstract = {A Gaussian copula is widely used to define correlated random variables. To obtain a prescribed Pearson correlation coefficient of $\rho$x between two random variables with given marginal distributions, the correlation coefficient $\rho$z between two standard normal variables in the copula must take a specific value which satisfies an integral equation that links $\rho$x to $\rho$z. In a few cases, this equation has an explicit solution, but in other cases it must be solved numerically. This paper attempts to address this issue. If two continuous random variables are involved, the marginal transformation is approximated by a weighted sum of Hermite polynomials; via Mehler's formula, a polynomial of $\rho$z is derived to approximate the function relationship between $\rho$x and $\rho$z. If a discrete variable is involved, the marginal transformation is decomposed into piecewise continuous ones, and $\rho$x is expressed as a polynomial of $\rho$z by Taylor expansion. For a given $\rho$x, $\rho$z can be efficiently determined by solving a polynomial equation.},
author = {Xiao, Qing and Zhou, Shaowu},
doi = {10.1080/03610926.2018.1439962},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Xiao, Zhou/Xiao, Zhou - 2019 - Matching a correlation coefficient by a Gaussian copula.pdf:pdf},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Gaussian copula,Mehler's formula,Mehler's formula.,continuous variables,correlation coefficient,discrete variables},
number = {7},
pages = {1728--1747},
title = {{Matching a correlation coefficient by a Gaussian copula}},
volume = {48},
year = {2019}
}
@misc{Wang2009b,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
booktitle = {Nature Reviews Genetics},
doi = {10.1038/nrg2484},
issn = {14710056},
title = {{RNA-Seq: A revolutionary tool for transcriptomics}},
year = {2009}
}
@manual{Fasiolo2016,
author = {Fasiolo, Matteo},
title = {{An introduction to mvnfast. R package version 0.1.6.}},
url = {https://cran.r-project.org/package=mvnfast},
year = {2016}
}
@article{LX19,
abstract = {ABSTRACTIt is of fundamental interest in statistics to test the significance of a set of covariates. For example, in genome-wide association studies, a joint null hypothesis of no genetic effect is tested for a set of multiple genetic variants. The minimum p-value method, higher criticism, and Berk–Jones tests are particularly effective when the covariates with nonzero effects are sparse. However, the correlations among covariates and the nonGaussian distribution of the response pose a great challenge toward the p-value calculation of the three tests. In practice, permutation is commonly used to obtain accurate p-values, but it is computationally very intensive, especially when we need to conduct a large amount of hypothesis testing. In this paper, we propose a Gaussian approximation method based on a Monte Carlo scheme, which is computationally more efficient than permutation while still achieving similar accuracy. We derive nonasymptotic approximation error bounds that could vanish in the limit even if ...},
author = {Liu, Yaowu and Xie, Jun},
doi = {10.1080/01621459.2017.1407776},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Berk–Jones test,Genome-wide association study,High dimensionality,Higher criticism,Monte Carlo method},
title = {{Accurate and Efficient P-value Calculation Via Gaussian Approximation: A Novel Monte-Carlo Method}},
year = {2019}
}
@article{Tchen1980,
author = {Tchen, Andre H.},
doi = {10.1214/aop/1176994668},
issn = {0091-1798},
journal = {The Annals of Probability},
keywords = {tchen80},
number = {4},
pages = {814--827},
title = {{Inequalities for Distributions with Given Marginals}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {8},
year = {1980}
}
@article{Park1996,
abstract = {Correlated binary data are frequently analyzed in studies of repeated measurements, reliability analysis, and others. In such studies correlations among binary variables are usually nonnegative. This article provides a simple algorithm for generating an arbitrary dimensional random vector of non-negatively correlated binary variables. In some frequently encountered situations the algorithm reduces to explicit expressions. The correlated binary variables are generated from correlated Poisson variables. The key idea lies in the property that any Poisson random variable can be expressed as a convolution of other independent Poisson random variables. The binary variables have desired correlations by sharing common independent Poisson variables. {\textcopyright} 1996 Taylor {\&} Francis Group, LLC.},
author = {Park, Chul Gyu and Park, Taesung and Shin, Dong Wan},
doi = {10.1080/00031305.1996.10473557},
issn = {15372731},
journal = {American Statistician},
keywords = {Generalized estimating equations,Poisson variables,Random number generation},
title = {{A Simple Method for Generating Correlated Binary Variates}},
year = {1996}
}
@misc{TCGAg,
author = {{TCGA Research Network}},
title = {{The Cancer Genome Atlas}},
url = {https://cancergenome.nih.gov/}
}
@article{Zhao2018,
author = {Zhao, Lili and Wu, Weisheng and Feng, Dai and Jiang, Hui and Nguyen, XuanLong},
doi = {10.1214/17-BA1055},
issn = {1936-0975},
journal = {Bayesian Analysis},
number = {2},
pages = {411--436},
title = {{Bayesian Analysis of RNA-Seq Data Using a Family of Negative Binomial Models}},
url = {https://projecteuclid.org/euclid.ba/1491616976},
volume = {13},
year = {2018}
}
@misc{Conesa2016b,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Michal Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
booktitle = {Genome Biology},
doi = {10.1186/s13059-016-0881-8},
issn = {1474760X},
title = {{A survey of best practices for RNA-seq data analysis}},
year = {2016}
}
@article{NK10,
abstract = {Multivariate count data occur in several different disciplines. However, existing models do not offer great flexibility for dependence modeling. Models based on copulas nowadays are widely used for continuous data dependence modeling. Modeling count data via copulas is still in its infancy; see the recent article of Genest and Neslehova (2007). A series of different copula models providing various residual dependence structures are considered for vectors of count response variables whose marginal distributions depend on covariates through negative binomial regressions. A real data application related to the number of purchases of different products is provided.},
author = {Nikoloulopoulos, Aristidis K. and Karlis, Dimitris},
doi = {10.1080/03610910903391262},
issn = {03610918},
journal = {Communications in Statistics: Simulation and Computation},
keywords = {Archimedean copulas,Kendall's tau,Market basket count data,Mixtures of max-id copulas,Partially symmetric copulas},
title = {{Modeling multivariate count data using copulas}},
year = {2010}
}
@article{VanWieringen2016,
abstract = {The ridge estimation of the precision matrix is investigated in the setting where the number of variables is large relative to the sample size. First, two archetypal ridge estimators are reviewed and it is noted that their penalties do not coincide with common quadratic ridge penalties. Subsequently, starting from a proper ℓ2-penalty, analytic expressions are derived for two alternative ridge estimators of the precision matrix. The alternative estimators are compared to the archetypes with regard to eigenvalue shrinkage and risk. The alternatives are also compared to the graphical lasso within the context of graphical modeling. The comparisons may give reason to prefer the proposed alternative estimators.},
archivePrefix = {arXiv},
arxivId = {1403.0904},
author = {{Van Wieringen}, Wessel N. and Peeters, Carel F.W.},
doi = {10.1016/j.csda.2016.05.012},
eprint = {1403.0904},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Graphical modeling,High-dimensional precision matrix estimation,Multivariate normal,Precision matrix,ℓ2-penalization},
title = {{Ridge estimation of inverse covariance matrices from high-dimensional data}},
year = {2016}
}
@article{BF17,
abstract = {A package for the stochastic simulation of discrete variables with assigned marginal distributions and correlation matrix is presented and discussed. The simulating mechanism relies upon the Gaussian copula, linking the discrete distributions together, and an iterative scheme recovering the correlation matrix for the copula that ensures the desired correlations among the discrete variables. Examples of its use are provided as well as three possible applications (related to probability, sampling, and inference), which illustrate the utility of the package as an efficient and easy-to-use tool both in statistical research and for didactic purposes.},
author = {Barbiero, Alessandro and Ferrari, Pier Alda},
doi = {10.1080/03610918.2016.1146758},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
keywords = {Correlation matrix,Gaussian copula,Multivariate discrete distribution},
month = {aug},
number = {7},
pages = {5123--5140},
title = {{An R package for the simulation of correlated discrete variables}},
url = {https://www.tandfonline.com/doi/full/10.1080/03610918.2016.1146758},
volume = {46},
year = {2017}
}
@article{GH02,
author = {Ghosh, Soumyadip and Henderson, Shane G},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Ghosh, Henderson/Ghosh, Henderson - 2002 - Properties Of The Norta Method In Higher Dimensions.pdf:pdf},
journal = {Proceedings of the 2002 Winter Simulation Conference},
pages = {263--269},
title = {{Properties Of The Norta Method In Higher Dimensions}},
year = {2002}
}
@book{Chernick2008,
address = {Hoboken, NJ},
author = {Chernick, Michael R.},
edition = {2nd},
title = {{Bootstrap Methods: A Guide for Practitioners and Researchers}},
year = {2008}
}
@article{Schissler2018,
author = {Schissler, Alfred Grant and Piegorsch, Walter W and Lussier, Yves A},
doi = {10.1177/0962280217712271},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Schissler, Piegorsch, Lussier/Schissler, Piegorsch, Lussier - 2018 - Testing for differentially expressed genetic pathways with single-subject N-of-1 data in the pres.pdf:pdf},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
keywords = {affinity propagation clustering,exemplar learning,gene expression data,gene set,inter-gene correlation,n-of-1,precision medicine,rna-seq,single-subject inference,triple negative breast cancer},
number = {12},
pages = {3797--3813},
title = {{Testing for differentially expressed genetic pathways with single-subject N-of-1 data in the presence of inter-gene correlation}},
url = {http://journals.sagepub.com/doi/10.1177/0962280217712271},
volume = {27},
year = {2018}
}
@book{Nelsen2007,
address = {New York},
author = {Nelsen, Roger B.},
edition = {2},
isbn = {9781475719062},
publisher = {Springer Science {\&} Business Media},
title = {{An Introduction to copulas}},
year = {2007}
}
@article{Genest1986g,
author = {Genest, Christian and Mackay, Jock},
doi = {10.1080/00031305.1986.10475414},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {nov},
number = {4},
pages = {280--283},
title = {{The Joy of Copulas: Bivariate Distributions with Uniform Marginals}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1986.10475414},
volume = {40},
year = {1986}
}
@article{CCK13,
abstract = {We derive a Gaussian approximation result for the maximum of a sum of high-dimensional random vectors. Specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the Gaussian random vectors with the same covariance matrices as the original vectors. This result applies when the dimension of random vectors ({\$}p{\$}) is large compared to the sample size ({\$}n{\$}); in fact, {\$}p{\$} can be much larger than {\$}n{\$}, without restricting correlations of the coordinates of these vectors. We also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional Gaussian random vectors obtained by multiplying the original vectors with i.i.d. Gaussian multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure. Here too, {\$}p{\$} can be large or even much larger than {\$}n{\$}. These distributional approximations, either Gaussian or conditional Gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. We demonstrate how our Gaussian approximations and the multiplier bootstrap can be used for modern high-dimensional estimation, multiple hypothesis testing, and adaptive specification testing. All these results contain nonasymptotic bounds on approximation errors.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1214/13-AOS1161},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Anti-concentration,Dantzig selector,High dimensionality,Maximum of vector sums,Slepian,Stein method},
title = {{Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors}},
year = {2013}
}
@article{MB13,
abstract = {This article describes a method for simulating n-dimensional multivariate non-normal data, with emphasis on count-valued data. Dependence is characterized by either Pearson correlations or Spearman correlations. The simulation is accomplished by simulating a vector of correlated standard normal variates. The elements of this vector are then transformed to achieve the target marginal distributions. We prove that the method corresponds to simulating data from a multivariate Gaussian copula. The simulation method does not restrict pairwise dependence beyond the limits imposed by the marginal distributions and can achieve any Pearson or Spearman correlation within those limits. Two examples are included. In the first example, marginal means, variances, Pearson correlations, and Spearman correlations are estimated from the epileptic seizure data set of Diggle et al. [P. Diggle, P. Heagerty, K.Y. Liang, and S. Zeger, Analysis of Longitudinal Data, Oxford University Press, Oxford, 2002]. Data with these means an...},
author = {Madsen, L. and Birkes, D.},
doi = {10.1080/00949655.2011.632774},
issn = {00949655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Pearson correlation,Spearman correlation,count data,rank correlation},
title = {{Simulating dependent discrete data}},
year = {2013}
}
@article{Xia17,
abstract = {For the issue of generating correlated random vector containing discrete variables, one major obstacle is to determine a suitable correlation coef- ficient $\rho$z in normal space for a specified correlation coefficient $\rho$x . This paper develops a method to solve this problem. First, the double inte- gral evaluated for $\rho$x is transformed into independent standard uniform space, then, a Quasi Monte Carlo method is introduced to calculate the double integral. For a given $\rho$x , an appropriate $\rho$z is determined by a false position method. Compared with existing methodologies, the proposed method is less efficient, but it is relatively easy to implement.},
author = {Xiao, Qing},
doi = {10.1080/03610926.2015.1024860},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Correlation,Gaussian copula,NORTA algorithm,Quasi Monte Carlo,discrete distribution},
title = {{Generating correlated random vector involving discrete variables}},
year = {2017}
}
@book{Chambers1983,
address = {Belmont, CA},
author = {Chambers, J. M. and Cleveland, W. S. and Kleiner, B. and Tukey, P. A.},
publisher = {Wadsworth {\&} Brooks},
title = {{Graphical Methods for Data Analysis}},
year = {1983}
}
@incollection{Nik13a,
address = {Heidelberg},
author = {Nikoloulopoulos, Aristidis K.},
booktitle = {Lecture Notes in Statistics: Copulae in Mathematical and Quantitative Finance},
edition = {213},
pages = {231--249},
publisher = {Springer},
title = {{Copula-based models for multivariate discrete response data}},
year = {2013}
}
@article{Sklar1959,
abstract = {Sklar, A. (1959), "Fonctions de r{\'{e}}partition {\`{a}} n dimensions et leurs marges", Publ. Inst. Statist. Univ. Paris (in French) 8: 229–231.},
author = {Sklar, A},
journal = {Publications de L'Institut de Statistique de L'Universit{\{}{\'{e}}{\}} de Paris},
title = {{Fonctions de R{\{}{\'{e}}{\}}partition {\{}{\`{a}}{\}} n Dimensions et Leurs Marges}},
year = {1959}
}
@book{Efron2012,
address = {Cambridge},
author = {Efron, Bradley},
edition = {1},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Efron/Efron - 2012 - Large-Scale Inference Empirical Bayes Methods for Estimation, Testing and Prediction.pdf:pdf},
keywords = {Projects:N1PAS},
mendeley-tags = {Projects:N1PAS},
publisher = {Cambridge University Press},
title = {{Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing and Prediction}},
year = {2012}
}
@article{Efron2004,
abstract = {Current scientific techniques in genomics and image processing routinely produce hypothesis testing problems with hundreds or thousands of cases to consider simultaneously. This poses new difficulties for the statistician, but also opens new opportunities. In particular, it allows empirical estimation of an appropriate null hypothesis. The empirical null may be considerably more dispersed than the usual theoretical null distribution that would be used for any one case considered separately. An empirical Bayes analysis plan for this situation is developed, using a local version of the false discovery rate to examine the inference issues. Two genomics problems are used as examples to show the importance of correctly choosing the null hypothesis.},
author = {Efron, Bradley},
doi = {10.1198/016214504000000089},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Empirical bayes,Empirical null hypothesis,Local false discovery rate,Microarray analysis,Projects:N1PAS,Unobserved covariates},
mendeley-tags = {Projects:N1PAS},
title = {{Large-scale simultaneous hypothesis testing: The choice of a null hypothesis}},
year = {2004}
}
@article{QS2006,
author = {Qi, Houduo and Sun, Defeng},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Qi, Sun/Qi, Sun - 2006 - Computing the A Quadratically Convergent Newton Method For Computing The Nearest Correlation Matrix.pdf:pdf},
journal = {SIAM Journal on matrix analysis and applications},
keywords = {correlation matrix,newton method,quadratic con-,semismooth matrix equation},
number = {2},
pages = {360--385},
title = {{Computing the A Quadratically Convergent Newton Method For Computing The Nearest Correlation Matrix}},
volume = {28},
year = {2006}
}
