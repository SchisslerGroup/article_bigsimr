---
title: "Simulating Ultra High-Dimensional Multivariate Data"
author:
  - Alfred G. Schissler
  - Alexander D. Knudson
  - Tomasz J. Kozubowski
  - Anna K. Panorska
  - Juli Petereit
institute: |
    "Department of Mathematics & Statistics" |
    "University of Nevada, Reno"
date: "21 Dec 2020 (updated: `r Sys.Date()`)"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
    logo: ../images/hex-bigsimr.png
bibliography: ../bigsimr.bib
abstract: "It is critical to realistically simulate data when conducting Monte Carlo studies and methods. But measurements are often correlated and high dimensional in this era of big data, such as data obtained through high-throughput biomedical experiments. Due to computational complexity and a lack of user-friendly software available to simulate these massive multivariate constructions, researchers may resort to simulation designs that posit independence or perform arbitrary data transformations. This greatly diminishes insights into the empirical operating characteristics of any proposed methodology, such as false positive rates, statistical power, interval coverage, and robustness. This article introduces the `bigsimr` R package that provides a flexible, scaleable procedure to simulate high-dimensional random vectors with given marginal characteristics and dependency measures. We'll describe the functions included in the package, including multi-core and graphical-processing-unit accelerated algorithms to simulate random vectors, estimate correlation, and find close positive semi-definite matrices. Finally, we demonstrate the power of `bigsimr` by applying these functions to our motivating dataset --- RNA-sequencing data obtained from breast cancer tumor samples with sample size $n=1212$ patients and dimension $d =1026$"
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
set.seed(12212020)
## devtools::install_github("ggobi/GGally")
## for ggpairs
library(GGally)
## xaringan::inf_mr('cmstats2020_talk.rmd')
```

<!-- 20-25 minutes -->

# 1 â€“ Introduction

## Big Multivariate Data 

- It is critical to realistically simulate data when conducting Monte Carlo studies and methods.
- But measurements are often **correlated**, **non-normal**, and **high dimensional** in this era of big data.
- One example are *RNA-seq data* obtained through high-throughput biomedical experiments.
- 3 out of 20,501 mRNA measurements from 1212 breast cancer patients (TCGA BRCA):

```{r, echo=F, out.width='60%', fig.align='center'}
knitr::include_graphics("../fig/realDataFig.png")
```

## Desired high-dimensional multivariate simulation properties

- Our goal is to simulate $N$ random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with **correlated** components.

Our purposed methodology should possess the following properties  
(adapted from @Nik13a):

* BP1: A wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of bivariate marginals can be equal to the number of dependence parameters.
* BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.

Moreover, the simulation method must **scale** to high dimensions:

* SP1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.

## Our contribution

- We propose a Gaussian-copula random vector algorithm; an augmented NORmal To Anything (NORTA) @Cario1997 method. 
- We provide a high-performance and GPU-accelerated implementation in the `R` package `bigsimr`, along with related tools.

# 2 - Background 

## Motivating example: RNA-seq data

- Simulating high-dimensional, non-normal, correlated data motivates this work --- in pursuit of modeling RNA-sequencing (RNA-seq) data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. 
- The RNA-seq data-generating process involves counting how often a particular messenger RNA (mRNA) is expressed in a biological sample.
- RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in high-dimensional data.
- For human derived samples, this results in count data corresponding to over 20,000 genes (protein-coding genomic regions) or even over 77,000 isoforms when alternatively-spliced mRNA are counted.
- Importantly, due to inherent biological processes, gene expression data exhibits correlation --- co-expression --- across genes [@BE07; @Schissler2018]. 

```{r ch010-myProb, include=FALSE, cache = TRUE}
myProb <- 0.95
```

```{r ch010-processBRCA, echo=FALSE, eval=TRUE, results='none', cache=F}
brcaRDS <- paste0( "../data/brca", round(myProb*100), ".rds" )
if ( !file.exists( brcaRDS ) )  {
    ## full workflow simulating RNA-seq data
    allDat <- readRDS( file = "~/Downloads/complete_processed_tcga2stat_RNASeq2_with_clinical.rds" )
    lastClinical <- which( names(allDat) == 'tumorsize' )
    brca <- allDat[ allDat$disease == "BRCA", (lastClinical+1):ncol(allDat) ]
    dim(brca) ## num of genes 20501
    ## compute the median expression! avoid the 0s
    brcaMedian <- apply(brca, 2, median)
    ## retain top (1-myProb)*100% highest expressing genes for illustration
    cutPoint <- quantile( x = brcaMedian, probs = myProb )
    ## genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    brca <- brca[ , genesToKeep ]
    ## ncol(brca) / 20501
    ## print(d <- length(genesToKeep))
    ## save data as rds
    saveRDS(brca, brcaRDS)
}
brca <- readRDS(file = brcaRDS)
d <- ncol(brca)
```

---

```{r ch010-realDataTab, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE}
## Describe and plot real data
if ( !file.exists( '../data/exampleGenes.rds' ) )  {
    set.seed(10192020)
    exampleGenes <- colnames(brca)[sort(sample(ncol(brca), 3))]
    saveRDS(exampleGenes, file = '../data/exampleGenes.rds')
}
exampleGenes <- readRDS('../data/exampleGenes.rds')
small <-  brca %>% select( exampleGenes )
knitr::kable(round(head( small, n = 5)),
      booktabs = TRUE,
      caption = 'mRNA counts for three selected high-expressing genes from the first five observations of the BRCA data set.'
)
```

---

```{r ch010-realDataFig, cache=F, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap="Marginal scatterplots, densities, and estimated pairwise Spearman's correlations for three example genes. The data possess heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology."}
# lowerfun <- function(data,mapping){
#   ggplot(data = data, mapping = mapping)+
#     geom_point()+
#     scale_x_continuous(limits = c(-1.5,1.5))+
#     scale_y_continuous(limits = c(-1.5,1.5))
# }  

if ( !file.exists( '../fig/realDataFig.png' ) ) {
    p <- GGally::ggpairs( data = small,
                                        # lower = list( continuous = wrap(lowerfun)),
                         upper = list(
                             continuous = wrap('cor', method = "spearman")
                         )) + theme_bw()
    ggsave('../fig/realDataFig.png', plot = p, dpi = 300)
}
knitr::include_graphics("../fig/realDataFig.png")
```

## Measures of dependency

- In multivariate analysis, an analyst must select a metric to quantify dependency.
- The most widely-known is the Pearson (product-moment) correlation coefficient that describes the linear association between two random variables $X$ and $Y$, and, it is given by

$$
\rho_P(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ var(X)var(Y)\right]^{1/2}}.
$$

<!-- 
As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector, the Pearson correlation completely describes the dependency between the components. 
For non-normal marginals with monotone correlation patterns, $\rho_P$ suffers some drawbacks and may mislead or fail to capture important relationships (@MK01).
Alternatively in these settings, analysts often prefer rank-based correlation measures to describe the degree of monotonic association.
-->

## Rank-based measures of dependency

Two nonparametric, rank-based measures common in practice are **Spearman's correlation** (denoted $\rho_S$) and **Kendall's $\tau$**. 

- Spearman's:

$$
\rho_S(X,Y) = 3 \left[ P\left[ (X_1 - X_2)(Y_1-Y_3) > 0 \right] - P\left[ (X_1 - X_2)(Y_1-Y_3) < 0 \right] \right],
$$

\noindent where $(X_1, Y_1) \overset{d}{=} (X,Y), X_2 \overset{d}{=} X, Y_3 \overset{d}{=} Y$ with $X_2$ and $Y_3$ are independent of one other and of $(X_1, Y_1)$. 

-  Kendall $\tau$, on the other hand, is the difference in probabilities of concordant and discordant pairs of observations $(X_i, Y_i)$ and $(X_j, Y_j)$.

Both $\tau$ and $\rho_S$ are **invariant to under monotone transformations** of the underlying random variates.

## Correspondence among Pearson, Spearman, $\tau$

- There is no closed form, general correspondence among the rank-based measures and the Pearson correlation coefficient, as the marginal distributions $F_i$ are intrinsic in their calculation.
- But for **bivariate normal vectors**, however, the correspondence is well-known for Kendall's $\tau$:

$$
\rho_{P} = sin \left( \tau \times \frac{\pi}{2} \right), 
$$ 

- and similarly for Spearman's $\rho$ [@K58],

$$
\rho_P = 2 \times sin \left( \rho_S \times \frac{\pi}{6} \right).
$$ 

## NORmal To Anything algorithm

- The well-known NORTA algorithm [@Cario1997] can be used simulate a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$.
- Specifically, the NORTA algorithm follows like this:

1. Simulate a random vector $\bf Z$ with $d$ **independent** and **identical** standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ so that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_{Y_i}^{-1}[\Phi(X_i)], \; i=1,2,...,d$.

- Exact Pearson solutions are possible but require computing $d \choose 2$ double integrals $EY_iY_j = \int \int y_i y_j f_{X|r}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$.
- But even when closed form solutions matching exactly can be impossible (*NORTA-defective* correlation matrices) and the probability of this increases with $d$ @GH02.


## Other `R` packages for simulating random vectors

Here are some other  R packages for multivariate simulation.

* `copula` Highly flexible copula specification [@Yan2007]
* `nortaRA` Implements exact Pearson matching (step 2 in NORTA) [@Chen2001] 
* `Genord` Simulated correlated discrete variables [@BF17]
* `mvnfast` High-performance multivariate normal simulator [@Fasiolo2016]

# 3 - Methodology/Algorithm

## Random vector generation via `bigsimr::rvec` {#rand-vec-gen}

1. Pre-processing for nonparametric dependency matching.  
	(i) Convert from either ${\bf R_{Spearman}}$ or ${\bf R_{Kendall}}$ into the corresponding MVN input correlation ${\bf R_{Pearson}}$.
	(ii) Check that ${\bf R_{Pearson}}$ is semi-positive definite.  
	(iii) If not compute a close semi-positive definite correlation matrix ${\bf \widetilde{R}_{Pearson}}$.  
2. Gaussian copula construction.  
	(i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_{Pearson}})$.  
	(ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$.  
3. Quantile evaluations.  
	(i) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

# 4 - Monte carlo experiments and examples

## 4A - Bivariate studies

```{r ch040-biNegBin, echo = FALSE, eval = TRUE}
size <- 4
prob <- 3e-04
```

- Identical margin, bivariate simulation configurations to provide a low-dimensional evaluation:

| Simulation Reps ($B$) | Correlation Types | Identical-margin 2D distribution |
|-------------|:--------------:|----------------------:|
|$1000$ | Pearson ($\rho_P$) | ${ \bf Y} \sim MVN( \mu= 0 , \sigma = 1, \rho )$ |
|$10,000$ | Spearman ($\rho_S$) | ${ \bf Y} \sim MVG( shape = 10, rate = 1, \rho )$ |
| $100,000$| Kendall ($\tau$)| ${ \bf Y} \sim MVNB(p = `r prob`, r = 4,\rho)$ |

- For each of the unique 9 simulation configurations above, we estimate the correlation bounds and vary $\rho$ along a sequence of 100 points evenly placed within the bounds.

---

```{r ch040-bPlot, cache=F, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, fig.width = 7, fig.height = 6, fig.align='center'}
# https://www.datanovia.com/en/blog/how-to-change-ggplot-facet-labels/
# New facet label names
repsLabs <- paste0("B=", c("1,000", "10,000", "100,000") )
names(repsLabs) <- c(1000, 10000, 100000)
typeLabs <- c( 'Pearson', 'Spearman', 'Kendall' )
names(typeLabs) <- c( 'Pearson', 'Spearman', 'Kendall' )

# Set colors
## RColorBrewer::display.brewer.all()
numColors <- 4
numGroups <- length(levels(allDat$margins))
## myColors <- rev( RColorBrewer::brewer.pal(n = numColors, name = 'Greys')[ ((numColors - numGroups) + 1): numColors  ] )
myColors <- rev( RColorBrewer::brewer.pal(n = numColors, name = 'Blues')[ ((numColors - numGroups) + 1): numColors  ] )

allDat %>%
    ## ggplot(aes(x = rho, y = rho_hat, color = margins, shape = margins)) +
    ggplot(aes(x = rho, y = rho_hat, color = margins)) +
    ## ggplot(aes(rho, rho_hat)) +
    ## geom_point(alpha = 1, size = 2) +
    geom_point(size = 2) +
    ## scale_shape_manual(values=c(1, 2, 3))+
    ## scale_color_manual(values=c('#999999','#E69F00', '#56B4E9')) +
    scale_color_manual(values = myColors ) +
    geom_abline(slope = 1, linetype = 'dashed') +
    labs(x = "Specified Correlation", y = "Estimated Correlation") +
    ## facet_wrap(~ + N) + theme_bw()
    facet_wrap(~ type + N, labeller = labeller(N = repsLabs, type = typeLabs)) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal")

## allDat %>%
##     ggplot(aes(x = rho, y = rho_hat, color = type, shape = type)) +
##     ## ggplot(aes(rho, rho_hat)) +
##     geom_point(alpha = 0.75) +
##     geom_abline(slope = 1) +
##     labs(x = "Specified Correlation", y = "Estimated Correlation") + 
##     ## facet_wrap(~ + N) + theme_bw()
##     facet_wrap(~ margins + N) + theme_bw()
 
## ggsave('fig/plot-biNormPlot.pdf')
```

## 4A - Average absolute error in bivariate studies

```{r ch040-combineBiSims, cache=F, echo=F}
## compare relative differences in MSE for each dependency, compared to Pearson
allDat <- NULL
dat <- readRDS("../results/biNorm_sims.rds") %>% select( margins, type, N, rho, rho_hat  ) 
allDat <- rbind( allDat, dat )
dat <- readRDS("../results/biGamma_sims.rds") %>% select( margins, type, N, rho, rho_hat  )
allDat <- rbind( allDat, dat )
dat <- readRDS("../results/biNB_sims.rds") %>% select( margins, type, N, rho, rho_hat  )
allDat <- rbind( allDat, dat )
allDat$margins <- factor(allDat$margins, levels = c( 'norm', 'gamma', 'nbinom'),
                         labels = c( 'MVN', 'MVG', 'MVNB' ) )
allDat$type <- factor(allDat$type, levels = c( 'pearson', 'spearman', 'kendall' ),
                      labels = c( 'Pearson', 'Spearman', 'Kendall' ) )
allDat$N <- factor(allDat$N, levels = c( 1000, 10000, 100000) )
```

```{r ch040-biMAEtable, cache=F, echo=F, warning=F, message=F}
## tabMSE  <- allDat %>%
##    group_by( N, margins ) %>%
##    summarize( mse = mean( ( rho - rho_hat )^2 ) )
tabMSE  <- allDat %>%
    filter( N == 100000) %>%
    group_by( N, type, margins ) %>%
##     summarize( mse = mean( ( rho - rho_hat )^2 ) ) %>%
    summarize( MAE = mean( abs( rho - rho_hat )  ) )
## normMSE <- as.numeric( tabMSE[ tabMSE$margins == 'norm', 'mse' ] ) 
## ( tabMSE$mse - normMSE ) / normMSE
## ( tabMSE$mse - as.numeric( tabMSE[ tabMSE$margins == 'norm', 'mse' ]) )
## ( tabMSE$mse - as.numeric( tabMSE[ tabMSE$margins == 'norm', 'mse' ]) ) / tabMSE$mse

knitr::kable(tabMSE,
             col.names = c("No. of random vectors",
                           "Correlation type",
                           "Distribution",
                           "Mean abs. error"),
             caption = '',
)
```

## 4B - Simulating High-Dimensional RNA-seq data

```{r ch050-readBRCA, echo=FALSE, eval=TRUE, cache=F}
## full workflow simulating RNA-seq data
brcaRDS <- '../data/brca95.rds'
myProb = 0.95
brca <- readRDS(file = brcaRDS)
d <- ncol(brca)
## smaller set for prototyping
## d=5
## brca <- brca[ , 1:d]
```

First estimate the desired correlation matrix using the fast implementation provided by `bigsimr`:

```{r ch050-estRhoBRCA, echo=TRUE, eval=TRUE, cache=F}
## Estimate Spearman's correlation on the count data
corType <- 'spearman'
system.time( nb_Rho <- bigsimr::cor_fast( brca, method = corType ) )
```
## 4B - Estimate the marginal parameters

- We use method of moments (MoM) to estimate the marginal parameters for the multivariate negative binomial model.
- The marginal distributions are from the same probability family (NB) yet are heterogeneous in terms of the parameters probability and size $(p_i, n_i)$ for $i,\ldots,d$.

```{r ch050-nbHelpers, echo=TRUE}
make_nbinom_alist <- function(sizes, probs) {
  lapply(1:length(sizes), function(i) {
    substitute(qnbinom(size = s, prob = p), 
               list(s = sizes[i], p = probs[i]))
  })
}
## make_nbinom_alist(c(20, 21, 22), c(0.3, 0.4, 0.5))
nbinom_mom <- function(x) {
  m <- mean(x)
  s <- sd(x)
  s2 <- s^2
  p <- m/s2
  r <- m^2 / (s2 - m)
  c(r, p)
}
```

## 4B - Estimate the marginal parameters

```{r ch050-estMargins, echo = TRUE, eval = TRUE, cache=F}
sizes <- apply( unname(as.matrix(brca)), 2, nbinom_mom )[1, ]
probs <- apply( unname(as.matrix(brca)), 2, nbinom_mom )[2, ]
```

- Notably, the marginal NB probabilities $\hat{p}_i's$ are small --- ranging in $[`r min(probs)` , `r max(probs)`]$.
- This gives rise to highly variable counts and, typically, less restriction on potential pairwise correlation pairs.
- Once the functions are defined/executed to complete marginal estimation, we specify targets and generate the desired random vectors using `rvec`.

## 4B - Simulate RNA-seq data via `bigsimr::rvec`

```{r ch050-runBRCA-echo, echo=TRUE, eval=FALSE, cache = FALSE}
## Set the number of random vectors
n <- 10000
## construct margins
nb_margins <- make_nbinom_alist(sizes, probs)
## run sims
sim_nbinom <- rvec(n, nb_Rho, nb_margins, type = corType,
                   ensure_PSD = TRUE, cores = cores) 
```

```{r ch050-runBRCA, echo=FALSE, results=FALSE, cache = FALSE}
nb_margins <- make_nbinom_alist(sizes, probs)
brcaSimRDS <- paste0( "../results/brca", round(myProb*100), "sim.rds" )
sim_nbinom <- readRDS( brcaSimRDS )
```

## 4B -  Results for the three example genes 

```{r ch050-simDataFig, echo=F, fig.align='center', fig.cap='Simulated data for three selected high-expressing genes replicates the estimated data structure.', fig.show = 'hold', out.width='50%'}
knitr::include_graphics("../fig/realDataFig.png")
knitr::include_graphics('../fig/ch050-simDataFig.png')
```


## 4B - Results for all genes

<!-- 
Displayed are the aggregated results of our simulation by comparing the specified target parameter (horizontal axes) with the corresponding quantities estimated from the simulated data (vertical axes).
The evaluation shows that the simulated counts approximately match the target parameters and exhibit the full range of estimated correlation from the data.
Utilizing 15 CPU threads in a MacBook Pro carrying a 2.4 GHz 8-Core Intel Core i9 processor, the simulation completed just over of 2 minutes.
-->

```{r ch050-figBRCA, echo=F, fig.align='center', out.width='60%'}
knitr::include_graphics('../fig/ch050-figBRCA.png')
```

## 4C - Simulation-based joint probability calculations

- To conduct statistical inference a critical task is to evaluate the joint probability mass (or density) function:

$$
P( {\bf Y} = {\bf y} ), y_i \in \chi_i.
$$

- So we may estimate via:

$$
\hat{P}( {\bf Y} >= {\bf y_0 } ) = \sum_{b=1}^B I( {\bf Y^{(b) }} > {\bf y_0 } ) / B
$$

- where ${\bf Y^{(b)} }$ is the $b^{th}$ simulated vector in a total of $B$ simulation replicates and $I ( )$ is the indicator function.

## 4C - Simulation-based joint probability calculations

- For example, we can estimate from our  simulated vectors the probability that all $`r d`$ genes are expressed (i.e., ${\bf y}_i \geq 1, \forall \; i$) :

```{r ch050-densityEvaluation, echo=TRUE, eval=TRUE, cache=F}
d <- ncol(sim_nbinom)
B <- nrow(sim_nbinom)
threshold <- rep( 1, d)
mean(apply( sim_nbinom,  1,
           function(X, y0=threshold) {
               all( X > y0) }
           ))
```

## 4D - Assessing estimation efficiency

-  To assess the error in our correlation estimation above:

```{r ch050-quadlossECHO, echo=T, eval=F, cache=FALSE}
## Number of experiments
m <- 10
## Simulate random vectors equal to the sample size
n <- nrow(brca)
## convert outside for faster simulation
nb_Rho_p <- bigsimr::cor_convert( rho = nb_Rho,
                                 from = corType, to = "pearson" )
## ensure PSD
nb_Rho_p <- bigsimr::cor_nearPSD( G = nb_Rho_p )
## create m random vectors and estimate correlation
simRho <- replicate(n = m,
  expr = { tmpSim <- rvec(n = n , nb_Rho_p, nb_margins,
    type = 'pearson', ensure_PSD = FALSE, cores = cores);
    bigsimr::cor_fast( x = tmpSim, method = corType )} ,
simplify = FALSE)
## find quadratic loss at each rep
quadLoss <- unlist( lapply( simRho, rags2ridges::loss,
                           T = nb_Rho, type = "quadratic"))

```

```{r ch050-quadloss, echo=F, eval=T, cache=FALSE}
quadLossRDS <- paste0( "../results/brca", round(myProb*100), "quadLoss.rds" )
quadLoss <- readRDS( quadLossRDS )
```

## 4D - Assessing estimation efficiency

The `R` summary function supplies the mean-augmented five-number summary of the quadratic loss distribution computed above.

```{r ch050-quadloss-summary, echo=TRUE, eval=T, cache=FALSE}
summary(quadLoss)
```

- This distribution could be compared to high-dimensional designed covariance estimators to guide to help decide whether the additional complexity and computation time are warranted.

## 4E - Scale up to high dimensional RNA-seq data

- Lastly, we provide computation times at increasing $d$ to assess scaling feasibility:

```{r, echo=F, out.width='80%', fig.align='center'}
knitr::include_graphics("../fig/cpu-gpu-times.png")
```

# 5 - Conclusions
	
## Concluding remarks

- Our scaleable NORmal To Anything provides a flexible way to simulate high-dimensional *fake datasets*.
- `bigsimr` is on `Github`: [https://github.com/SchisslerGroup/bigsimr](https://github.com/SchisslerGroup/bigsimr).
- Use `devtools` to install.
- `bigsimr` uses `JAX` python libraries for GPU acceleration and other high
  performance algorithms.

### Pros

* An easy-to-use tool to produce multivariate data of dimension larger than any
  other existing tool.
* Flexible modeling providing highly hetereogenous specification of joint probability models.
* The R `bigsimr`package is a high-performance and GPU-accelerated implementation
  of algorithm and other fast correlation utilities.

### Cons
* Our method requires a well-defined inverse CDF.
* Discrete margins can slightly disrupt the matching algorithm (due the probability of ties). 
* Not all algorithms are currently optimized.

## Future Work

* Investigate *rescaling* the correlations to adjust for discrete margins.
* GPU accelerate the `normal2marginal()`.
* Better handling of non-positive definiteness (fast checks for PD and faster
  implementation/algorithms of nearest PD).
* High-performance implementations for exact Pearson matching.
* Match *partial correlations*.
* Implement user-defined inverse CDFs.
* Implement high-dimensional covariance estimation algorithms.
* Take advantage of sparsity.
* Explore different simulation algorithms.

## Acknowledgements

- Ed
- Walt
- Heather
- Eric Li
- Fred Harris
- CP3 grant
- Co-aothers

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):before {
  background: none;
}
</style>

## References
