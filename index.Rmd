--- 
title: Simulating Ultra High-Dimensional Multivariate Data Using the bigsimr R Package
author:
  - name: Alice Anonymous
    email: alice@example.com
    affiliation: Some Institute of Technology
    footnote: 1
  - name: Bob Security
    email: bob@example.com
    affiliation: Another University
  - name: Cat Memes
    email: cat@example.com
    affiliation: Another University
    footnote: 2
  - name: Derek Zoolander
    email: derek@example.com
    affiliation: Some Institute of Technology
    footnote: 2
address:
  - code: Some Institute of Technology
    address: Department, Street, City, State, Zip
  - code: Another University
    address: Department, Street, City, State, Zip
footnote:
  - code: 1
    text: "Corresponding Author"
  - code: 2
    text: "Equal contribution"
date: "`r format(Sys.Date(), '%A, %B %d, %Y')`"
bibliography:
  - bigsimr.bib
  - packages.bib
site: bookdown::bookdown_site
journal: "Journal of Computational And Applied Mathematics (JCAM)"
#linenumbers: true
#numbersections: true
abstract: |
  In this era of Big Data, it is critical to realistically simulate data to conduct informative Monte Carlo studies. This is problematic when data are inherently multivariate while at the same time are (ultra-) high dimensional. This situation appears frequently in observational data found on online and in high-throughput biomedical experiments (e.g., RNA-sequencing). Due to the difficulty in simulating realistic correlated data points, researchers often resort to simulation designs that posit independence --- greatly diminishing the insight into the empirical operating characteristics of any proposed methodology.  Major challenges lie in the computational complexity involved in simulating these massive random vectors. We propose a fairly general, scalable procedure to simulate high-dimensional multivariate distributions with pre-specified marginal characteristics and dependency characteristics. As a motivating example, we use our methodology to study large-scale statistical inferential procedures applied to cancer-related RNA-sequencing data sets. The proposed algorithm is implemented as the `bigsimr` R package.
---

```{r indexSetup, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(knitr)
library(rmarkdown)
library(bookdown)
## setwd('~/article_bigsimr')
## bookdown::render_book("index.Rmd", "bookdown::pdf_book")
## bookdown::serve_book()
set.seed(09212020)
```

```{r copyBib, echo = FALSE, eval = FALSE, message=FALSE, warning=FALSE, results = "hide"}
system( 'cp ~/bib/bigsimr.bib ./.' )
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{r serveHelper, include=FALSE, eval=FALSE}
## rmote::start_rmote()
## library(ggplot2); qplot(mpg, wt, data=mtcars, colour=cyl)
bookdown::serve_book()
```

```{r countWords, echo = FALSE, eval = FALSE, message=FALSE, warning=FALSE, results = "hide"}
rmdFiles <- dir(pattern = 'Rmd$', ignore.case= T)
## rmdFiles <- rmdFiles[-grep('^_', rmdFiles)]
rmdFiles <- rmdFiles[-grep('references', rmdFiles)]
## rmdFiles <- rmdFiles[-grep('README', rmdFiles)]
## rmdFiles <- rmdFiles[-grep('article', rmdFiles)] ## after generating a single document
## wordcountaddin::word_count( rmdFiles[11] )
(counts <- sapply( rmdFiles, wordcountaddin::word_count, simplify = T ))
sum(counts)
```

# Introduction

Massive high-dimensional data sets are now commonplace in many areas of scientific inquiry. As new methods are developed for these data, a fundamental challenge lies in designing and conducting simulation studies to assess operating characteristics of proposed methodology, such as false positive rates, statistical power, and robustness --- often in comparison to existing methods. Another application lies in the discovering the sampling distribution of test statistic under hypothesized null models in complex scenarios. Such Monte Carlo studies in the high-dimensional setting often become difficult to conduct with existing algorithms and tools, or require expertise in computing on clusters. This is particularly true when simulating massive multivariate, non-normal distributions. 

Along those lines, simulating high-dimensional non-normal data motivates this work --- in pursuit of modeling RNA-sequencing data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. This laboratory procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. Often researchers posit a negative binomial model (refs) as counts are often over (or under) expressed that a Poisson model would suggest. Yet due to inherent biological processes, gene expression data exhibits correlation (coexpression) across genes [@BE07; @Schissler2018]. RNA-sequencing analysis has garnered much interest in the statistical literature (refs). Often researchers simulate independently or from their proposed model in order to conduct Monte Carlo studies (refs). An alternative strategy is to think *generatively* about the data collection process and scientific domain knowledge and design simulations with probabilistic models that reflect those processes. In our motivating example, we'll study the consequences correlation in the simulation study replicating test statistics from an illustrative, classical large-scale hypothesis testing study [see @Efron2004].

## Ultra High Dimensional multivariate modeling and simulation desired properties

With our application in mind and seeking a fairly general-purpose algorithm with broad applications, our purposed methodology should possess the following properties (adapted from criteria from @Nik13a):

* P1: Wide range of dependence, allowing both positive and negative dependence
* P2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* P3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.

Moreover, the simulation method must scale to high dimensions:

* S1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* S2: Procedure must scale to high dimensions while maintaining accuracy.

As others @MB13 have noted, however, simulating dependent data can be challenging. A central issue lies characterizing dependency between components in the high-dimensional random vector. The choice of correlation in typical practice usually relates to the eventual analytic goal and distributional assumptions of the data (e.g. non-normal, discrete, finite support, etc). Here we propose a scheme that ignores the analytic goal --- and, instead, aims to match the empirically-derived estimated correlation data and marginal characteristics. For normal data, the Pearson product-moment correlation describes the dependency completely. As we will see, however, simulating arbitrary random vectors with a given Pearson correlation matrix is computationally intense [@Chen2001, @Xia17], violating property **S1**. On the other hand, an analyst may consider the use of non-parametric correlation measures, such as Spearman's $\rho$ and Kendall's $\tau$, particularly when modeling non-normal data as in our application. Adding to the complexity is the well-known, marginal-dependent constraints on the possible bivariate correlation (a consequence of the bounds of bivariate distribution functions --- the *Frechet-Hoeffding bounds* [@Nelsen2007]). Denote the pointwise upper bound as $M(y_i, y_{i^\prime})$ These bounds give strict restraints on the possible bounds and cannot be overcome through algorithm design. Yet not all multivariate simulation approaches obtain these bounds [for example REF]. Computationally, algorithms that rely on serial computation scale poorly to high dimensions (refs - hierarchy) and fail to make use of modern high-performance computing including parallelization and graphical processing unit acceleration [@Li2019gpu].

To meet the desired properties in the face of the challenging setting, we present a general-purpose, scalable multivariate simulation algorithm. The crux of the method lies in the construction of a Gaussian copula [refs]. The idea that many multivariate and marginally heterogeneous distribution can be constructed in such a manner is well known [refs]. This article's contribution lies in it's application to high-dimensional data through a high-performance implementation (`bigsimr`) and speed-focused algorithm design. The algorithm design relies on useful properties of non-standard correlation measures, namely Kendall's $\tau$ and Spearman's $\rho$. We'll show that quality of simulation does not require the use of the standard Pearson correlation matrix, even for normally distributed data. The purpose of our method is to generate random vectors that match exactly specified any possible Kendall's $\tau$ or Spearman's $\rho$, and approximately Pearson product-moment correlation.

The study proceeds with more details on our motivating example in RNA-sequencing breast cancer data. Then we describe and justify our simulation methodology, followed by extensive Monte Carlo studies under various distributional assumptions --- emphasizing normal and non-normal scenarios. In these studies, we evaluate the accuracy and speed of our approach in comparison to other readily available software. After evaluating *in silico*, we revisit our motivating
example by employing our method and compare our simulations to empirically-derived statistics. Finally, we'll make concluding remarks regarding the method's utility and future directions.

