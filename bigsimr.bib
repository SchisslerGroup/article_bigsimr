Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Schissler2019,
author = {Schissler, Alfred Grant and Aberasturi, Dillon and Kenost, Colleen and Lussier, Yves A.},
doi = {10.3389/fgene.2019.00414},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Schissler et al/Schissler et al. - 2019 - A Single-Subject Method to Detect Pathways Enriched With Alternatively Spliced Genes.pdf:pdf},
issn = {1664-8021},
journal = {Frontiers in Genetics},
keywords = {Alternative splicing,Hellinger distance,Isoform,Local false discovery rate,Pathways,Precision medicine,RNA-Seq,Systems biology},
month = {may},
number = {414},
title = {{A Single-Subject Method to Detect Pathways Enriched With Alternatively Spliced Genes}},
url = {https://www.frontiersin.org/article/10.3389/fgene.2019.00414/full},
volume = {10},
year = {2019}
}
@article{K38,
abstract = {In psychological work the problem of comparing two different rankings of the same set of individuals may be divided into two types. In the first type the individuals have a given order A which is objectively defined with reference to some quality, and a characteristic question is: if an observer ranks the individuals in an order B, does a comparison of B with A suggest that he possesses a reliable judgment of the quality, or, alternatively, is it probable that B could have arisen by chance? In the second type no objective order is given. Two observers consider the individuals and rank them in orders A and B. The question now is, are these orders sufficiently alike to indicate similarity of taste in the observers, or, on the other hand, are A and B incompatible within assigned limits of probability 1 An example of the first type occurs in the familiar experiments wherein an observer has to arrange a known set of weights in ascending order of weight; the second type would arise if two observers had to rank a set of musical compositions in order of preference. The measure of rank correlation proposed in this paper is capable of being applied to both problems, which are, in fact, formally very much the same. For purposes of simplicity in the exposition it has, however, been thought convenient to preserve a distinction between them.},
author = {Kendall, M. G.},
doi = {10.2307/2332226},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {81--93},
title = {{A New Measure of Rank Correlation}},
volume = {30},
year = {1938}
}
@article{Xia17,
abstract = {For the issue of generating correlated random vector containing discrete variables, one major obstacle is to determine a suitable correlation coef- ficient $\rho$z in normal space for a specified correlation coefficient $\rho$x . This paper develops a method to solve this problem. First, the double inte- gral evaluated for $\rho$x is transformed into independent standard uniform space, then, a Quasi Monte Carlo method is introduced to calculate the double integral. For a given $\rho$x , an appropriate $\rho$z is determined by a false position method. Compared with existing methodologies, the proposed method is less efficient, but it is relatively easy to implement.},
author = {Xiao, Qing},
doi = {10.1080/03610926.2015.1024860},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Correlation,Gaussian copula,NORTA algorithm,Quasi Monte Carlo,discrete distribution},
title = {{Generating correlated random vector involving discrete variables}},
year = {2017}
}
@article{Demirtas2011,
abstract = {Correlations among variables are typically not free to vary between -1 and 1, with bounds determined by the marginal distributions. Computing upper and lower limits of correlations given the marginal characteristics often raises theoretical and computational challenges. We propose a simple sorting technique that is predicated upon a little-known consequence of a well-established fact from statistical distribution theory to obtain approximate correlation bounds. This approach works regardless of the data type or distribution. We believe that it has practical value in appropriately specifying the correlation structure in simulation studies. {\textcopyright} 2011 American Statistical Association.},
author = {Demirtas, Hakan and Hedeker, Donald},
doi = {10.1198/tast.2011.10090},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Demirtas, Hedeker/Demirtas, Hedeker - 2011 - A practicalway for computing approximate lower and upper correlation bounds.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Correlation limits,Simulation,Sorting},
number = {2},
pages = {104--109},
title = {{A practicalway for computing approximate lower and upper correlation bounds}},
volume = {65},
year = {2011}
}
@incollection{Nik13a,
address = {Heidelberg},
author = {Nikoloulopoulos, Aristidis K.},
booktitle = {Lecture Notes in Statistics: Copulae in Mathematical and Quantitative Finance},
edition = {213},
pages = {231--249},
publisher = {Springer},
title = {{Copula-based models for multivariate discrete response data}},
year = {2013}
}
@article{Song00,
author = {Song, Peter Xue-kun},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Song/Song - 2000 - Multivariate Dispersion Models Generated from Gaussian Copula.pdf:pdf},
journal = {Scandinavian Journal of Statistics},
keywords = {copula,dependence,dispersion model,general-,generalized estimating equation},
number = {2},
pages = {305--320},
title = {{Multivariate Dispersion Models Generated from Gaussian Copula}},
volume = {27},
year = {2000}
}
@article{Ledoit2004g,
abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
author = {Ledoit, Olivier and Wolf, Michael},
doi = {10.1016/S0047-259X(03)00096-4},
issn = {0047-259X},
journal = {Journal of Multivariate Analysis},
keywords = {Projects:GPU{\_}NORTA},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {feb},
number = {2},
pages = {365--411},
publisher = {Academic Press},
title = {{A well-conditioned estimator for large-dimensional covariance matrices}},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X03000964},
volume = {88},
year = {2004}
}
@misc{Wang2009b,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
booktitle = {Nature Reviews Genetics},
doi = {10.1038/nrg2484},
issn = {14710056},
title = {{RNA-Seq: A revolutionary tool for transcriptomics}},
year = {2009}
}
@article{CCK13,
abstract = {We derive a Gaussian approximation result for the maximum of a sum of high-dimensional random vectors. Specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the Gaussian random vectors with the same covariance matrices as the original vectors. This result applies when the dimension of random vectors ({\$}p{\$}) is large compared to the sample size ({\$}n{\$}); in fact, {\$}p{\$} can be much larger than {\$}n{\$}, without restricting correlations of the coordinates of these vectors. We also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional Gaussian random vectors obtained by multiplying the original vectors with i.i.d. Gaussian multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure. Here too, {\$}p{\$} can be large or even much larger than {\$}n{\$}. These distributional approximations, either Gaussian or conditional Gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. We demonstrate how our Gaussian approximations and the multiplier bootstrap can be used for modern high-dimensional estimation, multiple hypothesis testing, and adaptive specification testing. All these results contain nonasymptotic bounds on approximation errors.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1214/13-AOS1161},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Anti-concentration,Dantzig selector,High dimensionality,Maximum of vector sums,Slepian,Stein method},
title = {{Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors}},
year = {2013}
}
@article{NK10,
abstract = {Multivariate count data occur in several different disciplines. However, existing models do not offer great flexibility for dependence modeling. Models based on copulas nowadays are widely used for continuous data dependence modeling. Modeling count data via copulas is still in its infancy; see the recent article of Genest and Neslehova (2007). A series of different copula models providing various residual dependence structures are considered for vectors of count response variables whose marginal distributions depend on covariates through negative binomial regressions. A real data application related to the number of purchases of different products is provided.},
author = {Nikoloulopoulos, Aristidis K. and Karlis, Dimitris},
doi = {10.1080/03610910903391262},
issn = {03610918},
journal = {Communications in Statistics: Simulation and Computation},
keywords = {Archimedean copulas,Kendall's tau,Market basket count data,Mixtures of max-id copulas,Partially symmetric copulas},
title = {{Modeling multivariate count data using copulas}},
year = {2010}
}
@article{Wu2012b,
abstract = {Competitive gene set tests are commonly used in molecular pathway analysis to test for enrichment of a particular gene annotation category amongst the differential expression results from a microarray experiment. Existing gene set tests that rely on gene permutation are shown here to be extremely sensitive to inter-gene correlation. Several data sets are analyzed to show that inter-gene correlation is non-ignorable even for experiments on homogeneous cell populations using genetically identical model organisms. A new gene set test procedure (CAMERA) is proposed based on the idea of estimating the inter-gene correlation from the data, and using it to adjust the gene set test statistic. An efficient procedure is developed for estimating the inter-gene correlation and characterizing its precision. CAMERA is shown to control the type I error rate correctly regardless of inter-gene correlations, yet retains excellent power for detecting genuine differential expression. Analysis of breast cancer data shows that CAMERA recovers known relationships between tumor subtypes in very convincing terms. CAMERA can be used to analyze specified sets or as a pathway analysis tool using a database of molecular signatures.},
author = {Wu, Di and Smyth, Gordon K.},
doi = {10.1093/nar/gks461},
isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
pmid = {22638577},
title = {{Camera: A competitive gene set test accounting for inter-gene correlation}},
year = {2012}
}
@article{K58,
abstract = {Ordinally invariant, i.e., rank, measures of association for bivariate populations are discussed, with emphasis on the probabilistic and operational interpretations of their population values. The three measures considered at length are the quadrant measure, Kendall's tau, and Spearman's rho. Relationships between these measures are discussed, as are connections between these measures and certain measures of association for cross classifications. Sampling theory is surveyed with special attention to the motivation for sample values of the measures. The historical development of ordinal measures of association is outlined.},
author = {Kruskal, William H.},
doi = {10.1080/01621459.1958.10501481},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Kruskal/Kruskal - 1958 - Ordinal Measures of Association.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {284},
pages = {814--861},
title = {{Ordinal Measures of Association}},
volume = {53},
year = {1958}
}
@article{Li2019gpu,
author = {Li, Xiang and Schissler, A. Grant and Wu, Rui and Barford, Lee and {Harris, Fredrick C.}, Jr.},
chapter = {46},
edition = {800},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Li et al/Li et al. - 2019 - A graphical processing unit accelerated NORmal-To-Anything algorithm for high dimensional multivariate simulation.pdf:pdf},
journal = {Advances in Intelligent Systems and Computing},
pages = {339--346},
title = {{A graphical processing unit accelerated NORmal-To-Anything algorithm for high dimensional multivariate simulation}},
year = {2019}
}
@article{BE07,
abstract = {Large-scale hypothesis testing problems, with hundreds or thousands of test statistics zi to consider at once, have become familiar in current practice. Applications of popular analysis methods, such as false discovery rate techniques, do not require independence of the zi's, but their accuracy can be compromised in high-correlation situations. This article presents computational and theoretical methods for assessing the size and effect of correlation in large-scale testing. A simple theory leads to the identification of a single omnibus measure of correlation for the zi's order statistic. The theory relates to the correct choice of a null distribution for simultaneous significance testing and its effect on inference. Large-scale hypothesis testing problems, with hundreds or thousands of test statistics zi to consider at once, have become familiar in current practice. Applications of popular analysis methods, such as false discovery rate techniques, do not require independence of the zi's, but their accuracy can be compromised in high-correlation situations. This article presents computational and theoretical methods for assessing the size and effect of correlation in large-scale testing. A simple theory leads to the identification of a single omnibus measure of correlation for the zi's order statistic. The theory relates to the correct choice of a null distribution for simultaneous significance testing and its effect on inference.},
author = {Efron, Bradley},
doi = {10.1198/016214506000001211},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Efron/Efron - 2007 - Correlation and large-scale simultaneous significance testing.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Correlated processes,Empirical null,False discovery rate,Microarray},
number = {477},
pages = {93--103},
title = {{Correlation and large-scale simultaneous significance testing}},
volume = {102},
year = {2007}
}
@article{Schissler2018,
author = {Schissler, A Grant and Piegorsch, Walter W and Lussier, Yves A},
doi = {10.1177/0962280217712271},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Schissler, Piegorsch, Lussier/Schissler, Piegorsch, Lussier - 2018 - Testing for differentially expressed genetic pathways with single-subject N-of-1 data in the pres.pdf:pdf},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
keywords = {affinity propagation clustering,exemplar learning,gene expression data,gene set,inter-gene correlation,n-of-1,precision medicine,rna-seq,single-subject inference,triple negative breast cancer},
number = {12},
pages = {3797--3813},
title = {{Testing for differentially expressed genetic pathways with single-subject N-of-1 data in the presence of inter-gene correlation}},
url = {http://journals.sagepub.com/doi/10.1177/0962280217712271},
volume = {27},
year = {2018}
}
@article{Chen2001,
author = {Chen, Huifen},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Chen/Chen - 2001 - Initialization for NORTA Generation of Random Vectors with Specified Marginals and Correlations.pdf:pdf},
journal = {INFORMS Journal on Computing},
number = {4},
pages = {312--331},
title = {{Initialization for NORTA: Generation of Random Vectors with Specified Marginals and Correlations}},
volume = {13},
year = {2001}
}
@techreport{Cario1997g,
abstract = {We describe a model for representing random vectors whose component random variables have arbitrary marginal distributions and correlation matrix, and describe how to generate data based upon this model for use in a stochastic simulation. The central idea is to transform a multivariate normal random vector into the desired random vector, so we refer to these vectors as having a NORTA (NORmal To Anything) distribution. NORTA vectors are most useful when the marginal distributions of the component random variables are neither identical nor from the same family of distributions, and they are particularly valuable when the dimension of the random vector is greater than two. Several numerical examples are provided.},
author = {Cario, Marne C. and Nelson, Barry L.},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Cario, Nelson/Cario, Nelson - 1997 - Modeling and generating random vectors with arbitrary marginal distributions and correlation matrix.pdf:pdf},
keywords = {Projects:GPU{\_}NORTA,copulas,correlation matrix,input modeling,random vector,simulation},
mendeley-tags = {Projects:GPU{\_}NORTA},
pages = {1--19},
publisher = {Citeseer},
title = {{Modeling and generating random vectors with arbitrary marginal distributions and correlation matrix}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.281{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@article{Smith2020,
abstract = {Variational methods are attractive for computing Bayesian inference for highly parametrized models and large datasets where exact inference is impractical. They approximate a target distribution - either the posterior or an augmented posterior - using a simpler distribution that is selected to balance accuracy with computational feasibility. Here we approximate an element-wise parametric transformation of the target distribution as multivariate Gaussian or skew-normal. Approximations of this kind are implicit copula models for the original parameters, with a Gaussian or skew-normal copula function and flexible parametric margins. A key observation is that their adoption can improve the accuracy of variational inference in high dimensions at limited or no additional computational cost. We consider the Yeo-Johnson and G{\&}H transformations, along with sparse factor structures for the scale matrix of the Gaussian or skew-normal. We also show how to implement efficient reparametrization gradient methods for these copula-based approximations. The efficacy of the approach is illustrated by computing posterior inference for three different models using six real datasets. In each case, we show that our proposed copula model distributions are more accurate variational approximations than Gaussian or skew-normal distributions, but at only a minor or no increase in computational cost.},
archivePrefix = {arXiv},
arxivId = {1904.07495},
author = {Smith, Michael Stanley and Loaiza-Maya, Rub{\'{e}}n and Nott, David J.},
doi = {10.1080/10618600.2020.1740097},
eprint = {1904.07495},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
title = {{High-dimensional Copula Variational Approximation through Transformation}},
year = {2020}
}
@article{Efron2004,
abstract = {Current scientific techniques in genomics and image processing routinely produce hypothesis testing problems with hundreds or thousands of cases to consider simultaneously. This poses new difficulties for the statistician, but also opens new opportunities. In particular, it allows empirical estimation of an appropriate null hypothesis. The empirical null may be considerably more dispersed than the usual theoretical null distribution that would be used for any one case considered separately. An empirical Bayes analysis plan for this situation is developed, using a local version of the false discovery rate to examine the inference issues. Two genomics problems are used as examples to show the importance of correctly choosing the null hypothesis.},
author = {Efron, Bradley},
doi = {10.1198/016214504000000089},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Empirical bayes,Empirical null hypothesis,Local false discovery rate,Microarray analysis,Projects:N1PAS,Unobserved covariates},
mendeley-tags = {Projects:N1PAS},
title = {{Large-scale simultaneous hypothesis testing: The choice of a null hypothesis}},
year = {2004}
}
@book{MK01,
author = {Mari, Dominique Drouet and Kotz, Samuel},
isbn = {1860942644},
publisher = {World Scientific},
title = {{Correlation and dependence}},
year = {2001}
}
@article{Tchen1980,
author = {Tchen, Andre H.},
doi = {10.1214/aop/1176994668},
issn = {0091-1798},
journal = {The Annals of Probability},
keywords = {tchen80},
number = {4},
pages = {814--827},
title = {{Inequalities for Distributions with Given Marginals}},
url = {http://projecteuclid.org/euclid.aop/1176996548},
volume = {8},
year = {1980}
}
@article{Zhao2018,
author = {Zhao, Lili and Wu, Weisheng and Feng, Dai and Jiang, Hui and Nguyen, XuanLong},
doi = {10.1214/17-BA1055},
issn = {1936-0975},
journal = {Bayesian Analysis},
number = {2},
pages = {411--436},
title = {{Bayesian Analysis of RNA-Seq Data Using a Family of Negative Binomial Models}},
url = {https://projecteuclid.org/euclid.ba/1491616976},
volume = {13},
year = {2018}
}
@article{Won2013g,
author = {Won, Joong-Ho and Lim, Johan and Kim, Seung-Jean and Rajaratnam, Bala},
doi = {10.1111/j.1467-9868.2012.01049.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Condition number,Convex optimization,Covariance estimation,Cross‐validation,Eigenvalue,Portfolio optimization,Projects:GPU{\_}NORTA,Regularization,Risk comparisons,Shrinkage},
mendeley-tags = {Projects:GPU{\_}NORTA},
month = {jun},
number = {3},
pages = {427--450},
publisher = {Blackwell Publishing Ltd},
title = {{Condition-number-regularized covariance estimation}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2012.01049.x},
volume = {75},
year = {2013}
}
@article{Bouezmarni2009,
abstract = {We study Kendall's tau and Spearman's rho concordance measures for discrete variables. We mainly provide their best bounds using positive dependence properties. These bounds are difficult to write down explicitly in general. Here, we give the explicit formula of the best bounds in a particular Fr{\'{e}}chet space in order to understand the behavior of the ranges of these measures. Also, based on the empirical copula which is viewed as a discrete distribution, we propose a new estimator of the copula function. Finally, we give useful dependence properties of the bivariate Poisson distribution and show the relationship between parameters of the Poisson distribution and both tau and rho. {\textcopyright} 2009 Taoufik Bouezmarni et al.},
author = {Bouezmarni, Taoufik and Mesfioui, Mhamed and Tajar, Abdelouahid},
doi = {10.1155/2009/895742},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Bouezmarni, Mesfioui, Tajar/Bouezmarni, Mesfioui, Tajar - 2009 - On concordance measures for discrete data and dependence properties of poisson model.pdf:pdf},
issn = {1687952X},
journal = {Journal of Probability and Statistics},
title = {{On concordance measures for discrete data and dependence properties of poisson model}},
volume = {2009},
year = {2009}
}
@article{XZ18,
author = {Xiao, Qing and Zhou, Shaowu},
doi = {10.1080/03610926.2018.1439962},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Gaussian copula,Mehler's formula.,continuous variables,correlation coefficient,discrete variables},
title = {{Matching a correlation coefficient by a Gaussian copula}},
year = {2018}
}
@article{LX19,
abstract = {ABSTRACTIt is of fundamental interest in statistics to test the significance of a set of covariates. For example, in genome-wide association studies, a joint null hypothesis of no genetic effect is tested for a set of multiple genetic variants. The minimum p-value method, higher criticism, and Berk–Jones tests are particularly effective when the covariates with nonzero effects are sparse. However, the correlations among covariates and the nonGaussian distribution of the response pose a great challenge toward the p-value calculation of the three tests. In practice, permutation is commonly used to obtain accurate p-values, but it is computationally very intensive, especially when we need to conduct a large amount of hypothesis testing. In this paper, we propose a Gaussian approximation method based on a Monte Carlo scheme, which is computationally more efficient than permutation while still achieving similar accuracy. We derive nonasymptotic approximation error bounds that could vanish in the limit even if ...},
author = {Liu, Yaowu and Xie, Jun},
doi = {10.1080/01621459.2017.1407776},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Berk–Jones test,Genome-wide association study,High dimensionality,Higher criticism,Monte Carlo method},
title = {{Accurate and Efficient P-value Calculation Via Gaussian Approximation: A Novel Monte-Carlo Method}},
year = {2019}
}
@article{BF17,
abstract = {A package for the stochastic simulation of discrete variables with assigned marginal distributions and correlation matrix is presented and discussed. The simulating mechanism relies upon the Gaussian copula, linking the discrete distributions together, and an iterative scheme recovering the correlation matrix for the copula that ensures the desired correlations among the discrete variables. Examples of its use are provided as well as three possible applications (related to probability, sampling, and inference), which illustrate the utility of the package as an efficient and easy-to-use tool both in statistical research and for didactic purposes.},
author = {Barbiero, Alessandro and Ferrari, Pier Alda},
doi = {10.1080/03610918.2016.1146758},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
keywords = {Correlation matrix,Gaussian copula,Multivariate discrete distribution},
month = {aug},
number = {7},
pages = {5123--5140},
title = {{An R package for the simulation of correlated discrete variables}},
url = {https://www.tandfonline.com/doi/full/10.1080/03610918.2016.1146758},
volume = {46},
year = {2017}
}
@article{Nelsen1987,
abstract = {Using convex linear combinations of the probability functions for the discrete Fr{\'{e}}chet boundary distributions and the probability function for independent random variables, we construct bivariate probability functions for dependent discrete random variables with arbitrary marginals and any correlation between the theoretical minimum and maximum. The technique is elementary and the results can be adapted readily for simulation. {\textcopyright} 1987, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Nelsen, Roger B.},
doi = {10.1080/03610918708812585},
file = {:Users/alfred/OneDrive - University of Nevada, Reno/Mendeley{\_}library/Nelsen/Nelsen - 1987 - Discrete bivariate distributions with given marginals and correlation.pdf:pdf},
isbn = {0361091870881},
issn = {0361-0918},
journal = {Communications in Statistics-Simulation and Computation},
keywords = {bivariate Poisson distribution,dependent random variables,discrete Fr{\'{e}}chet bounds,simulation},
number = {1},
pages = {199--208},
publisher = {Taylor {\&} Francis},
title = {{Discrete bivariate distributions with given marginals and correlation}},
volume = {16},
year = {1987}
}
@misc{Conesa2016b,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Michal Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
booktitle = {Genome Biology},
doi = {10.1186/s13059-016-0881-8},
issn = {1474760X},
title = {{A survey of best practices for RNA-seq data analysis}},
year = {2016}
}
@article{Nik13b,
abstract = {The continuous extension of a discrete random variable is amongst the computational methods used for estimation of multivariate normal copula-based models with discrete margins. Its advantage is that the likelihood can be derived conveniently under the theory for copula models with continuous margins, but there has not been a clear analysis of the adequacy of this method. We investigate the asymptotic and small-sample efficiency of two variants of the method for estimating the multivariate normal copula with univariate binary, Poisson, and negative binomial regressions, and show that they lead to biased estimates for the latent correlations, and the univariate marginal parameters that are not regression coefficients. We implement a maximum simulated likelihood method, which is based on evaluating the multidimensional integrals of the likelihood with randomized quasi-Monte Carlo methods. Asymptotic and small-sample efficiency calculations show that our method is nearly as efficient as maximum likelihood for fully specified multivariate normal copula-based models. An illustrative example is given to show the use of our simulated likelihood method. {\textcopyright} 2013 Elsevier B.V.},
author = {Nikoloulopoulos, Aristidis K.},
doi = {10.1016/j.jspi.2013.06.015},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Continuous extension,Jitters,Multivariate normal copula,Rectangle probabilities,Simulated likelihood},
title = {{On the estimation of normal copula discrete regression models using the continuous extension and simulated likelihood}},
year = {2013}
}
@article{MB13,
abstract = {This article describes a method for simulating n-dimensional multivariate non-normal data, with emphasis on count-valued data. Dependence is characterized by either Pearson correlations or Spearman correlations. The simulation is accomplished by simulating a vector of correlated standard normal variates. The elements of this vector are then transformed to achieve the target marginal distributions. We prove that the method corresponds to simulating data from a multivariate Gaussian copula. The simulation method does not restrict pairwise dependence beyond the limits imposed by the marginal distributions and can achieve any Pearson or Spearman correlation within those limits. Two examples are included. In the first example, marginal means, variances, Pearson correlations, and Spearman correlations are estimated from the epileptic seizure data set of Diggle et al. [P. Diggle, P. Heagerty, K.Y. Liang, and S. Zeger, Analysis of Longitudinal Data, Oxford University Press, Oxford, 2002]. Data with these means an...},
author = {Madsen, L. and Birkes, D.},
doi = {10.1080/00949655.2011.632774},
issn = {00949655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Pearson correlation,Spearman correlation,count data,rank correlation},
title = {{Simulating dependent discrete data}},
year = {2013}
}
@article{MD07,
abstract = {In this study we compare two techniques for simulating count-valued random n-vectors Y with specified mean and correlation structure. The first technique is to use a lognormal-Poisson hierarchy (L-P method). A vector of correlated normals Z is generated and transformed to a vector of lognormals X. Then, Y is generated as conditionally independent Poissons with means X-i. The L-P method is simple, fast, and familiar to many researchers. However, the method requires each Y-i to be overdispersed (i.e., sigma(2) {\textgreater} mu), and only low correlations are possible with this method when the variables have small means. We develop a second technique to generate the elements of Y as overlapping sums (OS) of independent X-j's (OS method). For example, suppose X, X-1, and X-2 are independent. If Y-1 = X + X-1 and Y-2 = X + X-2, then Y-1 and Y-2 are correlated because they share the common component X. A generalized version of the OS method for simulating n-vectors of two-Parameter count-valued distributions is presented. The OS method is shown to address some of the shortcomings of the L-P method. In particular, underdispersed random variables can be simulated, and high correlations are feasible even when the means are small. However, negative correlations cannot be simulated with the OS method, and when n {\textgreater} 3, the OS method is more complicated to implement than the L-P method.},
author = {Madsen, L. and Dalthorp, D.},
doi = {10.1007/s10651-007-0008-1},
issn = {13528505},
journal = {Environmental and Ecological Statistics},
keywords = {Generate correlated discrete,Lognormal-Poisson,Negative binomial,Spatial dynamics,Spatial ecology,Taylor's power law},
title = {{Simulating correlated count data}},
year = {2007}
}
@book{Nelsen2007,
address = {New York},
author = {Nelsen, Roger B.},
edition = {2},
isbn = {9781475719062},
publisher = {Springer Science {\&} Business Media},
title = {{An Introduction to copulas}},
year = {2007}
}
