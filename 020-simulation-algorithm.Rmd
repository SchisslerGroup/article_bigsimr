# Algorithms {#algorithms}

```{r ch020-preview, echo = FALSE, eval = FALSE}
bookdown::preview_chapter('020-simulation-algorithm.Rmd')
```

This section describes our methods involved in simulating a random vector $\bf Y$ with $Y_i$ components for $i=1,2,\ldots,d$.
Each $Y_i$ has a specified marginal CDF $F_i$ and its inverse $F^{-1}_i$.
To characterize dependency, every pair $(Y_i, Y_j)$ has a given Pearson correlation $\rho_P$, Spearman correlation $\rho_S$, and/or Kendall's $\tau$.
The method is best understand as a **high-performance Gaussian copula** (Equation \@ref(eq:gauss)) providing a high-dimensional NORTA-inspired algorithm.

## NORmal To Anything (NORTA)

The well-known NORTA algorithm [@Cario1997] can be used simulate a random vector $\bf Y$ with variance-covariance matrix $\Sigma_{\bf Y}$.
Specifically, the NORTA algorithm follows like this:

1. Simulate a random vector $\bf Z$ with $d$ **independent** and **identical** standard normal components.
2. Determine the input matrix $\Sigma_{\bf Z}$ that corresponds with the specified output $\Sigma_{\bf Y}$.
3. Produce a Cholesky factor $M$ of $\Sigma_{\bf Z}$ so that $M M^{\prime}=\Sigma_{\bf Z}$.
4. Set $X$ by $X \gets MZ$.
5. $\text{Return} \; Y \; \text{where} \; Y_i \gets F_{Y_i}^{-1}[\Phi(X_i)], \; i=1,2,...,d$.

With modern parallelized computing, steps 1, 3, 4, 5 are readily implemented as high-performance, multicore and/or graphical-processing-unit (GPU) accelerated, algorithms --- providing the fast scaleability using readily-available hardware.

Matching specified Pearson correlation coefficients exactly (step 2 above), however, is problematic.
In general, there is no closed form correspondence between the components of the input $\Sigma_{\bf Z}$ and target $\Sigma_{\bf Y}$.
Matching the correlations involves evaluating or approximating $\binom{d}{2}$ integrals of the form $EY_iY_j = \int \int y_i y_j f_{X}(F_i^{-1}(\Phi(z_i)), F_j^{-1}(\Phi(z_j))dy_idy_j$, for $i,j=1,2,\ldots,d, \; i \neq j$.
For high-dimensional data, these evaluations are often too costly to enable feasible simulation studies.
For low-dimensional problems, methods and tools exist to match Pearson correlations precisely (see @Chen2001; @Xia17; @MB13), including the publicly available `nortaRA` R package.

To maintain scaleability (SP1), our solution is to essentially avoid this complication in Pearson matching.
Since our goal is to simulate non-normal marginals, we greatly prefer the use of rank-based measures $\rho_S$ and $\tau$ from a modeling standpoint.
Further, $\rho_S$ and $\tau$'s invariance under monotone transformation (see [Background](#background)), perserves the correlation coefficients through steps 3, 4, and 5 in the NORTA algorithm above.
This eliminates the need for computing the $\binom{d}{2}$ integrals to match exactly (nothing is for free, however, as discussed in below in Section \@ref(rand-vec-gen)).

Despite all this, if one does desire to characterize dependency using Pearson correlations, simply using the target Pearson correlation matrix as the initial conditions to our proposed algorithm will lead to approximate matching in the resultant distribution (@Song00) in many practical applications.
The quality of this approximation depends on the setting, but in practice, for high-dimensional count data we find the accuracy to be adequate.
Later, we'll study the robustness of our method to this limitation in selected [Monte Carlo evalutations]({#simulations}).

## Random vector generation via `bigsimr::rvec` {#rand-vec-gen}

Now we describe `bigsimr::rvec`, our algorithm to generate random vectors.
It mirrors the classical NORTA algorithm above with some modifications for rank-based dependency matching:

1. Pre-processing for nonparameteric dependency matching.  
	(i) Convert from either ${\bf R_{Spearman}}$ or ${\bf R_{Kendall}}$ into the corresponding MVN input correlation ${\bf R_{Pearson}}$.
	(ii) Check that ${\bf R_{Pearson}}$ is semi-positive definite.  
	(iii) If not compute a close semi-positive definite correlation matrix ${\bf \widetilde{R}_{Pearson}}$.  
2. Gaussian copula construction.  
	(i) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R_{Pearson}})$.  
	(ii) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$.  
3. Quantile evaluations.  
	(i) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

The pre-processing (Step1) takes advantage of the closed-form relationships between $\rho_S$ and $\tau$ with $\rho_P$ for bivariate normal random variables via Equations \@ref(eq:convertSpearman) or \@ref(eq:convertKendall), respectively (implemented as `bigsimr::cor_covert`).

A complication often arises at this stage: the parallelized, pairwise conversions may create a (spuriously) non-positive definite correlation matrix, especially in high dimensions.
Researchers working in multivariate computation frequently encounter such difficulties and need to find a close positive (semi-)definiteness matrix.
The most widely-available routine for this task in `R` is called `matrix::nearPD` and is not suitable for high dimensions.
To overcome this issue, we've developed `bigsimr::nearPSD`, a quadractically-convergent Netwon method for finding the nearest correlation matrix, developed by @QS2006.
We hope that this routine could be useful in many applications aside from our primary goal of random vector generation.

Once the target margins and algorithm inputs are determined, steps 2 and 3 are essentially a NORTA algorithm with modern high-performance computing implementations.
Specifically, step 2i uses either an efficient multicore multivariate normal simulator (the `R` package `mvnfast`[@Fasiolo2016]) or a using Google's JAX python library NumPy for graphical-processing-unit (GPU) acceleration of the Cholesky factorization and matrix multiplication (steps 3, 4 in the NORTA algorithm in the preceeding section).
(note that JAX is a not acronym and can be thought of a high-performance NumPy).

<!-- 
## Other high-performance `bigsimr` algorithms

<mark> ALEX, briefly discuss other algorithms here </mark>
- `cor_bounds()`  
- `cor_fast()`  
- `cor_nearPSD()`  

For discrete marginals, achieving a target Spearman correlation under this scheme is possible by using components from Equation \@ref(eq:spearmanRescaled) to further adjust the input correlation matrix. Let the unscaled Spearman correlation coefficients be $\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)$ for two marginal distributions and divide the target correlation by the product in the denominator of Equation \@ref(eq:spearmanRescaled). Let these adjustment factors be denoted as $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ and specifically rescale the target Spearman correlation matrix by

\begin{equation}
(\#eq:convertSpearmanDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

In a similar fashion, we rescale Kendall's $\tau$ to adjust the input correlation matrix. The conversion formula is given by

\begin{equation}
(\#eq:convertKendallDiscrete)
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

your comment -->
