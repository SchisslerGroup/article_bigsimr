# Background and notation {#background}

The article presents several algorithms to work with high-dimesional multivariate data, but all `bigsimr` algorithms were originally designed to support a single task:
to generate random vectors drawn from given marginal distributions and component-wise dependency metrics.
Specifically, our goal is to efficiently simulate a large number, $B$, of random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with **correlated** components and hetereogeneous marginal distributions, where $d$ can be very large (possibly millions of variables).

When designing this methodology, we developed the following properties to guide our effort.
We divide the properties into two categories: (1) basic properties (**BP**) and "scaleability" properties (**SP**). 
The BPs are adapted from an existing criteria due to @Nik13a.
Our simulation strategy should allow:

* BP1: Wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.

Moreover, the simulation method must **scale** to high dimensions:

* SP1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.

To fix ideas and provide example applications enabled via `bigsimr`, we the next section describes our motivating data that originally inspired the authors` interest in creating this methodology.

## Motivating example: RNA-seq data from breast cancer tumor samples

Simulating high-dimensional non-normal data motivates this work --- in pursuit of modeling RNA-sequencing data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. 
This laboratory procedure results in count data with infinite support, since RNA-sequencing platforms measure gene expression by enumerating the number of reads aligned to genomic regions. 
Often researchers posit a negative-binomial (NB) model as counts are often over-dispersed that a Poisson model would suggest.
Yet due to inherent biological processes, gene expression data exhibits correlation (co-expression) across genes [@BE07; @Schissler2018]. 

```{r ch010-processBRCA, echo=FALSE, eval=TRUE, results='none'}
if ( !file.exists( 'data/brca99.rds' ) )  {
    ## full workflow simulating RNA-seq data
    allDat <- readRDS( file = "~/Downloads/complete_processed_tcga2stat_RNASeq2_with_clinical.rds" )
    lastClinical <- which( names(allDat) == 'tumorsize' )
    brca <- allDat[ allDat$disease == "BRCA", (lastClinical+1):ncol(allDat) ]
    dim(brca) ## num of genes 20501
    ## compute the median expression! avoid the 0s
    brcaMedian <- apply(brca, 2, median)
    ## retain top (1-probs)*100% highest expressing genes for illustration
    myProb <- 0.99
    cutPoint <- quantile( x = brcaMedian, probs = myProb )
    ## genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    brca <- brca[ , genesToKeep ]
    ## ncol(brca) / 20501
    print(d <- length(genesToKeep))
    ## save data as rds
    saveRDS(brca, 'data/brca99.rds')
}
```

```{r ch010-realDataTabl, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE}
## Describe and plot real data
brca <- readRDS(file = './data/brca99.rds')
set.seed(10192020)
exampleGenes <- colnames(brca)[sort(sample(ncol(brca), 3))]
saveRDS(exampleGenes, file = './data/exampleGenes.rds')
kable(round(brca[1:4, exampleGenes]))
## GGally::ggpairs(data = as.data.frame(brca[ ,exampleGenes] ))
```

Figure X.

```{r ch010-realDataFig, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap='Real data for the 3 selected high-expressing genes'}
## Describe and plot real data
GGally::ggpairs(data = as.data.frame(brca[ ,exampleGenes] ))
```

## Measures of dependency

The population correlation coefficient, commonly called the Pearson (product-moment) correlation coefficient, describes the linear association between two random variables $X$ and $Y$ and is given by

\begin{equation}
(\#eq:pearson)
\rho(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ var(X)var(Y)\right]^{1/2}} 
\end{equation}

As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector the Pearson correlation adequately describes the dependency between the components. $\rho$'s utility in non-normal or non-linear associations is lacking (@MK01). Rank-based (ordinal) approaches performance better in these settings, such as Spearman's (denoted $\rho_s$) and Kendall's $\tau$. Define 

\begin{equation}
(\#eq:spearman)
\rho_s(X,Y) = 3 \left[ P\left[ (X_1 - X_2)(Y_1-Y_3) > 0 \right] - P\left[ (X_1 - X_2)(Y_1-Y_3) < 0 \right] \right]
\end{equation}

\noindent where $(X_1, Y_1) \overset{d}{=} (X,Y), X_2 \overset{d}{=} X, Y_3 \overset{d}{=} Y$ with $X_2$ and $Y_3$ are independent of one other and of $(X_1, Y_1)$. For continuous marginals, the measure works well due to zero probability of ties. 

To do this, we take advantage of a closed form relationship [ref?] between Kendall's $\tau$ and Pearson's correlation coefficient for bivariate normal random variables:

\begin{equation}
(\#eq:convertKendall)
r_{Pearson} = sin \left( \tau_{Kendall} \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's $\rho$ [@K58],

\begin{equation}
(\#eq:convertSpearman)
\rho_{Pearson} = 2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right).
\end{equation}

*Marginal-dependent bivariate correlation bounds*. 
Adding to the complexity is the well-known, marginal-dependent constraints on the possible bivariate correlation.
This is a consequence of the bounds of bivariate distribution functions --- the *Frechet-Hoeffding bounds* [@Nelsen2007]). 
Denote the pointwise upper bound as $M(y_i, y_{i^\prime})$ These bounds give strict restraints on the possible bounds and cannot be overcome through algorithm design. 
Yet not all multivariate simulation approaches obtain these bounds [for example REF].
Computationally, algorithms that rely on serial computation scale poorly to high dimensions (refs - hierarchy) and fail to make use of modern high-performance computing including parallelization and graphical processing unit acceleration [@Li2019gpu].

The pairwise correlation between two correlated random variables cannot in general obtain the full range of possible values, $[-1,-1]$. The range, called the Frechet(-Hoeffding) bounds, is a well-known function of the marginal distributions and are given by [@BF17]:

\begin{equation}
(\#eq:frechet)
\rho^{max} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho^{min} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where $U$ is a uniform random variable in $(0,1)$, and $F^{-1}_1, F^{-1}_2$ are the inverse cdf of random variables $X_1$ and $X_2$, respectively. For discrete random variables, define $F^{-1}$ as in Equation \@ref(eq:inverseCDF).

For discrete marginals, however, one could perform a rescaled version of $\rho_s$ for random variables $X,Y$ with pmfs (or pdfs) $p(x)$ and $q(y)$, respectively. 

\begin{equation}
(\#eq:spearmanRescaled)
\rho_{RS}(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}
\end{equation}

## Gaussian copulas

The crux of the method is construction of a Gaussian copula. This idea is well known [refs]. We chose a Gaussian copula among the many available copula functions primarily for two reasons: 1) the computationally convient closed expression to convert among the most common dependency measures when the margins are Gaussian and 2) the availability of high-performance multivariate normal simulators in R ('mvnfast' ref).  

A copula is a distribution function on $[0,1]^d$ describing a random vector with standard uniform marginals. Moreover, for any random vector ${\bf X}=(X_1, \ldots, X_d)$ with cumulative distribution function (CDF) $F$ and marginal CDFs $F_i$ there is a copula function 
$C(u_1, \ldots, u_d)$ so that 

\[
F(x_1, \ldots,x_d) = \mathbb P(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), \,\,\, x_i\in \mathbb R, i=1,\ldots,d. 
\]  

A Gaussian copula is the case where all marginal CDFs $F_i$ are the standard normal cdf, $\Phi$. The Gaussian copula is the one that corresponds to a multivariate normal distribution with standard normal marginal distributions and covariance matrix ${\bf R}$. (Since the marginals are standard normal, this ${\bf R}$ is also the correlation matrix). If $F_{{\bf R}}$ is the CDF of such multivariate normal distribution, then the corresponding Gaussian copula $C_{{\bf R}}$ is defined through

\begin{equation}
(\#eq:gauss)
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

where $\Phi(\cdot)$ is the standard normal CDF. Note that the copula $C_{{\bf R}}$ is simply the CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where  $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$. 

Sklar's Theorem (ref) guarantee's that any random vector with computational feasible inverse CDFs (P4 above) and obtainable correlation matrix (within the Frechet bounds) can be obtained via transformations involving copula functions. Namely, to simulate a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$, we can construct a Gaussian copula ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$. When an $Y_i$ is discrete, some care must be taken to define $F_i$. Letting 

\begin{equation}
F_{i}^{-1} = inf\{y:F_{i}(y) \geq u \}
(\#eq:inverseCDF)
\end{equation}

\noindent ensures that $Y_i \sim F_i$.

Simulation of a general multivariate random vector ${\bf T}$ with margins $F_i$ based on this copula is quite simple. Ensuring that one obtains a certain dependence among the marginal distributions, however, proves challenging in our proposed Gaussian-copula-based scheme. Two core issues: 1) marginal characteristics induce bounds on the possible bivariate correlations and 2) monotone transformations deform the Pearson correlation which no closed form expression exists in general (ref? Chen2001). The Frechet bounds are well-known.
