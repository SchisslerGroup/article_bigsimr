# Background and notation {#background}

The article presents several algorithms to work with high-dimesional multivariate data, but all `bigsimr` algorithms were originally designed to support a single task:
to generate random vectors drawn from given marginal distributions and component-wise dependency metrics.
Specifically, our goal is to efficiently simulate a large number, $B$, of random vectors ${\bf Y}=(Y_1, \ldots, Y_d)^\top$ with **correlated** components and hetereogeneous marginal distributions, where $d$ can be very large (possibly millions of variables).

When designing this methodology, we developed the following properties to guide our effort.
We divide the properties into two categories: (1) basic properties (**BP**) and "scaleability" properties (**SP**). 
The BPs are adapted from an existing criteria due to @Nik13a.
Our simulation strategy should allow:

* BP1: Wide range of dependences, allowing both positive and negative values, and, ideally, admitting the full range of possible values.
* BP2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* BP3: Flexible marginal modeling, generating heterogeneous data --- possibly from differing probability families.

Moreover, the simulation method must **scale** to high dimensions:

* SP1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* SP2: Procedure must scale to high dimensions while maintaining accuracy.

To fix ideas and provide example applications enabled via `bigsimr`, we the next section describes our motivating data that originally inspired the authors` interest in creating this methodology.

## Motivating example: RNA-seq data

Simulating high-dimensional, non-normal data motivates this work --- in pursuit of modeling RNA-sequencing data [@Wang2009b; @Conesa2016b] derived from breast cancer patients. 
The RNA-seq data-generating process involves counting how often a particular messenger RNA (mRNA) is expressed in a biological sample.
RNA-seq platforms typically quantify the entire transcriptome in one experimental run, resulting in high-dimensional data.
In humans, this results in count data corresponding to over 20,000 genes (coding genomic regions) or even over 77,000 isoforms when alternating spliced mRNA are counted.
Yet due to inherent biological processes, gene expression data exhibits correlation (co-expression) across genes [@BE07; @Schissler2018]. 

Specifically, we'll illustrate our methodology using the Breast Invasive Carcinoma (BRCA) data set housed in The Cancer Genome Atlas; TCGA; see Acknowledgements).
For simplicity, we begin by filtering to retain the top `r (1-myProb)*100`% highest expressing genes of the 20,501 gene measurements from $N=`r nrow(brca)`$ patients' tumor samples, resulting in $d=`r d`$ genes.
Further, to illustrate our methodology's flexible/robust simulation scheme and better align with the actual data-generating process, we remove the statistical adjustment for read alignment ambiguity (RNA-seq by Expectation Maximization; RSEM; @Li2011c) by simple rounding to integer values.
Table \@ref(tab:ch010-realDataTab) displays counts for three selected genes for the first 5 patients' breast tumor samples. 

```{r ch010-processBRCA, echo=FALSE, eval=TRUE, results='none'}
if ( !file.exists( 'data/brca99.rds' ) )  {
    ## full workflow simulating RNA-seq data
    allDat <- readRDS( file = "~/Downloads/complete_processed_tcga2stat_RNASeq2_with_clinical.rds" )
    lastClinical <- which( names(allDat) == 'tumorsize' )
    brca <- allDat[ allDat$disease == "BRCA", (lastClinical+1):ncol(allDat) ]
    dim(brca) ## num of genes 20501
    ## compute the median expression! avoid the 0s
    brcaMedian <- apply(brca, 2, median)
    ## retain top (1-probs)*100% highest expressing genes for illustration
    myProb <- 0.99
    cutPoint <- quantile( x = brcaMedian, probs = myProb )
    ## genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    genesToKeep <- names( brcaMedian ) [ which(brcaMedian >= cutPoint) ]
    brca <- brca[ , genesToKeep ]
    ## ncol(brca) / 20501
    print(d <- length(genesToKeep))
    ## save data as rds
    saveRDS(brca, 'data/brca99.rds')
}
```

```{r ch010-realDataTab, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE}
## Describe and plot real data
brca <- readRDS(file = './data/brca99.rds')
set.seed(10192020)
exampleGenes <- colnames(brca)[sort(sample(ncol(brca), 3))]
saveRDS(exampleGenes, file = './data/exampleGenes.rds')
kable(round(head( brca[, exampleGenes], n = 5)),
      booktabs = TRUE,
      caption = 'Counts of 3 high-expressing genes for the first 5 observations of the BRCA dataset.'
)
## GGally::ggpairs(data = as.data.frame(brca[ ,exampleGenes] ))
```

Figure \@ref(fig:ch010-realDataFig) plots the marginal distributions and estimated Spearman's correlations (see Equation \@ref(eq:spearman) below).

```{r ch010-realDataFig, echo = FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.width= 8, fig.align='center', fig.cap="Marginal scatterplots, densities, and estimated pairwise Spearman's $rho$ for three example genes. The data possess heavy-right tails, are discrete, and have non-trivial intergene correlations. Modeling these data motivate our simulation methodology."}
## Describe and plot real data
GGally::ggpairs(data = as.data.frame(brca[ ,exampleGenes] ))
```

## Measures of dependency

The population correlation coefficient, commonly called the Pearson (product-moment) correlation coefficient, describes the linear association between two random variables $X$ and $Y$ and is given by

\begin{equation}
(\#eq:pearson)
\rho(X,Y) = \frac{E(XY) - E(X)E(Y)}{\left[ var(X)var(Y)\right]^{1/2}} 
\end{equation}

As @MB13 and @MK01 discuss, for a bivariate normal $(X,Y)$ random vector the Pearson correlation adequately describes the dependency between the components. $\rho$'s utility in non-normal or non-linear associations is lacking (@MK01). Rank-based (ordinal) approaches performance better in these settings, such as Spearman's (denoted $\rho_s$) and Kendall's $\tau$. Define 

\begin{equation}
(\#eq:spearman)
\rho_s(X,Y) = 3 \left[ P\left[ (X_1 - X_2)(Y_1-Y_3) > 0 \right] - P\left[ (X_1 - X_2)(Y_1-Y_3) < 0 \right] \right]
\end{equation}

\noindent where $(X_1, Y_1) \overset{d}{=} (X,Y), X_2 \overset{d}{=} X, Y_3 \overset{d}{=} Y$ with $X_2$ and $Y_3$ are independent of one other and of $(X_1, Y_1)$. For continuous marginals, the measure works well due to zero probability of ties. 

To do this, we take advantage of a closed form relationship [ref?] between Kendall's $\tau$ and Pearson's correlation coefficient for bivariate normal random variables:

\begin{equation}
(\#eq:convertKendall)
r_{Pearson} = sin \left( \tau_{Kendall} \times \frac{\pi}{2} \right), 
\end{equation}

\noindent and similarly for Spearman's $\rho$ [@K58],

\begin{equation}
(\#eq:convertSpearman)
\rho_{Pearson} = 2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right).
\end{equation}

*Marginal-dependent bivariate correlation bounds*. 
Adding to the complexity is the well-known, marginal-dependent constraints on the possible bivariate correlation.
This is a consequence of the bounds of bivariate distribution functions --- the *Frechet-Hoeffding bounds* [@Nelsen2007]). 
Denote the pointwise upper bound as $M(y_i, y_{i^\prime})$ These bounds give strict restraints on the possible bounds and cannot be overcome through algorithm design. 
Yet not all multivariate simulation approaches obtain these bounds [for example REF].
Computationally, algorithms that rely on serial computation scale poorly to high dimensions (refs - hierarchy) and fail to make use of modern high-performance computing including parallelization and graphical processing unit acceleration [@Li2019gpu].

The pairwise correlation between two correlated random variables cannot in general obtain the full range of possible values, $[-1,-1]$. The range, called the Frechet(-Hoeffding) bounds, is a well-known function of the marginal distributions and are given by [@BF17]:

\begin{equation}
(\#eq:frechet)
\rho^{max} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (U) \right), \quad \rho^{min} = \rho \left( F^{-1}_1 (U), F^{-1}_2 (1 - U) \right)
\end{equation}

\noindent where $U$ is a uniform random variable in $(0,1)$, and $F^{-1}_1, F^{-1}_2$ are the inverse cdf of random variables $X_1$ and $X_2$, respectively. For discrete random variables, define $F^{-1}$ as in Equation \@ref(eq:inverseCDF).

For discrete marginals, however, one could perform a rescaled version of $\rho_s$ for random variables $X,Y$ with pmfs (or pdfs) $p(x)$ and $q(y)$, respectively. 

\begin{equation}
(\#eq:spearmanRescaled)
\rho_{RS}(X,Y) = \frac{\rho_s(X,Y)}{ \left[ \left[ 1 - \sum_x p(x)^3 \right] \left[ 1 - \sum_y q(y)^3 \right] \right]^{1/2}}
\end{equation}

## Gaussian copulas

The crux of the method is construction of a Gaussian copula. This idea is well known [refs]. We chose a Gaussian copula among the many available copula functions primarily for two reasons: 1) the computationally convient closed expression to convert among the most common dependency measures when the margins are Gaussian and 2) the availability of high-performance multivariate normal simulators in R ('mvnfast' ref).  

A copula is a distribution function on $[0,1]^d$ describing a random vector with standard uniform marginals. Moreover, for any random vector ${\bf X}=(X_1, \ldots, X_d)$ with cumulative distribution function (CDF) $F$ and marginal CDFs $F_i$ there is a copula function 
$C(u_1, \ldots, u_d)$ so that 

\[
F(x_1, \ldots,x_d) = \mathbb P(X_1\leq x_1, \ldots,X_d\leq x_d) = C(F_1(x_1), \ldots, F_d(x_d)), \,\,\, x_i\in \mathbb R, i=1,\ldots,d. 
\]  

A Gaussian copula is the case where all marginal CDFs $F_i$ are the standard normal cdf, $\Phi$. The Gaussian copula is the one that corresponds to a multivariate normal distribution with standard normal marginal distributions and covariance matrix ${\bf R}$. (Since the marginals are standard normal, this ${\bf R}$ is also the correlation matrix). If $F_{{\bf R}}$ is the CDF of such multivariate normal distribution, then the corresponding Gaussian copula $C_{{\bf R}}$ is defined through

\begin{equation}
(\#eq:gauss)
F_{{\bf R}}(x_1, \ldots, x_d) = C_{{\bf R}}(\Phi(x_1), \ldots, \Phi(x_d)),
\end{equation}

where $\Phi(\cdot)$ is the standard normal CDF.
Note that the copula $C_{{\bf R}}$ is simply the CDF of the random vector $(\Phi(X_1), \ldots, \Phi(X_d))$, where  $(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$. 

Sklar's Theorem (ref) guarantees that given inverse CDFs (quantile) and obtainable correlation matrix (within the Frechet bounds) can be obtained via transformations involving copula functions. 
Namely, to simulate a random vector ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$, we can construct a Gaussian copula ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$. 
When an $Y_i$ is discrete, some care must be taken to define $F_i$. Letting 

\begin{equation}
F_{i}^{-1} = inf\{y:F_{i}(y) \geq u \}
(\#eq:inverseCDF)
\end{equation}

\noindent ensures that $Y_i \sim F_i$.

Simulation of a multivariate random vector ${\bf T}$ with margins $F_i$ based on this copula 
