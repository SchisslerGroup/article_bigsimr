---
title: "Simulating High-Dimensional Multivariate Data"
author: "A. Grant Schissler and Alex Knudson"
institute: "Department of Mathematics and Statistics University of Nevada, Reno"
bibliography: /Users/alfred/Dropbox/bib/MulNB.bib
theme: shadow
output: beamer_presentation
aspectratio: 43
---

```{r setup, include=FALSE}
library(ggplot2)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE)
set.seed(44)
```

# 1.1 â€“ Overview  

- In this era of Big Data, it is critical to realistically simulate data to conduct informative Monte Carlo studies. 
- This is often problematic when data are inherently multivariate while at the same time are (ultra-) high dimensional.
- This situation appears frequently in observational data found on online and in high-throughput biomedical experiments (e.g., RNA-sequencing).
- Help researchers think/simulate their data *generatively* from simple joint probability models rather than assuming their statistical model, etc.

# 1.2 - Desired Properties

We seek a general-purpose algorithm that possesses the following properities [adapted from @Nik13a]:

* P1: Wide range of dependence, allowing both positive and negative dependence
* P2: Flexible dependence, meaning that the number of bivariate marginals is (approximately) equal to the number of dependence parameters.
* P3: Flexible marginal modeling, generating hetereogenous data --- possibly from differing (discrete) probability families.

Moreover, in simulation method must scale to high dimensions:  

* S1: Procedure must scale to high dimensions, computable in a reasonable amount time.
* S2: Procedure must scale to high dimensions while maintaining accuracy.

# 2 - Motivating example: large-scale hypothesis testing (Efron 2007) of RNA-seq data

\begin{columns}[T]
  \begin{column}{0.5\columnwidth}
	\begin{itemize}
\item RNA-sequencing data derived from breast cancer patients (n=1093), with 20501 genes measured (from TCGA).
\item We filter to genes with average expression greater than 10000 counts (d = 4777) across both groups.
	\item Consider basic differentially expressed genes (DEG) analysis between surviving (n0 = 941) and deceased (n1 = 152) patients.
\end{itemize}

  \end{column}
    \begin{column}{0.5\columnwidth}
	\begin{itemize}
\item This setting can be described as large-scale simultaneous hypothesis testing, under correlation.
\item We should evaluate existing and new methodology using simulations that reflect dependency among variables.
\item But multivariate simulation tools often scale poorly to high dimension or do not model the full range of dependency.
 
\end{itemize}
	\end{column}
	\end{columns}

```{r load, echo=FALSE, eval=TRUE}
load("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_rnaseq.RData")
## str(brca_rnaseq)
## params <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_nb_params_all.RDS")
## round to remove RSEM adjustment
brca_rnaseq <- round(brca_rnaseq)
## brca_rnaseq[1:5, 1:5]
## get genes from simulated data
sim_kendall_data0 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_kendall_MulNB_threshold=1000_d=4777_N=941_vital=0.rds")
sim_kendall_data1 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_kendall_MulNB_threshold=1000_d=4777_N=152_vital=1.rds")
## str(sim_data)
d <- ncol(sim_kendall_data1)
## Use sim data for 
tmp_genes <- colnames(sim_kendall_data1)
## head(tmp_genes)
## j = 4777
## tmp_genes[j]
## summary(brca_rnaseq[,tmp_genes[j]])

#### Get cinical data
## brca_all <- TCGA2STAT::getTCGA(disease = "BRCA", clinical = TRUE)
## str(brca_all)
## brca_clinical <- brca_all$clinical[row.names(brca_rnaseq),]
## saveRDS(brca_clinical, "~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_clinical.rds")
brca_clinical <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_clinical.rds")
## colnames(brca_clinical)

## Create two groups via a clinical variable

## try vital status
group <- as.numeric(brca_clinical[ , "vitalstatus"])
n1 <- sum(group) ## 152 deaths
n0 <- sum(1 - group) ## 941 survived

## susbest to the genes of interest
tmp_rnaseq <- brca_rnaseq[ ,colnames(brca_rnaseq) %in% tmp_genes]

## str(tmp_rnaseq)

## estimate the correlation
## reps <- 1e3
## pairs_mat <- combn(x = d, m = 2)
## index <- sample(ncol(pairs_mat), size = reps)
## tmp_index <- pairs_mat[,j]
## 
## real_cor_values0 <- apply(pairs_mat[ ,index], 2, function(tmp_index){
##     cor(tmp_rnaseq[ group == 0, tmp_index[1]], tmp_rnaseq[group == 0 ,tmp_index[2]])
## })
## summary(real_cor_values0)
## 
## real_cor_values1 <- apply(pairs_mat[ ,index], 2, function(tmp_index){
##     cor(tmp_rnaseq[ group == 1, tmp_index[1]], tmp_rnaseq[group == 1 ,tmp_index[2]])
## })
## summary(real_cor_values1)
## 
## on average the correlation is weak --- may need a more striking example data set

```

```{r empirical, echo = FALSE, eval=TRUE}
## 1. Empirical case
## j = 1
empirical_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = tmp_rnaseq[group == 0, j], y = tmp_rnaseq[group == 1, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(empirical_z_values)
## sd(empirical_z_values)
## qqnorm(empirical_z_values); abline(0,1)
 
## compute p-values
empirical_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = tmp_rnaseq[group == 0, j], y = tmp_rnaseq[group == 1, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})
lvl <- 0.05
## summary(empirical_p_values)
## sum(empirical_p_values <= lvl)
## sum(empirical_p_values <= lvl)/ d
## Holm's adjustment
fdr_lvl <- holm_lvl <- 0.1
empirical_holm_values <- p.adjust(p = empirical_p_values, method = "holm")
## summary(empirical_holm_values)
## sum(empirical_holm_values <= fdr_lvl)
## sum(empirical_holm_values <= fdr_lvl)/ d
## fdr
empirical_fdr_values <- p.adjust(p = empirical_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(empirical_fdr_values)
## sum(empirical_fdr_values <= fdr_lvl)
## sum(empirical_fdr_values <= fdr_lvl)/ d

## ALL genes were null then this should be N(0,1) (the theoretical null)

```{r independent, echo = FALSE, eval=TRUE}

#### Simulations

## 2. Independence case

## this is quick
params0 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_nb_params_vital=0.RDS")
params0 <- params0[ ,colnames(params0) %in% tmp_genes]
params1 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/brca_nb_params_vital=1.RDS")
params1 <- params1[ ,colnames(params1) %in% tmp_genes]

sim_ind0 <- sapply(1:d, function(j) {
    res <- rnbinom(n = n0, size = params0["alpha",j], prob = (1 + params0["lambda",j])^(-1))
    if (any(is.nan(res))) print(j)
    return(res)
})
## any(is.na(sim_ind0))

## j = 4777
sim_ind1 <- sapply(1:d, function(j) {
    res <- rnbinom(n = n1, size = params1["alpha",j], prob = (1 + params1["lambda",j])^(-1))
    if (any(is.nan(res))) print(j)
    return(res)
})

sim_ind_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_ind0[, j], y = sim_ind1[, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(sim_ind_z_values)
## sd(sim_ind_z_values)
## qqnorm(sim_ind_z_values); abline(0,1)

## compute p-values
sim_ind_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_ind0[, j], y = sim_ind1[, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})
lvl <- 0.05
## summary(sim_ind_p_values)
## sum(sim_ind_p_values <= lvl)
## sum(sim_ind_p_values <= lvl)/ d
## Holm's adjustment
holm_lvl <- 0.1
sim_ind_holm_values <- p.adjust(p = sim_ind_p_values, method = "holm")
## summary(sim_ind_holm_values)
## sum(sim_ind_holm_values <= fdr_lvl)
## sum(sim_ind_holm_values <= fdr_lvl)/ d
## fdr
sim_ind_fdr_values <- p.adjust(p = sim_ind_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(sim_ind_fdr_values)
## sum(sim_ind_fdr_values <= fdr_lvl)
## sum(sim_ind_fdr_values <= fdr_lvl)/ d

```

```{r kendall, echo = FALSE, eval=TRUE}
## 3. Simulated multivariate (show it better mimics the empirical data)

## j = 1
sim_kendall_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_kendall_data0[, j], y = sim_kendall_data1[, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(sim_kendall_z_values)
## sd(sim_kendall_z_values)
## qqnorm(sim_kendall_z_values); abline(0,1)

## compute p-values
sim_kendall_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_kendall_data0[, j], y = sim_kendall_data1[, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})

lvl <- 0.05
## summary(sim_kendall_p_values)
## sum(sim_kendall_p_values <= lvl)
## sum(sim_kendall_p_values <= lvl)/ d
## ## Holm's adjustment
holm_lvl <- 0.1
sim_kendall_holm_values <- p.adjust(p = sim_kendall_p_values, method = "holm")
## summary(sim_kendall_holm_values)
## sum(sim_kendall_holm_values <= fdr_lvl)
## sum(sim_kendall_holm_values <= fdr_lvl)/ d
## fdr
sim_kendall_fdr_values <- p.adjust(p = sim_kendall_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(sim_kendall_fdr_values)
## sum(sim_kendall_fdr_values <= fdr_lvl)
## sum(sim_kendall_fdr_values <= fdr_lvl)/ d
 
```

```{r exactR, echo = FALSE, eval=TRUE}
## 4. Exact Pearson matching via NORTARA.

```

```{r condreg, echo = FALSE, eval=TRUE, include=FALSE}
## Using unadjusted input
sim_condreg_data0 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_condreg_direct_threshold=1000_d=4777_N=941_vital=0.rds")
sim_condreg_data1 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_condreg_direct_threshold=1000_d=4777_N=152_vital=1.rds")

## j = 1
sim_condreg_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_condreg_data0[, j], y = sim_condreg_data1[, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(sim_condreg_z_values)
## sd(sim_condreg_z_values)
## qqnorm(sim_condreg_z_values); abline(0,1)

## compute p-values
sim_condreg_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_condreg_data0[, j], y = sim_condreg_data1[, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})

lvl <- 0.05
## summary(sim_condreg_p_values)
## sum(sim_condreg_p_values <= lvl)
## sum(sim_condreg_p_values <= lvl)/ d
## ## Holm's adjustment
holm_lvl <- 0.1
sim_condreg_holm_values <- p.adjust(p = sim_condreg_p_values, method = "holm")
## summary(sim_condreg_holm_values)
## sum(sim_condreg_holm_values <= fdr_lvl)
## sum(sim_condreg_holm_values <= fdr_lvl)/ d
## fdr
sim_condreg_fdr_values <- p.adjust(p = sim_condreg_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(sim_condreg_fdr_values)
## sum(sim_condreg_fdr_values <= fdr_lvl)
## sum(sim_condreg_fdr_values <= fdr_lvl)/ d

```


```{r approxR, echo = FALSE, eval=TRUE}
## Using unadjusted input
sim_approxR_data0 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_pearson_threshold=1000_d=4777_N=941_vital=0.rds")
sim_approxR_data1 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_pearson_threshold=1000_d=4777_N=152_vital=1.rds")

## j = 1
sim_approxR_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_approxR_data0[, j], y = sim_approxR_data1[, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(sim_approxR_z_values)
## sd(sim_approxR_z_values)
## qqnorm(sim_approxR_z_values); abline(0,1)

## compute p-values
sim_approxR_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_approxR_data0[, j], y = sim_approxR_data1[, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})

lvl <- 0.05
## summary(sim_approxR_p_values)
## sum(sim_approxR_p_values <= lvl)
## sum(sim_approxR_p_values <= lvl)/ d
## ## Holm's adjustment
holm_lvl <- 0.1
sim_approxR_holm_values <- p.adjust(p = sim_approxR_p_values, method = "holm")
## summary(sim_approxR_holm_values)
## sum(sim_approxR_holm_values <= fdr_lvl)
## sum(sim_approxR_holm_values <= fdr_lvl)/ d
## fdr
sim_approxR_fdr_values <- p.adjust(p = sim_approxR_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(sim_approxR_fdr_values)
## sum(sim_approxR_fdr_values <= fdr_lvl)
## sum(sim_approxR_fdr_values <= fdr_lvl)/ d

```


```{r spearman, echo = FALSE, eval=TRUE}
sim_spearman_data0 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_spearman_direct_threshold=1000_d=4777_N=941_vital=0.rds")
sim_spearman_data1 <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/brca/results/sim_data_brca_spearman_direct_threshold=1000_d=4777_N=152_vital=1.rds")

## j = 1
sim_spearman_z_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_spearman_data0[, j], y = sim_spearman_data1[, j])
    ## close to z already for large samples
    tmp_z <- qnorm( p = pt(q = tmp_t$statistic, df = tmp_t$parameter) )
    return(unname(tmp_z))
})
## summary(sim_spearman_z_values)
## sd(sim_spearman_z_values)
## qqnorm(sim_spearman_z_values); abline(0,1)

## compute p-values
sim_spearman_p_values <- sapply(1:d, function(j) {
    tmp_t <- t.test(alternative = "two.sided", x = sim_spearman_data0[, j], y = sim_spearman_data1[, j])
    ## close to z already for large samples
    tmp_p <- tmp_t$p.value
    return(unname(tmp_p))
})

lvl <- 0.05
## summary(sim_spearman_p_values)
## sum(sim_spearman_p_values <= lvl)
## sum(sim_spearman_p_values <= lvl)/ d
## ## Holm's adjustment
holm_lvl <- 0.1
sim_spearman_holm_values <- p.adjust(p = sim_spearman_p_values, method = "holm")
## summary(sim_spearman_holm_values)
## sum(sim_spearman_holm_values <= fdr_lvl)
## sum(sim_spearman_holm_values <= fdr_lvl)/ d
## fdr
sim_spearman_fdr_values <- p.adjust(p = sim_spearman_p_values, method = "fdr")
## fdr_lvl = 0.1
## summary(sim_spearman_fdr_values)
## sum(sim_spearman_fdr_values <= fdr_lvl)
## sum(sim_spearman_fdr_values <= fdr_lvl)/ d

```

```{r combine, echo = FALSE, eval=TRUE}
## empirical is efancy for empirical data
## method_names <- c("empirical", "independent", "approxR", "condreg", "Spearman", "Kendall")
method_names <- c("Empirical", "Independent", "ApproxR", "Spearman", "Kendall")
num_methods <- length(method_names)
all_dat <- data.frame(dat_source = rep( method_names, each = d),
                      index = rep(1:d, times = num_methods))

## aggregate z scores (order matters!)
all_dat$z_score <- c( sort(empirical_z_values, decreasing = T),
                     sort(sim_ind_z_values, decreasing = T),
                     sort(sim_approxR_z_values, decreasing = T),
                     ## sort(sim_condreg_z_values, decreasing = T),
                     sort(sim_spearman_z_values, decreasing = T),
                     sort(sim_kendall_z_values, decreasing = T) )

## aggregate p values (order matters
all_dat$p_value <- c( sort(empirical_p_values),
                     sort(sim_ind_p_values),
                     sort(sim_approxR_p_values),
                     ## sort(sim_condreg_p_values),
                     sort(sim_spearman_p_values),
                     sort(sim_kendall_p_values) )
## all_dat$holm_value <- c(empirical_holm_values, sim_ind_holm_values, sim_mul_holm_values)
## all_dat$fdr_value <- c(empirical_fdr_values, sim_ind_fdr_values, sim_mul_fdr_values)
## str(all_dat)

## add labels
## method_names <- c("empirical", "independent", "approxR", "condreg", "Spearman", "Kendall")
## method_labs <- c("Kendall Copula", "Empirical", "Independent", "Null ref: N(0,1) ")
## factor(as.character(all_dat$dat_source), labels = )

```

# Correspondence between Pearson and rank correlation 

For bivariate Normal r.v's there is a closed form relationship between Pearson's correlation and Spearman's / Kendall's correlation:  

\begin{equation}
\label{convertSpearman}
\rho_{Pearson} = 2 \times sin \left( \rho_{Spearman} \times \frac{\pi}{6} \right).
\end{equation}

\begin{equation}
\label{convertKendall}
r_{Pearson} = sin \left( \tau_{Kendall} \times \frac{\pi}{2} \right), 
\end{equation}

# 3.1 - Methods: Scalable copula-based methods for simulating RNA-seq data as multivariate negative binomial
 
\begin{columns}[T]
  \begin{column}{0.5\columnwidth}
 \includegraphics[width=0.5\paperwidth]{../figures/title_fig.pdf} 
  \end{column}
    \begin{column}{0.5\columnwidth}
	\begin{itemize}
	\item Model genes as marginally negative binomial with heterogenous genewise parameters
\item Compute sample correlations (using desired dependency measure) for the 11,407,476 genes pairs, for each group.
\item We implemented a {\bf parallelized Gaussian copula}, with that produces the correct margins while matching the target correlations.
\end{itemize}
	\end{column}
	\end{columns}

# A remark about discrete marginals

It's known [@MB13] that when the marginals are discrete it constraints the possible correlations for Spearman's correlation. One can rescale to allow the full range of dependency. Call these adjustment factors $a_i = \left[ 1 - \sum_y p_i(y)^3 \right]^{1/2}$ for marginal $F_i$.

\begin{equation}
\label{convertSpearmanDiscrete}
\rho_{rs} \left(Y_{i}, Y_{i^\prime}\right) = \frac{\rho_{s} \left(Y_{i}, Y_{i^\prime}\right)}{a_i \times a_{i^\prime}}.
\end{equation}

This is difficult to compute for discrete marginals with infinite support (such as negative binomial in our example). So one must approximate with a finite sum. The error is known and controlled if done carefully. More details are in the paper.

# Algorithm input
(1) Marginal characteristics including the same of distributional family and parameter values, denotes as $F_i$ for marginal component random variable $Y_i$ for $i=1,\ldots,d$.
(2) A target (specified) Spearman's correlation matrix ${\bf R_{Spearman}}$ with each $\rho_{Spearman}$ within the Frechet limits. Alternatively, the rescaled Spearman's correlation can be provided.
(3) An error tolerance $\epsilon$ for finding the nearest positive definite matrix a transformed correlation matrix (see step iii below). Alternatively, a maximum number of iterations can be supplied.

# Algorithm steps and return value
(i) Compute Spearman's $\rho_{rs}$ from the specified $\rho_{spearman}$ for each pair of marginal random variables $(Y_i,Y_{i^\prime})$ with $i \neq i^\prime$ when either marginal is discrete. 
(ii) Convert ${\bf R_{Spearman}}$ into ${\bf R_{Pearson}}$ via $\rho_{Pearson} = 2 \times sin \left( \rho_{rs} \times \frac{\pi}{6} \right)$.   
(iii) Finally, ensure that ${\bf R_{Pearson}}$ is positive definite, by finding the nearest positive definite correlation matrix ${\bf R}$ --- using the `R` routine `nearPD` in the `Matrix` package --- within an error tolerance of $\epsilon$.
(iv) Generate ${\bf X}=(X_1, \ldots, X_d) \sim N_d({\bf 0}, {\bf R})$;  
(v) Transform ${\bf X}$ to ${\bf U} = (U_1, \ldots,  U_d)$ viz $U_i=\Phi(X_i)$, $i=1, \ldots, d$;  
(vi) Return ${\bf Y}  = (Y_1, \ldots,  Y_d)$, where $Y_i=F_i^{-1}(U_i)$, $i=1, \ldots, d$;  

# Key proposition

The simulation method produces random vectors $(Y_1, \ldots, Y_d)$ with marginal distribution functions $F_1,\ldots,F_d$ matching a given Spearman's/Kendall's correlation matrix for random variables.

- Appeal to Sklar's Theorem (1959) that any joint distribution has a copula representaton and that conversion steps are continuous functions.
- Approximation may be required when using the `Matrix::nearPD` routine (or simply replaced negative eigenvalues with small positive valuse). 
- The montone transformations in steps (v) and (vi) guarantee that the rank-based correlations are preserved for continuous marginals. 
- For discrete marginals with infinite support, the rescaling of Spearman's $\rho$ introduces approxixmation error.

# Implemtation `bigsimr`

We'll release the `bigsimr` `R` package to simulate random vectors. A few notes:  
- The most computationally extensive step lies in the second step of our algorithm.  
- We employ a optimized/parallelized multivariate normal simulator within the `R` package `mvnfast`.  
- The rest of the transformation are also parallelized, as the calculations can be done independently and results aggregated.  

```{r repData, echo = FALSE, eval=TRUE}
sim_ttests <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/results/sim_ttests_brca_spearman_rescaled_rvec_vital_d4777_rep1000_14nov2019.rds")
## sim_ttests <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/results/sim_ttests_brca_spearman_rescaled_rvec_vital_d4777_rep100_14nov2019.rds")
## sim_ttests <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/results/sim_ttests_brca_spearman_rescaled_rvec_vital_d4777_rep100_14nov2019.rds")
## add empirical data
## str(sim_ttests)
## which(!(1:100 %in% unique(sim_ttests$rep)))
sim_ttests$type = "sim"
emp_data <- data.frame(Gene = unique(sim_ttests$Gene), z = empirical_z_values, pvalue = empirical_p_values, rep = NA, type = "empirical", stringsAsFactors = FALSE)
sim_ttests <- rbind(sim_ttests, emp_data)
```

```{r repInd, echo=FALSE, eval=TRUE}
## Replicate with independence assumption
## set up parallel
## library(parallel)
## cl <- makeCluster(detectCores()-1)
## ## ## simulation replicates
## simreps <- 100
## parReplicate <- function(cl, n, expr, simplify=TRUE, USE.NAMES=TRUE)
##     parSapply(cl, integer(n), function(i, ex) eval(ex, envir=.GlobalEnv),
##               substitute(expr), simplify=simplify, USE.NAMES=USE.NAMES)
## # export functions/objects to the cores
## clusterExport(cl,c("sim_independent", "simreps", "n0", "n1", "params0", "params1", "d", "tmp_rnaseq"))
##  
## ## replicate data sets
## sim_ind_list <- parReplicate(cl, simreps, sim_independent(n0 = n0, n1 = n1, params0 = params0, params1 = params1, d = d), simplify = FALSE)
## str(sim_ind_list)
## ## stop the cluster
## stopCluster(cl)
## 
## ## put into long format for analysis
## rep_vec <- rep(1:simreps, each = d)
## sim_ind_ttests <- do.call("rbind", sim_ind_list)
## sim_ind_ttests$rep <- rep_vec
## sim_ind_ttests$type <- "ind"
## str(sim_ind_ttests)
## saveRDS(sim_ind_ttests, file = "sim_ind_ttests_simrep=100_twoSided.rds")
sim_ind_ttests <- readRDS("~/OneDrive - University of Nevada, Reno/Research/Mul_NB/JcompNgraphStats_submission/simDepHDdata_tf_rticle/sim_ind_ttests_simrep=100_twoSided.rds")
## combine
all_ttests <- rbind(sim_ttests, sim_ind_ttests)
all_ttests$type <- factor(all_ttests$type)
## table(all_ttests$type)
## table(all_ttests$rep)
```

# 4.2 Results 

```{r repZplot, echo=FALSE, eval=TRUE, warning=FALSE, out.width='80%', fig.cap="Curves illustrating z-values from 4777 t-tests under the observed statistics ('Empirical'; blue) and simulations."}
## str(sim_ttests)
## plot
## p0 <- ggplot(all_ttests, aes(x = z, group=rep, color = type))
p0 <- ggplot(all_ttests[all_ttests$type != "ind", ], aes(x = z, group=rep, color = type))
p1 <- p0 + geom_density() + theme_bw()
p1 + geom_density(data=all_ttests[all_ttests$type != "sim", ], aes(x = z, group=rep, color = type))
## ggplot(aes = aes( x = empirical_z_values)) + geom_density()
```

# 5. Conclusions / Future Work

- Correlations matter during testing involving high dimensional data [see @Efron2007a].
- High performance algorithms enable high-dimensional copula-based [e.g., @Li2019gpu] multivariate simulations.
- Release `bigsimr` R package implementing this method.
- Explore the role of both row and column correlation (instead of just columnwise dependence).
- Explore the scalability of exact Pearson-matching methods (for example, see @Chen2001 and @Xia17).
- Explore the role of covariance estimation (sparsity, low-rank, etc) in the simulation process.

# 6.1 - Acknowledgements

\begin{columns}[T]
  \begin{column}{0.55\columnwidth}
    \begin{block}{University of Nevada, Reno}
      Alex Knudson, MS Stat student\\
      Anna Panorska, PhD\\
      Tomasz Kozubowski, PhD
    \end{block}
    \begin{block}{University of Arizona}
      Walter W.~Piegorsch, PhD\\
      Edward J.~Bedrick, PhD
      %%D.~Dean Billheimer, PhD\\
      %%Hao Helen Zhang, PhD\\
      %%Qike Li, MS
    \end{block}

    %%\begin{block}{Biologists}
    %%  Ikbel Achour, PhD\\
    %%  Joanne Berghout, PhD
    %%\end{block}
  \end{column}
  \begin{column}{0.45\columnwidth}
%%    \begin{block}{Medical doctors}
%%      Yves A.~Lussier, MD
%%    \end{block}
    \begin{block}{Grants/travel awards}
      Research \& Innovation, UNR
    \end{block}
    \begin{block}{CFE-CMSTATS 2019 organizers}
		Mihye Ahn, PhD\\
		Chae Ryon Kang, PhD\\
		All others involved
    \end{block}
  \end{column}
\end{columns}

# 6.2 References
